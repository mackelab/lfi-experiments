{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients on proposal distributions (adaptive proposals)\n",
    "\n",
    "\n",
    "**Disclaimer**: If we don't want to assume knowing anything about the likelihood (here the conditional entropy $H_{x|\\theta}[X|\\theta_n]$ for samples $\\theta_n$), don't bother reading.\n",
    "\n",
    "### SNPE, CDELFI and rounds\n",
    "\n",
    "- SNPE and CDELFI are methods to fit a facotrizing model $q_\\phi(\\theta|x)p(x)$ for the joint density $p(x,\\theta)$ of parameters $\\theta$ and summary statistics $x$ defined by a stochastic simulation model $p(x|\\theta)$ and prior $p(\\theta)$. \n",
    "- both methods support proposal distributions $\\tilde{p}(\\theta)$ to substitute the prior $p(\\theta)$ during fitting, allowing to focus the density learning on regions of $p(x,\\theta)$ where $x \\approx x_0$ for actually observed data $x_0$. \n",
    "- coming up with such proposal distributions $\\tilde{p}(\\theta)$ is cumbersome - the standard approach of both methods is to run a sequence of 'rounds' - each being a full model fit of its own that initiates $\\tilde{p}(\\theta) \\leftarrow q_\\phi(\\theta \\ | \\ x_0)$ with the posterior estimate from the previous round.\n",
    "\n",
    "\n",
    "- rounds are problematic:\n",
    "\n",
    "\n",
    "- The simple assignment $\\tilde{p}(\\theta) \\leftarrow q_\\phi(\\theta \\ | \\ x_0)$ introduces hard-to-control dynamics across rounds.\n",
    "- initial $\\tilde{p}(\\theta)$ might be poor and not focus well on $x_0$, but early rounds nonetheless need to be run with large-enough simulated data-sets $\\mathcal{D}=\\{(x_n, \\theta_n)\\}_{n=1}^N$ to generate a better proposal for next round - this in practice means sampling enough data to fit a full mixture density network!\n",
    "\n",
    "- it is an open question at this point whether or not importance sampling and gradient descent go together well, at least in the setup where we sample a single big fixed data-set $\\mathcal{D}$ per round and then do importance-weighted gradient descent on $\\mathcal{D}$. \n",
    "Other examples from the literature (Gu, Gharamani, Turner, 2015, others??) combining importance sampling with gradient descent sample new batches from the proposal at every gradient step for the model.\n",
    "\n",
    "- a different approach is to parametrize the proposal and try to learn both proposal $p_\\psi(\\theta)$ and conditional density $q_\\phi(\\theta \\ | \\ x)$ jointly. \n",
    "\n",
    "- but what loss term should we use to choose a good proposal parameter $\\psi$?\n",
    "\n",
    "### parametrized proposal distributions\n",
    "\n",
    "- another problem here however also comes already from the SNPE/CDELFI loss, \n",
    "$$\\mathcal{L}(\\theta,\\psi) =  \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p(x)) = - \\int p_\\psi(\\theta) p(x|\\theta) \\log q_\\phi(\\theta | x) dx d\\theta - \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta $$\n",
    "When only optimizing for $\\phi$, treating $\\psi$ constant, this loss allows to fully avoid evaluating the unknown densities $p(x | \\theta)$ and especially $p(x) =  \\int p(x|\\theta) p_\\psi(\\theta) = f(x, \\psi)$ through MC-approximations of the integrals. It however no longer does so for variable $\\psi$. Likelihood and marginal evaluations need to be estimated, which requires costly simulations. This is particularly true for the marginal density $p(x)$ that would in practice require many likelihood estimates $p(x|\\theta_i), \\theta_i \\sim p_\\psi(\\theta)$ to approximate well with Monte Carlo. \n",
    "\n",
    "\n",
    "- it is interesting to note that the rhs term that is constant in $\\phi$ (but not in $\\psi$), \n",
    "$$ - \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta$$\n",
    "is a (degenerate) $D_{KL}$ between target joint distribution $p(\\theta)p(x|\\theta)$ and the problematic marginal $p(x)$. \n",
    "Can this term provide information to fit a parametrized model $p_\\omega(x)$ to approximate the marginal $p(x)$ induced by $p_\\psi(\\theta)$?\n",
    "- w.r.t. $\\omega$,  $-\\int p_\\psi(\\theta,x) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta|x)p(x)} dx d\\theta = D_{KL}(p(x) ||  p_\\omega(x)) + \\int p_\\psi(\\theta,x) \\log p_\\psi(\\theta|x) dx d\\theta$ is minimized if $p_\\omega(x) = p(x)$ almost everywhere. \n",
    "- w.r.t. $\\psi$,  the result of gradient descent is less obvious to me. \n",
    "\n",
    "\n",
    "- this link between proposal $p_\\psi$ and marginal $p_\\omega$ could help us choose a good proposal prior: the basis of most variational approximations is indeed to try to maximimize the marginal data-likelihood $p_\\omega(x_0)$.  \n",
    "\n",
    "### adaptive-proposal loss\n",
    "\n",
    "$$\\mathcal{L}(\\theta,\\psi,\\omega) =  \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p_\\omega(x)) -  \\log p_\\omega(x_0) \\\\ \n",
    "= - \\int p_\\psi(\\theta) p(x|\\theta) \\log q_\\phi(\\theta | x) dx d\\theta - \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta - \\log p_\\omega(x_0)$$\n",
    "\n",
    "- only unparametrized part of the loss is the likelihood $p(x|\\theta)$, the rest can be jointly optimized with (stochastic) gradient descent!\n",
    "- gradient w.r.t. $\\phi$ is virtually unchanged relative to SNPE / CDELFI\n",
    "- gradient w.r.t. $\\psi$ tries to balance maximizing conditional probabilities $q_\\phi(\\theta|x)$ while keeping the induced marginal $p(x)$ close to $p_\\omega(x)$ (in *some* sense).  \n",
    "- gradient w.r.t. $\\omega$ tries to balance staying close to $p(x)$ and maximizing the observed-data likelihood $p_\\omega(x_0)$.  \n",
    "- remember $\\frac{\\partial}{\\partial{}\\psi} \\int p_\\psi(\\theta) f(\\theta) d\\theta = \\int p_\\psi(\\theta) f(\\theta)\\frac{\\partial}{\\partial{}\\psi} \\log p_\\psi(\\theta) d\\theta$ under mild constraints, i.e. we can use Monte Carlo for gradients wrt. the proposal\n",
    "- we are left with an evaluation of the log-likelihood in the new loss! In practice, we will need the conditional entropy $H_{x|\\theta}[X|\\theta_n]$ over summary statistics at sampled parameters $\\theta_n$. Since $H_{x|\\theta}$ maps only from parameters $\\theta$ into the reals (whereas $p(x|\\theta)$ is defined over the joined set of all $(x,\\theta)$), this quantity may be easier to aquire than the likelihood, esp. when there are much fewer parameters than summary statistics. For the toy scenario investigated in this notebook, we will assume $H_{x|\\theta}[X|\\theta_n]$ given. In general, this term can be approximated with (multiple) summary statistics samples  $x_n^i$, $i=1,\\ldots,K$ for each $\\theta_n$. \n",
    "\n",
    "\n",
    "### questions\n",
    "- what **exactly** does $\\mbox{argmin}_{\\psi} - \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta$ do ? Ideally, it just sets proposal $\\psi$ such that $\\int p_\\psi(\\theta) \\ p(x|\\theta) d\\theta = p_\\omega(x)$\n",
    "- do the three major loss terms ($q_\\phi$, $H[p(X,\\theta)|p(X)], \\log p_\\omega(x_0)$) work together in concert as desired, or will one dominate (e.g. will the proposal overadapt to the current MDN $\\phi$ rather than try to follow and give 'wiggle-space' $p_\\omega(x_0)$)? Will we need to introduce weights, e.g. $\\frac{1}{\\lambda} \\log p_\\omega(x_0)$?  \n",
    "- what are good model assumptions for $p_\\omega$? How important is capturing the data marginal $p(x)$, how important is matching proposal*likelihood ?\n",
    "- how to best estimate the conditional entropies $H[X|\\theta]$ from simulations? Repeated sampling $x_n^i$ for each $\\theta_n$? Fit another network?\n",
    "- can this be extended with importance sampling? The above scheme requires analytical correction for the final proposal distribution (after convergence), but can we relax this when re-introducing importance weights? If yes, does the ability to choose another proposal distribution after each individual batch-gradient allow us to get more stable importance sampling schemes, i.e. keep the importance sampling up to date with the gradient descent wr.r.t $\\phi$?\n",
    "- can this be extended to SVI? Note that $p_\\omega(x)$ essentially just completes the parametrization of the joint-data likelihood $q_\\phi(\\theta_n|x_n)p_\\omega(x_n) = p(\\theta_n,x_n | \\phi, \\omega)$, but that a constantly updated proposal $p_\\psi(\\theta)$ destroys the notion of having a single fixed data-set. How do we do Bayesian online-learning when the data-distribution is known to be a (stochastically) moving target? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy problem setup\n",
    "- how much can be done analytical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary of analytic results (pasted below again where applicable)\n",
    "\n",
    "prior: \n",
    "\n",
    "$p(\\theta) = \\mathcal{N}(\\theta \\ | \\ 0, \\eta^2)$\n",
    "\n",
    "proposal prior: \n",
    "\n",
    "$p_\\psi(\\theta) = \\mathcal{N}(\\theta \\ | \\ \\nu, \\xi^2)$\n",
    "\n",
    "simulator: \n",
    "\n",
    "$p(x \\ | \\ \\theta) =  \\mathcal{N}(x \\ | \\ \\theta, \\sigma^2)$\n",
    "\n",
    "analytic posteriors: \n",
    "\n",
    "$p(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\eta^2}{\\eta^2 + \\sigma^2} x, \\eta^2 - \\frac{\\eta^4}{\\eta^2 + \\sigma^2})$ \n",
    "\n",
    "$\\tilde{p}(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\xi^2}{\\xi^2 + \\sigma^2} x + \\frac{\\sigma^2}{\\xi^2 + \\sigma^2} \\nu, \\xi^2 - \\frac{\\xi^4}{\\xi^2 + \\sigma^2})$\n",
    "\n",
    "variational marginal:\n",
    "\n",
    "$p_\\omega(x) = \\mathcal{N}(x \\ | \\ \\tau, \\rho^2)$\n",
    "\n",
    "Data:\n",
    "\n",
    "$(x_n, \\theta_n) \\sim p(\\theta) p(x \\ | \\ \\theta) = \\mathcal{N}( (x_n, \\theta_n) \\ | \\ (\\nu, \\nu), \n",
    "\\begin{pmatrix}\n",
    "\\xi^{2} + \\sigma^{2} &  \\xi^{2}  \\\\\n",
    "\\xi^{2} & \\xi^{2}  \\\\\n",
    "\\end{pmatrix})$\n",
    "\n",
    "Model: \n",
    "\n",
    "$ q_\\phi(\\theta_n | x_n) = \\mathcal{N}(\\theta_n \\ | \\ \\mu_\\phi(x_n), \\sigma^2_\\phi(x_n))$\n",
    "\n",
    "$ (\\mu_\\phi(x), \\sigma^2_\\phi(x))^\\top = MDN_\\phi(x) = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} x + \\begin{pmatrix} \\alpha \\\\ \\gamma^2 \\end{pmatrix}$\n",
    "\n",
    "Loss: \n",
    "\n",
    "$\\mathcal{L}(\\phi, \\psi, \\omega) = - \\lambda \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p_\\omega(x)) + \\log p_\\omega(x_0)$\n",
    "\n",
    "Gradients: \n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\phi} = \\frac{\\partial}{\\partial\\phi} \\int p(x|\\theta) p_\\psi(\\theta) \\ \\log q_\\phi(\\theta\\ | \\ x) \\ dx d\\theta \\approx \\frac{1}{N} \\sum_n \\frac{\\partial}{\\partial\\phi} \\log q_\\phi(\\theta_n | x_n)$\n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\omega} = \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_0) + \\lambda \\ \\frac{\\partial}{\\partial\\omega}  \\int p(x|\\theta) p_\\psi(\\theta) \\ \\log p_\\omega(x) \\ dx d\\theta \\approx  \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_0) + \\frac{\\lambda}{N} \\sum_n \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_n)$\n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\psi} \n",
    "%= \\frac{\\partial}{\\partial\\psi} \\int p(x|\\theta) p_\\psi(\\theta) \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) - \\log p(x|\\theta) p_\\psi(\\theta) \\right) dx d\\theta \n",
    "= \\int p(x|\\theta) p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) \\right) dx d\\theta - \\int p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] H[X|\\theta] d\\theta - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta] \\\\ \n",
    "\\approx \\frac{1}{N}\\sum_n \\left( \\log q_\\phi(\\theta_n|x_n) + \\log p_\\omega(x_n) - H[X|\\theta_n] \\right) \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta_n) - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.summarystats as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "\n",
    "from delfi.simulator.Gauss import Gauss\n",
    "\n",
    "### dL/d\\phi : updating the MDN given batch \n",
    "\n",
    "n_phi   = 3 # number of parameters for MDN\n",
    "n_omega = 2 # number of marginal parameters\n",
    "n_psi   = 2 # number of proposal parameters\n",
    "\n",
    "\n",
    "\"\"\" three-parameter linear toy MDN (sufficient for this class of problems!) \"\"\"\n",
    "\n",
    "# posterior density (properly normalized!)\n",
    "def lq_phi(th, x, phi):\n",
    "    \"\"\" assuming q_\\phi(\\theta | x) to be Gaussian \"\"\"\n",
    "    \n",
    "    alpha, beta, gamma2 = phi[0], phi[1], phi[2]\n",
    "\n",
    "    mu_phi = beta * x + alpha\n",
    "    sig2_phi = gamma2\n",
    "    \n",
    "    lq = -.5 * np.log(2 * np.pi) - np.log(sig2_phi) - 0.5 * (th-mu_phi)**2/sig2_phi\n",
    "    \n",
    "    return lq\n",
    "\n",
    "# gradient\n",
    "def dL_dphi( batch, phi):\n",
    "    \n",
    "    \"\"\" written as *minimizing* the loss \"\"\"\n",
    "        \n",
    "    th,x=batch\n",
    "\n",
    "    N = x.shape[0]\n",
    "    assert N == th.shape[0]\n",
    "    \n",
    "    normals = np.ones(N)/N # no importance weighting, no calibration kernel for now\n",
    "    \n",
    "    g = np.array([f(*batch, normals, *map(np.asarray, phi)) for f in (dL_dalpha, dL_dbeta, dL_dgamma2)])\n",
    "    return g.reshape(-1)\n",
    "\n",
    "def dL_dalpha( params, stats, normals, alpha, beta, gamma2):\n",
    "\n",
    "    mu_phi = beta * stats.reshape(-1,1) + alpha.reshape(1,-1)\n",
    "    sig_phi = gamma2\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - mu_phi)/sig_phi).sum(axis=0)\n",
    "\n",
    "\n",
    "def dL_dbeta( params, stats, normals, alpha, beta, gamma2):\n",
    "\n",
    "    mu_phi = beta * stats.reshape(-1,1) + alpha.reshape(1,-1)\n",
    "    sig_phi = gamma2\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - mu_phi)/sig_phi * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "def dL_dgamma2( params, stats, normals, alpha, beta, gamma2):\n",
    "\n",
    "    tmp = (params.reshape(-1,1) - beta*stats.reshape(-1,1) - alpha)**2 / gamma2.reshape(1,-1)\n",
    "    return 1/gamma2.reshape(1,-1) * (normals.reshape(-1,1) * (1 - tmp)).sum(axis=0)\n",
    "\n",
    "\"\"\" marginal distribution over x (part of likelihood actually) \"\"\"\n",
    "\n",
    "# marginal density (properly normalized!)\n",
    "def lp_omega(x, omega):\n",
    "    \"\"\" assuming approximate marginal p_\\omega(x) to be Gaussian\"\"\"\n",
    "    \n",
    "    mu, sig2 = omega[0], omega[1]\n",
    "\n",
    "    lp = -.5 * np.log(2 * np.pi) - np.log(sig2) - 0.5 * (x-mu)**2/sig2\n",
    "    \n",
    "    return lp\n",
    "\n",
    "# gradient\n",
    "def dL_domega( batch, x0, omega, lambd = 1):\n",
    "    \"\"\" assuming approximate marginal p_\\omega(x) to be Gaussian\"\"\"\n",
    "    \n",
    "    mu, sig2 = omega[0], omega[1]\n",
    "    th,x = batch\n",
    "\n",
    "    N = x.shape[0]\n",
    "    assert N == th.shape[0]\n",
    "    \n",
    "    g = np.ones(2) / sig2\n",
    "    g[0] *= - x0 + lambd/N * x.sum(axis=0) + (1+lambd)*mu\n",
    "    g[1] *= (1+lambd)/2 - ((x0-mu)**2 + lambd/N * np.sum((x-mu)**2,axis=0))/(2*sig2)\n",
    "    \n",
    "    return g\n",
    "    \n",
    "    \n",
    "def dL_dpsi( batch, phi, omega, psi, hX):\n",
    "    \"\"\" assuming proposal p_\\psi(\\theta) to be Gaussian \"\"\"\n",
    "    \n",
    "    \"\"\" written as *minimizing* the loss \"\"\"\n",
    "        \n",
    "    th,x = batch\n",
    "    mu = psi[0]\n",
    "    sig2 = psi[1] **2   # std parametrization \n",
    "    \n",
    "    N = x.shape[0]\n",
    "    assert N == th.shape[0]    \n",
    "    \n",
    "    fac = lq_phi(th, x, phi) + lp_omega(x, omega) + hX(th)\n",
    "    \n",
    "    g = np.ones(2)        \n",
    "    g[0] = - (fac * (th-mu)/sig2).mean(axis=0)    \n",
    "    g[1] = 0.5/(sig2) * ( (fac * (1-(th-mu)**2/sig2)).mean()  - 1 ) \n",
    "    \n",
    "    g[1] *= 2 * psi[1] # std parametrization \n",
    "    return g\n",
    "\n",
    "\n",
    "\"\"\" utility \"\"\"\n",
    "    \n",
    "# overall gradients   \n",
    "\n",
    "def grad( batch, pars, x0, hX, lambd=1):\n",
    "    \n",
    "    phi, omega, psi = unpack_pars(pars)\n",
    "    \n",
    "    g = np.hstack((dL_dphi( batch, phi), \n",
    "                   dL_domega( batch, x0, omega, lambd), \n",
    "                   dL_dpsi( batch, phi, omega, psi, hX) ))\n",
    "    \n",
    "    return g\n",
    "\n",
    "# parameter plumbing\n",
    "\n",
    "def pack_pars(phi, omega, psi):\n",
    "    \n",
    "    return np.hstack((phi.reshape(-1), omega.reshape(-1), psi.reshape(-1)))\n",
    "\n",
    "def unpack_pars(pars):\n",
    "\n",
    "    phi,omega,psi = pars[:n_phi], pars[n_phi : n_phi+n_omega], pars[-n_psi:]\n",
    "\n",
    "    return phi, omega, psi\n",
    "    \n",
    "# analytical correction for proposal prior\n",
    "\n",
    "def analytic_div(out, eta2, nus, ksi2s):\n",
    "    \"\"\" analytic correction of onedimensional Gaussians for proposal priors\"\"\"\n",
    "    # assumes true prior to have zero mean!\n",
    "    # INPUTS:\n",
    "    # - out: 3D-tensor: \n",
    "    #        1st axis gives ksi2s (proposal variances), \n",
    "    #        2nd axis gives number of experiments/runs/fits\n",
    "    #        3nd axis is size 2: out[i,j,0] Gaussian mean, out[i,j,0] Gaussian variance\n",
    "    # - eta2:  prior variance (scalar)\n",
    "    # - nus:   vector of proposal prior means\n",
    "    # - ksi2s: vector of proposal prior variances\n",
    "    \n",
    "    # OUTPUTS\n",
    "    # - out_: 3D tensor of proposal-corrected posterior means and variances\n",
    "    \n",
    "    out_ = np.empty_like(out)\n",
    "    for i in range(out_.shape[0]):\n",
    "        \n",
    "        # precision and precision*mean\n",
    "        P = 1/out[i,:,1]\n",
    "        Pm = P * out[i,:,0]\n",
    "\n",
    "        # multiply with prior\n",
    "        P = P + 1/eta2\n",
    "        Pm = Pm + 0/eta2\n",
    "\n",
    "        # divide by proposal\n",
    "        P = P - 1/ksi2s[i]\n",
    "        Pm = Pm - nus[i]/ksi2s[i]\n",
    "\n",
    "        out_[i,:,:] = np.vstack((Pm/P, 1/P)).T\n",
    "\n",
    "    return out_    \n",
    "\n",
    "# (tiny little) ADAM implementation in raw numpy (copy&paste from stitching project)\n",
    "\n",
    "def adam_step(pars,g,m,v,a,b1,b2,e,t):\n",
    "\n",
    "    m = (b1 * m + (1-b1) * g)\n",
    "    v = (b2 * v + (1-b2) * g**2)\n",
    "\n",
    "    mh,vh = adam_norm(m,v,b1,b2,t)\n",
    "\n",
    "    pars -= a* mh / (np.sqrt(vh) + e)\n",
    "\n",
    "    return pars, m, v\n",
    "\n",
    "def adam_norm(m, v, b1, b2,t):\n",
    "\n",
    "    m = m / (1-b1**t) if b1 != 1 else m.copy()\n",
    "    v = v / (1-b2**t) if b2 != 1 else v.copy()\n",
    "\n",
    "    return m,v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first check effects of the new term\n",
    "\n",
    "What does \n",
    "$\\mbox{argmin}_\\psi \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta$\n",
    "do to?\n",
    "\n",
    "- oddly, $\\psi$ seems to ignore likelihood variance completely: \n",
    "- Proposal mean $\\mu_\\psi$ is placed such that the mean of the induced marginal $p(x)=\\int p_\\psi(\\theta) p(x|\\theta)$ coincides with that of $p_\\epsilon(x)$.\n",
    "- Proposal variance $\\sigma^2_\\psi$ however directly accounts for **all** the marginal variance $\\sigma^2_\\omega$, and does not `leave space' for the additional likelihood variance.\n",
    "- this can also be shown from considering the $D_{\\mbox{KL}}(p_\\psi(\\theta) p(x|\\theta)|| p_\\omega(x))$ (where we need to add a degenerate Gaussian over $\\theta$ for analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key equations\n",
    "\n",
    "$$\\mathcal{L}(\\theta,\\psi,\\omega) = - \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p_\\omega(x)) + \\log p_\\omega(x_0) \\\\ \n",
    "= \\int p_\\psi(\\theta) p(x|\\theta) \\log q_\\phi(\\theta | x) dx d\\theta + \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta + \\log p_\\omega(x_0)$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\psi} \n",
    "%= \\frac{\\partial}{\\partial\\psi} \\int p(x|\\theta) p_\\psi(\\theta) \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) - \\log p(x|\\theta) p_\\psi(\\theta) \\right) dx d\\theta \n",
    "= \\int p(x|\\theta) p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) \\right) dx d\\theta - \\int p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] H[X|\\theta] d\\theta - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta] \\\\ \n",
    "\\approx \\frac{1}{N}\\sum_n \\left( \\log q_\\phi(\\theta_n|x_n) + \\log p_\\omega(x_n) - H[X|\\theta_n] \\right) \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta_n) - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig2 = 1.0  # likelihood variance : unfortunately does not seem to have much of an effect on psi here !\n",
    "\n",
    "def hX(th):\n",
    "    \"\"\" conditional entropy over X given \\theta for likelihood (reminder: knowing this is cheating!) \"\"\"\n",
    "    return np.log(2*np.pi*sig2)/2\n",
    "\n",
    "def dLpsi_dpsi( batch, phi, omega, psi, hX):\n",
    "    \"\"\" assuming proposal p_\\psi(\\theta) to be Gaussian \"\"\"\n",
    "    \n",
    "    \"\"\" written as *minimizing* the loss \"\"\"\n",
    "    \n",
    "    th,x = batch\n",
    "    mu = psi[0]\n",
    "    sig2 = psi[1] **2   # std parametrization \n",
    "    \n",
    "    N = x.shape[0]\n",
    "    assert N == th.shape[0]    \n",
    "    \n",
    "    g = np.ones(2)    \n",
    "    fac = lp_omega(x, omega) +  hX(th)\n",
    "    \n",
    "    g[0] = - (fac * (th-mu)/sig2).mean(axis=0)\n",
    "    \n",
    "    g[1] = 0.5/(sig2) * ( (fac * (1-(th-mu)**2/sig2)).mean()  - 1 ) \n",
    "    \n",
    "    g[1] *= 2 * psi[1] # std parametrization \n",
    "    return g\n",
    "\n",
    "\n",
    "phi = np.array([0.,0,1])\n",
    "omega = np.array([0., 1.0])\n",
    "psi = np.array([0., 1.])\n",
    "\n",
    "print('omega \\n', omega)\n",
    "\n",
    "lr = 0.01\n",
    "lr_decay = 0.99999\n",
    "b1,b2,e=0.9, 0.99, 1e-8\n",
    "m,v = np.zeros(2),np.ones(2)\n",
    "batch_size = 100\n",
    "\n",
    "#print(sig2)\n",
    "for i in range(5000):    \n",
    "\n",
    "    th = psi[0] + psi[1] * np.random.normal(size=batch_size)\n",
    "    x  = th + np.sqrt(sig2)* np.random.normal(size=batch_size)\n",
    "    batch = (th, x)\n",
    "    \n",
    "    if np.any(np.isnan(x)):\n",
    "        break\n",
    "    lr *= lr_decay\n",
    "        \n",
    "    g = dLpsi_dpsi( batch, phi, omega, psi, hX)\n",
    "    lr *= lr_decay\n",
    "    psi, m, v = adam_step(psi,g,m,v,lr,b1,b2,e,i+1)    \n",
    "    \n",
    "psi[1] = psi[1]**2\n",
    "print('psi \\n', psi)\n",
    "\n",
    "\n",
    "# compare visually with result from viewing the whole thing as D_{KL} with p(x,th) = p_\\omega(x) * N*(th),\n",
    "# with N*(th) being a Gaussian with infinite variance:\n",
    "# that one gives mu_\\psi = mu_\\omega, sig2_\\psi/sig2_\\omega = np.log(sig2_\\psi * sig2), i.e. sig2_\\psi = sig2_\\omega!\n",
    "\n",
    "sigs = np.linspace(omega[1]-.5,omega[1]+.5,100)\n",
    "\n",
    "plt.plot(sigs,  sigs/omega[1]- np.log(sigs*sig2))\n",
    "#plt.plot(sigs,  1/omega[1]- 1/sigs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full derivatives\n",
    "- tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d/d$\\phi$\n",
    "- checking with fixed large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig2 = 2.5  # likelihood variance : unfortunately does not seem to have much of an effect on psi here !\n",
    "\n",
    "def hX(th):\n",
    "    \"\"\" conditional entropy over X given \\theta for likelihood (reminder: knowing this is cheating!) \"\"\"\n",
    "    return np.log(2*np.pi*sig2)/2\n",
    "\n",
    "phi = np.array([0.0, 0.0, 1.])\n",
    "omega = np.array([0.0, 1.0])\n",
    "psi = np.array([0.4, 1.5])\n",
    "\n",
    "print('omega \\n', omega)\n",
    "\n",
    "lr = 0.005\n",
    "lr_decay = 1.0\n",
    "b1,b2,e=0.9, 0.99, 1e-8\n",
    "m,v = np.zeros(3),np.ones(3)\n",
    "\n",
    "#print(sig2)\n",
    "\n",
    "batch_size = 10000\n",
    "th = psi[0] + psi[1] * np.random.normal(size=batch_size)\n",
    "x  = th + np.sqrt(sig2)* np.random.normal(size=batch_size)\n",
    "batch = (th, x)\n",
    "\n",
    "for i in range(20000):    \n",
    "\n",
    "    \n",
    "    if np.any(np.isnan(x)):\n",
    "        break\n",
    "    lr *= lr_decay\n",
    "        \n",
    "    g = dL_dphi( batch, phi)        \n",
    "    lr *= lr_decay\n",
    "    phi, m, v = adam_step(phi,g,m,v,lr,b1,b2,e,i+1)    \n",
    "    \n",
    "print('phi \\n', phi)\n",
    "psi[1] = psi[1]**2\n",
    "\n",
    "# analytic solution\n",
    "\n",
    "a, b, = phi[0], phi[1]\n",
    "print('analytic variance sol.:\\n', (1-b)**2 * psi[1] + b**2 * sig2 + ( a + (b-1)*psi[0] )**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d/d$\\omega$\n",
    "- checking with fixed large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig2 = 1.  # likelihood variance : unfortunately does not seem to have much of an effect on psi here !\n",
    "\n",
    "x0 = 0.8 \n",
    "lambd = 1.\n",
    "\n",
    "phi = np.array([0.0, 0.0, 1.])\n",
    "omega = np.array([0.0, 1.0])\n",
    "psi = np.array([0., 1.])\n",
    "\n",
    "lr = 0.005\n",
    "lr_decay = 1.0\n",
    "b1,b2,e=0.9, 0.99, 1e-8\n",
    "m,v = np.zeros(2),np.ones(2)\n",
    "\n",
    "print('\\lambda \\n', lambd)\n",
    "print('x0 \\n', x0)\n",
    "\n",
    "batch_size = 10000\n",
    "th = psi[0] + psi[1] * np.random.normal(size=batch_size)\n",
    "x  = th + np.sqrt(sig2)* np.random.normal(size=batch_size)\n",
    "batch = (th, x)\n",
    "\n",
    "for i in range(10000):    \n",
    "\n",
    "    \n",
    "    if np.any(np.isnan(x)):\n",
    "        break\n",
    "    lr *= lr_decay\n",
    "        \n",
    "    g = dL_domega( batch, x0, omega, lambd = lambd)       \n",
    "    #g = dL_dpsi( batch, phi, omega, psi, hX)\n",
    "    lr *= lr_decay\n",
    "    omega, m, v = adam_step(omega,g,m,v,lr,b1,b2,e,i+1)    \n",
    "    \n",
    "print('omega \\n', omega)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d/d$\\psi$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig2 = 10.0  # likelihood variance : unfortunately does not seem to have much of an effect on psi here !\n",
    "\n",
    "phi = np.array([0., 0., 1.])\n",
    "omega = np.array([0., 1.])\n",
    "psi = np.array([0., 1.])\n",
    "\n",
    "print('omega \\n', omega)\n",
    "print('phi \\n', phi)\n",
    "\n",
    "lr = 0.01\n",
    "lr_decay = 0.99999\n",
    "b1,b2,e=0.9, 0.99, 1e-8\n",
    "m,v = np.zeros(2),np.ones(2)\n",
    "batch_size = 1000\n",
    "\n",
    "#print(sig2)\n",
    "for i in range(5000):    \n",
    "\n",
    "    th = psi[0] + psi[1] * np.random.normal(size=batch_size)\n",
    "    x  = th + np.sqrt(sig2)* np.random.normal(size=batch_size)\n",
    "    batch = (th, x)\n",
    "    \n",
    "    if np.any(np.isnan(x)):\n",
    "        break\n",
    "    lr *= lr_decay\n",
    "        \n",
    "    g = dL_dpsi( batch, phi, omega, psi, hX)\n",
    "    lr *= lr_decay\n",
    "    psi, m, v = adam_step(psi,g,m,v,lr,b1,b2,e,i+1)    \n",
    "    \n",
    "psi[1] = psi[1]**2\n",
    "print('psi \\n', psi)\n",
    "\n",
    "# compare with analytic solution to sigma^2_\\psi under all-Gaussian setup\n",
    "print('analytic variance sol.:\\n', 1/((1 - phi[1] )**2 / phi[2] + 1/omega[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad( batch, pars, x0, hX, lambd=1):\n",
    "    \n",
    "    phi, omega, psi = unpack_pars(pars)\n",
    "    \n",
    "    g = np.hstack((dL_dphi( batch, phi), \n",
    "                   dL_domega( batch, x0, omega, lambd), \n",
    "                   dL_dpsi( batch, phi, omega, psi, hX) ))\n",
    "    \n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = 0.8 \n",
    "lambd = 1.0\n",
    "sig2 = 1.0   # likelihood variance : unfortunately does not seem to have much of an effect on psi here !\n",
    "\n",
    "def hX(th):\n",
    "    \"\"\" conditional entropy over X given \\theta for likelihood (reminder: knowing this is cheating!) \"\"\"\n",
    "    return np.log(2*np.pi*sig2)/2\n",
    "\n",
    "phi =   np.array([0.0, 0.0, 1.0])\n",
    "omega = np.array([0.0, 1.0])\n",
    "psi =   np.array([0.0, 1.0])\n",
    "pars = pack_pars(phi, omega, psi)\n",
    "print('pars \\n', pars)\n",
    "\n",
    "lr = 0.001\n",
    "lr_decay = 1.0\n",
    "b1,b2,e=0.9, 0.99, 1e-8\n",
    "m,v = np.zeros(pars.size),np.ones(pars.size)\n",
    "batch_size = 1000\n",
    "\n",
    "#print(sig2)\n",
    "for i in range(10000):    \n",
    "\n",
    "    _, _, psi = unpack_pars(pars)    \n",
    "    # draw from proposal\n",
    "    th = psi[0] + psi[1] * np.random.normal(size=batch_size)\n",
    "    # draw from simulator\n",
    "    x  = th + np.sqrt(sig2)* np.random.normal(size=batch_size)\n",
    "    \n",
    "    if np.any(np.isnan(x)):\n",
    "        break\n",
    "    lr *= lr_decay\n",
    "        \n",
    "    g = grad( (th, x), pars, x0, hX, lambd)\n",
    "    # selectively clamp parameters:\n",
    "    #g[:3] *= 0   # clamp phi\n",
    "    #g[3:-2] *= 0  # clamp omega\n",
    "    #g[-2:] *= 0   # clamp psi\n",
    "    \n",
    "    lr *= lr_decay\n",
    "    pars, m, v = adam_step(pars,g,m,v,lr,b1,b2,e,i+1)    \n",
    "\n",
    "phi, omega, psi = unpack_pars(pars)       \n",
    "psi[1] = psi[1]**2\n",
    "print('phi \\n', phi)\n",
    "print('omega \\n', omega)\n",
    "print('psi \\n', psi)\n",
    "\n",
    "post_ = np.array([phi[1] * x0 + phi[0], phi[2]])\n",
    "post = analytic_div(post_.reshape(1,1,2), eta2, (psi[0],), (psi[1],))\n",
    "print('est. post \\n', post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_params = 1\n",
    "pars = np.array([0.,0.,1., 0., 1., 0., 1.])\n",
    "\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=np.zeros(n_params), \n",
    "                S=np.eye(n_params))    \n",
    "s = ds.Identity()\n",
    "g_ = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "lambd = 0.01\n",
    "batch_size = 10\n",
    "\n",
    "lr = 1e-2\n",
    "lr_decay = 0.995\n",
    "for i in range(1000):\n",
    "\n",
    "    phi, omega, psi = unpack_pars(pars)\n",
    "    g_.prior.m, g_.prior.S = psi[0], psi[1]\n",
    "    batch = g_.gen(batch_size, verbose=False)    \n",
    "    \n",
    "    lr *= lr_decay    \n",
    "    pars -= lr * grad( batch, pars, hX, lambd=lambd)\n",
    "    #pars[-2], pars[-1] = 0, 1 \n",
    "\n",
    "phi, omega, psi = unpack_pars(pars)\n",
    "print('MDN phi \\n', phi)\n",
    "print('marginal omega \\n', omega)\n",
    "print('proposal psi \\n', psi)\n",
    "\n",
    "out = np.array([phi[0] + phi[1] * x0, phi[2]])\n",
    "analytic_div(out.reshape(1,1,2), eta2, (psi[0],), (psi[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.998**1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_params = 1\n",
    "pars = np.array([0.,0.,1., 0., 1., 0., 1.])\n",
    "\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=np.zeros(n_params), \n",
    "                S=np.eye(n_params))    \n",
    "s = ds.Identity()\n",
    "g_ = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "lambd = 0.01\n",
    "\n",
    "batch_size = 10\n",
    "m, v = np.zeros_like(pars), np.ones_like(pars)\n",
    "\n",
    "a=0.01 \n",
    "b1=0.9\n",
    "b2=0.99\n",
    "e=1e-8\n",
    "a_decay = 0.999\n",
    "\n",
    "for i in range(5000):\n",
    "        \n",
    "    phi, omega, psi = unpack_pars(pars)\n",
    "    g_.prior.m, g_.prior.S = psi[0], psi[1]\n",
    "    batch = g_.gen(batch_size, verbose=False)\n",
    "    \n",
    "    g = grad( batch, pars, hX, lambd=lambd)\n",
    "    a *= a_decay\n",
    "    pars, m, v = adam_step(pars,g,m,v,a,b1,b2,e,i+1)\n",
    "    \n",
    "phi, omega, psi = unpack_pars(pars)\n",
    "print(phi)\n",
    "print(omega)\n",
    "print(psi)\n",
    "\n",
    "out = np.array([phi[0] + phi[1] * x0, phi[2]])\n",
    "analytic_div(out.reshape(1,1,2), eta2, (psi[0],), (psi[1],))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
