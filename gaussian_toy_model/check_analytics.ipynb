{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analytical approach\n",
    "\n",
    "- analytically tractable problem: Gaussian prior, Gaussian proposal, _linear-Gaussian likelihood_\n",
    "- analytically tractable MDN: linear-affine network\n",
    "- analytically tractable gradients and closed-form solution for MDN parameters for given dataset\n",
    "\n",
    "\n",
    "TO DO:\n",
    "- check gradients again (and again and again...)\n",
    "- analytical division for actual CDELDI posterior estimates (vs. 'proposal-posterior' estimates atm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "prior: \n",
    "\n",
    "$p(\\theta) = \\mathcal{N}(\\theta \\ | \\ 0, \\eta^2)$\n",
    "\n",
    "proposal prior: \n",
    "\n",
    "$\\tilde{p}(\\theta) = \\mathcal{N}(\\theta \\ | \\ \\nu, \\xi^2)$\n",
    "\n",
    "simulator: \n",
    "\n",
    "$p(x \\ | \\ \\theta) =  \\mathcal{N}(x \\ | \\ \\theta, \\sigma^2)$\n",
    "\n",
    "analytic posteriors: \n",
    "\n",
    "$p(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\eta^2}{\\eta^2 + \\sigma^2} x, \\eta^2 - \\frac{\\eta^4}{\\eta^2 + \\sigma^2})$ \n",
    "\n",
    "$\\tilde{p}(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\xi^2}{\\xi^2 + \\sigma^2} x + \\frac{\\sigma^2}{\\xi^2 + \\sigma^2} \\nu, \\xi^2 - \\frac{\\xi^4}{\\xi^2 + \\sigma^2})$\n",
    "\n",
    "Data:\n",
    "\n",
    "$(x_n, \\theta_n) \\sim p(\\theta) p(x \\ | \\ \\theta) = \\mathcal{N}( (x_n, \\theta_n) \\ | \\ (0, \\nu), \n",
    "\\begin{pmatrix}\n",
    "\\xi^{2} + \\sigma^{2} &  \\xi^{2}  \\\\\n",
    "\\xi^{2} & \\xi^{2}  \\\\\n",
    "\\end{pmatrix})$\n",
    "\n",
    "Loss: \n",
    "\n",
    "$ \\mathcal{L}(\\phi) = \\sum_n \\frac{{p}(\\theta_n)}{\\tilde{p}(\\theta_n)} K_\\epsilon(x_n | x_0) \\ \\log q_\\phi(\\theta_n | x_n)$\n",
    "\n",
    "Model: \n",
    "\n",
    "$ q_\\phi(\\theta_n | x_n) = \\mathcal{N}(\\theta_n \\ | \\ \\mu_\\phi(x_n), \\sigma^2_\\phi(x_n))$\n",
    "\n",
    "$ (\\mu_\\phi(x), \\sigma^2_\\phi(x)) = MDN_\\phi(x) = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} x + \\begin{pmatrix} \\alpha \\\\ \\gamma^2 \\end{pmatrix}$\n",
    "\n",
    "Gradients: \n",
    "\n",
    "$\\mathcal{N}_n := \\mathcal{N}(x_n, \\theta_n \\ | \\ \\mu_y, \\Sigma_y)$\n",
    "\n",
    "$\\Sigma_y = \n",
    "\\begin{pmatrix}\n",
    "\\epsilon^2  &  0  \\\\\n",
    "0 & \\left( \\eta^{-2} - \\xi^{-2} \\right)^{-1}  \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "\n",
    "$\\mu_y = \\begin{pmatrix} x_0  \\\\ \\frac{\\eta^2}{\\eta^2 + \\xi^2}\\nu \\end{pmatrix}$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\alpha} = -2 \\sum_n \\mathcal{N}_n \\frac{\\theta_n - \\mu_\\phi(x_n)}{\\sigma^2_\\phi(x_n)}$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\beta} = -2 \\sum_n \\mathcal{N}_n \\frac{\\theta_n - \\mu_\\phi(x_n)}{\\sigma^2_\\phi(x_n)} x_n$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\gamma^2} \n",
    "= \\sum_n \\mathcal{N}_n \\left( \\frac{1}{\\sigma^2_\\phi(x_n)} \n",
    "- \\frac{\\left(\\theta_n - \\mu_\\phi(x_n) \\right)^2}{\\sigma^4_\\phi(x_n)} \\right) \n",
    "= \\frac{1}{\\gamma^2} \\sum_n \\mathcal{N}_n \\left( 1 \n",
    "- \\frac{\\left(\\theta_n - \\mu_\\phi(x_n) \\right)^2}{\\gamma^2} \\right) $\n",
    "\n",
    "Optima: \n",
    "\n",
    "$\\hat{\\alpha} = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\left(\\theta_n - \\frac{\\sum_m \\mathcal{N}_m \\theta_m x_m}{\\sum_m \\mathcal{N}_m x_m^2} x_n \\right)}{\\sum_n \\mathcal{N}_n - \\frac{\\left( \\sum_n \\mathcal{N}_n x_n \\right)^2}{\\sum_n \\mathcal{N}_n x_n^2}}$\n",
    "\n",
    "$\\hat{\\beta} = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\theta_n x_n - \\hat{\\alpha} \\sum_n \\mathcal{N}_n x_n}{\\sum_n \\mathcal{N}_n x_n^2}$\n",
    "\n",
    "$\\hat{\\gamma}^2 = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\left( \\theta_n - \\hat{\\alpha} - \\hat{\\beta} x_n \\right)^2}{\\sum_n \\mathcal{N}_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.summarystats as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "\n",
    "from delfi.simulator.Gauss import Gauss\n",
    "    \n",
    "def gauss_weights(params, stats, mu_y, Sig_y):\n",
    "    \n",
    "    y = np.hstack((stats, params))\n",
    "    return mvn.pdf(x=y, mean=mu_y.reshape(-1), cov=Sig_y, allow_singular=True)   \n",
    "\n",
    "\n",
    "def gauss_weights_eps0(params, stats, mu_y, Sig_y):\n",
    "    \"\"\" stable version in case eps^2 is giant - stats.mvn return nonsense here \"\"\"\n",
    "    # Note: making use of the fact that covariances are zero for normal weights in SNPE/CDELFI MLE solutions\n",
    "    \n",
    "    x = -0.5 * (params-mu_y[1])**2 / Sig_y[1,1] # would like to use mvn.pdf, but that one freaks  \n",
    "    return np.exp( x.reshape(-1) )              # out for 1D problems with negative (co-)variance\n",
    "\n",
    "\n",
    "def sel_gauss_implementation(eps2, thresh=1000): \n",
    "    \n",
    "    return gauss_weights_eps0 if eps2 > thresh else gauss_weights\n",
    "\n",
    "\n",
    "#def studentT_weights(params, stats, mu_y, Sig_y):\n",
    "#    \n",
    "#    raise NotImplementedError\n",
    "#    \n",
    "#    \n",
    "#def studentT_weights_eps0(params, stats, mu_y, Sig_y, df=3):\n",
    "#    \"\"\" stable version in case eps^2 is giant - stats.mvn return nonsense here \"\"\"\n",
    "#    # Note: making use of the fact that covariances are zero for normal weights in SNPE/CDELFI MLE solutions\n",
    "#    \n",
    "#    exponent = -(df+1)/2 \n",
    "#    return (1 + (params-mu_y[1])**2/(df*Sig_y[1,1]))**exponent\n",
    "#\n",
    "#\n",
    "#def sel_studentT_implementation(eps2, thresh=1000): \n",
    "#    \n",
    "#    return studentT_weights_eps0 if eps2 > thresh else studentT_weights\n",
    "#\n",
    "\n",
    "def get_weights_fun(eps2, thresh=1000, proposal_form='normal'):\n",
    "    \n",
    "    if proposal_form=='normal':\n",
    "        selector = sel_gauss_implementation \n",
    "    elif proposal_form=='studentT':\n",
    "        selector = sel_studentT_implementation \n",
    "        \n",
    "    return selector(eps2, thresh)\n",
    "\n",
    "def get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=3): \n",
    "    \n",
    "    assert proposal_form in ('normal', 'studentT')\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        \n",
    "        eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "        Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "        mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "        comp_weights =get_weights_fun(eps2, thresh=1000, proposal_form=proposal_form)\n",
    "        \n",
    "        normals = comp_weights(data[0], data[1], mu_y, Sig_y)\n",
    "        \n",
    "    if proposal_form == 'studentT':\n",
    "\n",
    "        exponent = -(df+1)/2 \n",
    "        proposal_pdf = (1 + (params-nu)**2/(df*ksi2))**exponent\n",
    "        prior_pdf    = mvn.pdf(x=params, mean=0., cov=eta2)\n",
    "        normals = prior_pdf / proposal_pdf\n",
    "        if eps2 < 1000:\n",
    "            calibration_kernel_pdf = mvn.pdf(x=stats, mean=x0, cov=eps2)\n",
    "            normals *= calibration_kernel_pdf\n",
    "            \n",
    "    return normals\n",
    "\n",
    "\n",
    "\n",
    "def dL_dalpha( params, stats, normals, beta, gamma2, alphas):\n",
    "\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2 * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "\n",
    "def dL_dbeta( params, stats, normals, alpha, gamma2, betas):\n",
    "\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2).sum(axis=0)\n",
    "\n",
    "def dL_dgamma2( params, stats, normals, alpha, beta, gamma2s):\n",
    "\n",
    "    tmp = (params.reshape(-1,1) - beta*stats.reshape(-1,1) - alpha)**2 / gamma2s.reshape(1,-1)\n",
    "    return 1/gamma2s.reshape(1,-1) * (normals.reshape(-1,1) * (1 - out)).sum(axis=0)\n",
    "    \n",
    "def alpha(params, stats, normals):\n",
    "    \n",
    "    N = normals.size    \n",
    "\n",
    "    Eo  = (normals * params).sum()\n",
    "    Eox = (normals * stats * params).sum()\n",
    "    Ex2 = (normals * stats**2).sum()\n",
    "    Ex  = (normals * stats).sum()\n",
    "    E1  = normals.sum()\n",
    "    \n",
    "    #ahat = (normals * (Ex2 * params - Eox * stats)).sum()\n",
    "    #ahat /= (E1 * Ex2 - Ex**2)\n",
    "    \n",
    "    ahat = (Eo - Eox/Ex2 * Ex) / (E1 - Ex**2/Ex2)\n",
    "    \n",
    "    return ahat\n",
    "\n",
    "def beta(params, stats, normals, ahat=None):\n",
    "\n",
    "    ahat = alpha(params, stats, normals) if ahat is None else ahat\n",
    "\n",
    "    Eox = (normals * stats * params).sum()\n",
    "    Ex2 = (normals * stats**2).sum()\n",
    "    Ex  = (normals * stats).sum()\n",
    "    \n",
    "    bhat = (Eox - ahat * Ex) / Ex2\n",
    "    \n",
    "    return bhat\n",
    "    \n",
    "def gamma2(params, stats, normals, ahat=None, bhat=None):\n",
    "\n",
    "    ahat = alpha(params, stats, normals) if ahat is None else ahat\n",
    "    bhat = beta(params, stats, normals, ahat) if bhat is None else bhat\n",
    "\n",
    "    gamma2hat = (normals*(params - ahat - bhat * stats )**2).sum() / normals.sum()\n",
    "    \n",
    "    return gamma2hat\n",
    "\n",
    "\n",
    "def analytic_div(out, eta2, nus, ksi2s):\n",
    "    \"\"\" analytic correction of onedimensional Gaussians for proposal priors\"\"\"\n",
    "    # assumes true prior to have zero mean!\n",
    "    # INPUTS:\n",
    "    # - out: 3D-tensor: \n",
    "    #        1st axis gives ksi2s (proposal variances), \n",
    "    #        2nd axis gives number of experiments/runs/fits\n",
    "    #        3nd axis is size 2: out[i,j,0] Gaussian mean, out[i,j,0] Gaussian variance\n",
    "    # - eta2:  prior variance (scalar)\n",
    "    # - nus:   vector of proposal prior means\n",
    "    # - ksi2s: vector of proposal prior variances\n",
    "    \n",
    "    # OUTPUTS\n",
    "    # - out_: 3D tensor of proposal-corrected posterior means and variances\n",
    "    \n",
    "    out_ = np.empty_like(out)\n",
    "    for i in range(out_.shape[0]):\n",
    "        \n",
    "        # precision and precision*mean\n",
    "        P = 1/out[i,:,1]\n",
    "        Pm = P * out[i,:,0]\n",
    "\n",
    "        # multiply with prior\n",
    "        P = P + 1/eta2\n",
    "        Pm = Pm + 0/eta2\n",
    "\n",
    "        # divide by proposal\n",
    "        P = P - 1/ksi2s[i]\n",
    "        Pm = Pm - nu/ksi2s[i]\n",
    "\n",
    "        out_[i,:,:] = np.vstack((Pm/P, 1/P)).T\n",
    "\n",
    "    return out_\n",
    "\n",
    "n_bins = 50 # number of bins for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define problem setup\n",
    "\n",
    "- proposals narrower than the priors seem to induce some form of bias on MLE estimates (non-SVI fits), especially for posterior variances\n",
    "- up to a certain degree, this can be negated by switching to students-t proposals (with few degrees of freedom)\n",
    "- apart from bias, even switching to students-t distributions results in increased variance of the estimates (probably some ESS thing?)\n",
    "\n",
    "\n",
    "- generally speaking, the real challenge for SNPE arises when $\\sigma^2 << \\eta^2$, i.e. when the likelihood (and hence posterior) are much narrower than the prior\n",
    "- in a multi-round SNPE fit, the proposal will tend to be much narrower than the prior after several rounds\n",
    "- the 'relevant' (relative) proposal width thus is the one that corresponds to the posterior variance\n",
    "\n",
    "\n",
    "- for a simple setup that can be largely 'fixed' with students-t proposals choose $\\sigma^2 = \\eta^2$.\n",
    "\n",
    "- intermediate setup:  $\\sigma^2 = 9 \\cdot \\eta^2$ (i.e. posterior variance is $10\\%$ of prior variance)\n",
    "\n",
    "- to watch Rome burn, try  $\\sigma^2 = 99 \\cdot \\eta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## problem setup ##\n",
    "\n",
    "n_params = 1\n",
    "\n",
    "assert n_params == 1 # cannot be overstressed: everything in this notebook goes downhill sharply otherwise\n",
    "\n",
    "sig2 = 1.0/9. # likelihood variance\n",
    "eta2 = 1.0     # prior variance\n",
    "eps2 = 1e20    # calibration kernel width (everything above a certain threshold will be treated as 'uniform')\n",
    "\n",
    "# pick observed summary statistics\n",
    "x0 = 0.8 * np.ones(1) #_,obs = g.gen(1) \n",
    "\n",
    "# prior and analytic (!) likelihood & posterior\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))\n",
    "post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0[0], \n",
    "                     S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params))    \n",
    "\n",
    "\n",
    "\n",
    "## simulation setup ##\n",
    "\n",
    "n_fits = 1000  # number of MLE fits (i.e. dataset draws), each single-round fits with pre-specified proposal!\n",
    "N      = 500  # number of simulations per dataset\n",
    "\n",
    "# set proposal priors (one per experiment)\n",
    "ksi2s = np.array([0.01, 0.1, 0.5, 0.999]) * eta2  # proposal variance\n",
    "nus = 0. * np.ones(len(ksi2s))              # proposal mean\n",
    "\n",
    "\n",
    "res = {'normal' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       't_df10' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       't_df3'  : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       'cdelfi' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       'sig2' : sig2,\n",
    "       'eta2' : eta2,\n",
    "       'eps2' : eps2,\n",
    "       'ksi2s' : ksi2s,\n",
    "       'nus' : nus,\n",
    "       'x0' : x0,\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNPE (Gaussian proposals) \n",
    "## multiple fits, with different proposal widths\n",
    "- n_fits many runs, every run with different seed, i.e. different data set $\\{(x_n, \\theta_n)\\}_{n=1}^N$\n",
    "- collect statistics on mean and std. of estimated posterior mean and variance (bias?)\n",
    "- outer loop over different proposal distribution variances (can in principle also move means)\n",
    "  \n",
    "Keep in mind that for large enough $N$, in theory SNPE should return ground-truth posteriors irrespective of the proposal prior (cf. Kaan's proof in JakobsNotes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proposal_form = 'normal'\n",
    "df = None\n",
    "\n",
    "out_snpe  = res[proposal_form]\n",
    "\n",
    "track_rp = True # track real posterior: if False, will compare with 'proposal-posterior'\n",
    "plt.figure(figsize=(4*len(ksi2s),8))\n",
    "m_m, m_v, M_m, M_v, hh_m, hh_v = np.inf,np.inf,-np.inf,-np.inf,-np.inf,-np.inf\n",
    "\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    \n",
    "    nu, ksi2 = nus[i], ksi2s[i]\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                        S=ksi2 * np.eye(n_params))\n",
    "    elif proposal_form == 'studentT':\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params),\n",
    "                           dof=df)    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0[0] + sig2/(ksi2+sig2)*nu), \n",
    "                         S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "    \n",
    "    eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "    Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "    mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "    s = ds.Identity()\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    pbar = progressbar(total=n_fits)\n",
    "    desc = 'repeated fits'\n",
    "    pbar.set_description(desc)\n",
    "    with pbar:\n",
    "        for idx_seed in range(n_fits):\n",
    "\n",
    "            #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "            seed = 42 + idx_seed\n",
    "            g.model.seed = seed\n",
    "            g.prior.seed = seed\n",
    "            g.seed = seed\n",
    "\n",
    "            data = g.gen(N, verbose=False)\n",
    "            params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "            #normals = comp_weights(data[0], data[1], mu_y, Sig_y) if track_rp else np.ones(N)/N\n",
    "            normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n",
    "            ahat =       alpha(params, stats, normals)\n",
    "            bhat =        beta(params, stats, normals, ahat)\n",
    "            gamma2hat = gamma2(params, stats, normals, ahat, bhat)\n",
    "\n",
    "            mu_hat   = ahat + bhat * x0\n",
    "            sig2_hat = gamma2hat\n",
    "\n",
    "            out_snpe[i,idx_seed,:] = (mu_hat, sig2_hat)\n",
    "            pbar.update(1)\n",
    "\n",
    "    post_disp = post if track_rp else postpr\n",
    "\n",
    "    plt.subplot(len(ksi2s), 2, 2*i+1)\n",
    "    m_m, M_m = np.min((m_m, out_snpe[i,:,0].min())), np.max((M_m, out_snpe[i,:,0].max()))\n",
    "    plt.hist(out_snpe[i,:,0], bins=np.linspace(m_m, M_m, n_bins), normed=True)\n",
    "    hh_m = np.max((hh_m, plt.axis()[3]))\n",
    "    plt.plot([post_disp.mean, post_disp.mean], [0, hh_m], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,-1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([0,0]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([1,1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,1]), [ hh_m/2, hh_m/2], 'g', linewidth=2)\n",
    "    plt.ylabel('xi^2/eta^2 = ' + str(ksi2/eta2) )\n",
    "    \n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    m_v, M_v = np.min((m_v, out_snpe[i,:,1].min())), np.max((M_v, out_snpe[i,:,1].max()))\n",
    "    plt.hist(out_snpe[i,:,1], bins=np.linspace(m_v, M_v, n_bins), normed=True)\n",
    "    hh_v = np.max((hh_v, plt.axis()[3]))\n",
    "    plt.plot([post_disp.std**2, post_disp.std**2], [0, hh_v], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,-1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([0,0]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([1,1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,1]), [ hh_v/2, hh_v/2], 'g', linewidth=2)\n",
    "    #plt.ylabel('posterior variance')\n",
    "\n",
    "\n",
    "plt.subplot(len(ksi2s),2,1)\n",
    "plt.title('posterior mean')\n",
    "plt.subplot(len(ksi2s),2,2)\n",
    "plt.title('posterior variance')\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    plt.subplot(len(ksi2s),2, 2*i+1)\n",
    "    plt.axis([m_m, M_m, 0, hh_m])\n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    plt.axis([m_v, M_v, 0, hh_v])\n",
    "    \n",
    "plt.savefig('example_hist_SNPE_normal_1000fits.pdf')    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNPE (students T proposals, df = 10) \n",
    "## multiple fits, with different proposal widths\n",
    "- same as before, but with students T proposals instead of Gaussian proposals (same mean and variance)\n",
    "- df $=10$ in 1D : roughly Gaussian, but with slightly stronger tails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 10\n",
    "\n",
    "out_snpe  = res['t_df' + str(df)]\n",
    "\n",
    "track_rp = True # track real posterior: if False, will compare with 'proposal-posterior'\n",
    "plt.figure(figsize=(4*len(ksi2s),8))\n",
    "m_m, m_v, M_m, M_v, hh_m, hh_v = np.inf,np.inf,-np.inf,-np.inf,-np.inf,-np.inf\n",
    "\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    \n",
    "    nu, ksi2 = nus[i], ksi2s[i]\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                        S=ksi2 * np.eye(n_params))\n",
    "    elif proposal_form == 'studentT':\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params),\n",
    "                           dof=df)    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0[0] + sig2/(ksi2+sig2)*nu), \n",
    "                         S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "    \n",
    "    eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "    Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "    mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "    s = ds.Identity()\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    pbar = progressbar(total=n_fits)\n",
    "    desc = 'repeated fits'\n",
    "    pbar.set_description(desc)\n",
    "    with pbar:\n",
    "        for idx_seed in range(n_fits):\n",
    "\n",
    "            #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "            seed = 42 + idx_seed\n",
    "            g.model.seed = seed\n",
    "            g.prior.seed = seed\n",
    "            g.seed = seed\n",
    "\n",
    "            data = g.gen(N, verbose=False)\n",
    "            params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "            #normals = comp_weights(data[0], data[1], mu_y, Sig_y) if track_rp else np.ones(N)/N\n",
    "            normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n",
    "            ahat =       alpha(params, stats, normals)\n",
    "            bhat =        beta(params, stats, normals, ahat)\n",
    "            gamma2hat = gamma2(params, stats, normals, ahat, bhat)\n",
    "\n",
    "            mu_hat   = ahat + bhat * x0\n",
    "            sig2_hat = gamma2hat\n",
    "\n",
    "            out_snpe[i,idx_seed,:] = (mu_hat, sig2_hat)\n",
    "            pbar.update(1)\n",
    "\n",
    "    post_disp = post if track_rp else postpr\n",
    "\n",
    "    plt.subplot(len(ksi2s), 2, 2*i+1)\n",
    "    m_m, M_m = np.min((m_m, out_snpe[i,:,0].min())), np.max((M_m, out_snpe[i,:,0].max()))\n",
    "    plt.hist(out_snpe[i,:,0], bins=np.linspace(m_m, M_m, n_bins), normed=True)\n",
    "    hh_m = np.max((hh_m, plt.axis()[3]))\n",
    "    plt.plot([post_disp.mean, post_disp.mean], [0, hh_m], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,-1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([0,0]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([1,1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,1]), [ hh_m/2, hh_m/2], 'g', linewidth=2)\n",
    "    plt.ylabel('xi^2/eta^2 = ' + str(ksi2/eta2) )\n",
    "    \n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    m_v, M_v = np.min((m_v, out_snpe[i,:,1].min())), np.max((M_v, out_snpe[i,:,1].max()))\n",
    "    plt.hist(out_snpe[i,:,1], bins=np.linspace(m_v, M_v, n_bins), normed=True)\n",
    "    hh_v = np.max((hh_v, plt.axis()[3]))\n",
    "    plt.plot([post_disp.std**2, post_disp.std**2], [0, hh_v], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,-1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([0,0]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([1,1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,1]), [ hh_v/2, hh_v/2], 'g', linewidth=2)\n",
    "    #plt.ylabel('posterior variance')\n",
    "\n",
    "\n",
    "plt.subplot(len(ksi2s),2,1)\n",
    "plt.title('posterior mean')\n",
    "plt.subplot(len(ksi2s),2,2)\n",
    "plt.title('posterior variance')\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    plt.subplot(len(ksi2s),2, 2*i+1)\n",
    "    plt.axis([m_m, M_m, 0, hh_m])\n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    plt.axis([m_v, M_v, 0, hh_v])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNPE (students T proposals, df = 3) \n",
    "## multiple fits, with different proposal widths\n",
    "- same as before, but with students T proposals instead of Gaussian proposals (same mean and variance)\n",
    "- df $=3$ in 1D : much stronger tails than Gaussian (need df>2 for well-defined student-T variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 3\n",
    "\n",
    "out_snpe  = res['t_df' + str(df)]\n",
    "\n",
    "track_rp = True # track real posterior: if False, will compare with 'proposal-posterior'\n",
    "plt.figure(figsize=(4*len(ksi2s),8))\n",
    "m_m, m_v, M_m, M_v, hh_m, hh_v = np.inf,np.inf,-np.inf,-np.inf,-np.inf,-np.inf\n",
    "\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    \n",
    "    nu, ksi2 = nus[i], ksi2s[i]\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                        S=ksi2 * np.eye(n_params))\n",
    "    elif proposal_form == 'studentT':\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params),\n",
    "                           dof=df)    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0[0] + sig2/(ksi2+sig2)*nu), \n",
    "                         S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "    \n",
    "    eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "    Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "    mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "    s = ds.Identity()\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    pbar = progressbar(total=n_fits)\n",
    "    desc = 'repeated fits'\n",
    "    pbar.set_description(desc)\n",
    "    with pbar:\n",
    "        for idx_seed in range(n_fits):\n",
    "\n",
    "            #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "            seed = 42 + idx_seed\n",
    "            g.model.seed = seed\n",
    "            g.prior.seed = seed\n",
    "            g.seed = seed\n",
    "\n",
    "            data = g.gen(N, verbose=False)\n",
    "            params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "            #normals = comp_weights(data[0], data[1], mu_y, Sig_y) if track_rp else np.ones(N)/N\n",
    "            normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n",
    "            ahat =       alpha(params, stats, normals)\n",
    "            bhat =        beta(params, stats, normals, ahat)\n",
    "            gamma2hat = gamma2(params, stats, normals, ahat, bhat)\n",
    "\n",
    "            mu_hat   = ahat + bhat * x0\n",
    "            sig2_hat = gamma2hat\n",
    "\n",
    "            out_snpe[i,idx_seed,:] = (mu_hat, sig2_hat)\n",
    "            pbar.update(1)\n",
    "\n",
    "    post_disp = post if track_rp else postpr\n",
    "\n",
    "    plt.subplot(len(ksi2s), 2, 2*i+1)\n",
    "    m_m, M_m = np.min((m_m, out_snpe[i,:,0].min())), np.max((M_m, out_snpe[i,:,0].max()))\n",
    "    plt.hist(out_snpe[i,:,0], bins=np.linspace(m_m, M_m, n_bins), normed=True)\n",
    "    hh_m = np.max((hh_m, plt.axis()[3]))\n",
    "    plt.plot([post_disp.mean, post_disp.mean], [0, hh_m], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,-1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([0,0]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([1,1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,1]), [ hh_m/2, hh_m/2], 'g', linewidth=2)\n",
    "    plt.ylabel('xi^2/eta^2 = ' + str(ksi2/eta2) )\n",
    "    \n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    m_v, M_v = np.min((m_v, out_snpe[i,:,1].min())), np.max((M_v, out_snpe[i,:,1].max()))\n",
    "    plt.hist(out_snpe[i,:,1], bins=np.linspace(m_v, M_v, n_bins), normed=True)\n",
    "    hh_v = np.max((hh_v, plt.axis()[3]))\n",
    "    plt.plot([post_disp.std**2, post_disp.std**2], [0, hh_v], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,-1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([0,0]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([1,1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,1]), [ hh_v/2, hh_v/2], 'g', linewidth=2)\n",
    "    #plt.ylabel('posterior variance')\n",
    "\n",
    "\n",
    "plt.subplot(len(ksi2s),2,1)\n",
    "plt.title('posterior mean')\n",
    "plt.subplot(len(ksi2s),2,2)\n",
    "plt.title('posterior variance')\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    plt.subplot(len(ksi2s),2, 2*i+1)\n",
    "    plt.axis([m_m, M_m, 0, hh_m])\n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    plt.axis([m_v, M_v, 0, hh_v])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with CDELFI\n",
    "- simply set track_rp = False to compare MLE solution with mean and variance of proposal-posterior\n",
    "- however requires Gaussian proposals to directly compare with proposal-posterior\n",
    "- if the proposal-posterior is good, so should be the prior-corrected result of the analytical division\n",
    "- todo: import and use code for analyical division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proposal_form = 'normal'\n",
    "df = None\n",
    "\n",
    "out_snpe  = res['cdelfi']\n",
    "\n",
    "track_rp = False # track real posterior: if False, will compare with 'proposal-posterior'\n",
    "plt.figure(figsize=(4*len(ksi2s),8))\n",
    "m_m, m_v, M_m, M_v, hh_m, hh_v = np.inf,np.inf,-np.inf,-np.inf,-np.inf,-np.inf\n",
    "\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    \n",
    "    nu, ksi2 = nus[i], ksi2s[i]\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                        S=ksi2 * np.eye(n_params))\n",
    "    elif proposal_form == 'studentT':\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params),\n",
    "                           dof=df)    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0[0] + sig2/(ksi2+sig2)*nu), \n",
    "                         S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "    \n",
    "    eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "    Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "    mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "    s = ds.Identity()\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    pbar = progressbar(total=n_fits)\n",
    "    desc = 'repeated fits'\n",
    "    pbar.set_description(desc)\n",
    "    with pbar:\n",
    "        for idx_seed in range(n_fits):\n",
    "\n",
    "            #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "            seed = 42 + idx_seed\n",
    "            g.model.seed = seed\n",
    "            g.prior.seed = seed\n",
    "            g.seed = seed\n",
    "\n",
    "            data = g.gen(N, verbose=False)\n",
    "            params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "            #normals = comp_weights(data[0], data[1], mu_y, Sig_y) if track_rp else np.ones(N)/N\n",
    "            normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n",
    "            ahat =       alpha(params, stats, normals)\n",
    "            bhat =        beta(params, stats, normals, ahat)\n",
    "            gamma2hat = gamma2(params, stats, normals, ahat, bhat)\n",
    "\n",
    "            mu_hat   = ahat + bhat * x0\n",
    "            sig2_hat = gamma2hat\n",
    "\n",
    "            out_snpe[i,idx_seed,:] = (mu_hat, sig2_hat)\n",
    "            pbar.update(1)\n",
    "\n",
    "    post_disp = post if track_rp else postpr\n",
    "\n",
    "    plt.subplot(len(ksi2s), 2, 2*i+1)\n",
    "    m_m, M_m = np.min((m_m, out_snpe[i,:,0].min())), np.max((M_m, out_snpe[i,:,0].max()))\n",
    "    plt.hist(out_snpe[i,:,0], bins=np.linspace(m_m, M_m, n_bins), normed=True)\n",
    "    hh_m = np.max((hh_m, plt.axis()[3]))\n",
    "    plt.plot([post_disp.mean, post_disp.mean], [0, hh_m], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,-1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([0,0]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([1,1]), [0, hh_m/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,0].mean() + out_snpe[i,:,0].std()*np.array([-1,1]), [ hh_m/2, hh_m/2], 'g', linewidth=2)\n",
    "    plt.ylabel('xi^2/eta^2 = ' + str(ksi2/eta2) )\n",
    "    \n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    m_v, M_v = np.min((m_v, out_snpe[i,:,1].min())), np.max((M_v, out_snpe[i,:,1].max()))\n",
    "    plt.hist(out_snpe[i,:,1], bins=np.linspace(m_v, M_v, n_bins), normed=True)\n",
    "    hh_v = np.max((hh_v, plt.axis()[3]))\n",
    "    plt.plot([post_disp.std**2, post_disp.std**2], [0, hh_v], 'r', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,-1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([0,0]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([1,1]), [0, hh_v/2], 'g', linewidth=2)\n",
    "    plt.plot(out_snpe[i,:,1].mean() + out_snpe[i,:,1].std()*np.array([-1,1]), [ hh_v/2, hh_v/2], 'g', linewidth=2)\n",
    "    #plt.ylabel('posterior variance')\n",
    "\n",
    "\n",
    "plt.subplot(len(ksi2s),2,1)\n",
    "plt.title('posterior mean')\n",
    "plt.subplot(len(ksi2s),2,2)\n",
    "plt.title('posterior variance')\n",
    "\n",
    "for i in range(len(ksi2s)):\n",
    "    plt.subplot(len(ksi2s),2, 2*i+1)\n",
    "    plt.axis([m_m, M_m, 0, hh_m])\n",
    "    plt.subplot(len(ksi2s),2, 2*i+2)\n",
    "    plt.axis([m_v, M_v, 0, hh_v])\n",
    "    \n",
    "plt.savefig('example_hist_CDELFI_normal_1000fits.pdf')    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('res_analytic_n_fits' + str(n_fits) + '_N' + str(N), res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Ns = [ 50, 100, 500, 1000, 5000 ]\n",
    "n_fits = 1000\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "algs = ['normal', 't_df10', 't_df3']\n",
    "\n",
    "#mkrs, algs = mkrs[:3], algs[:3]\n",
    "\n",
    "\n",
    "for i in range(len(algs)):\n",
    "    \n",
    "    alg = algs[i]\n",
    "    \n",
    "    \n",
    "    for n in range(len(Ns)):\n",
    "\n",
    "        N = Ns[n]\n",
    "        clr = clrs[n]\n",
    "        \n",
    "        tmp = np.load('res_analytic_n_fits' + str(n_fits) + '_N' + str(N)+'.npy')[()]        \n",
    "        out_snpe = tmp[alg]\n",
    "\n",
    "        plt.subplot(2,len(algs),i+1)\n",
    "\n",
    "        if alg == 'cdelfi' : \n",
    "            m = analytic_div(out_snpe, eta2, nu*np.ones_like(ksi2s), ksi2s)[:,:,0].mean(axis=1)\n",
    "        else:\n",
    "            m = out_snpe[:,:,0].mean(axis=1)\n",
    "        plt.semilogx(ksi2s, m, color=clr, linewidth=2)\n",
    "        plt.title(alg + ' proposal prior')\n",
    "        \n",
    "        plt.subplot(2,len(algs),i+1+len(algs))\n",
    "        \n",
    "        if alg == 'cdelfi' : \n",
    "            m = analytic_div(out_snpe, eta2, nu*np.ones_like(ksi2s), ksi2s)[:,:,1].mean(axis=1)\n",
    "        else:\n",
    "            m = out_snpe[:,:,1].mean(axis=1)\n",
    "        plt.semilogx(ksi2s, m, color=clr, linewidth=2)\n",
    "\n",
    "    for n in range(len(Ns)):\n",
    "\n",
    "        N = Ns[n]\n",
    "        clr = clrs[n]\n",
    "        \n",
    "        ksi2s = tmp['ksi2s']\n",
    "        eta2, sig2, x0 = tmp['eta2'], tmp['sig2'], tmp['x0']\n",
    "        \n",
    "        plt.subplot(2,len(algs),i+1)\n",
    "\n",
    "        gt = eta2/(eta2+sig2)*x0[0]*np.ones_like(ksi2s)    \n",
    "        plt.semilogx(ksi2s, gt*np.ones_like(ksi2s), 'k--', linewidth=2)\n",
    "        \n",
    "        plt.subplot(2,len(algs),i+1+len(algs))\n",
    "\n",
    "        gt = eta2 - eta2**2 / (eta2 + sig2)\n",
    "        plt.semilogx(ksi2s, gt*np.ones_like(ksi2s), 'k--', linewidth=2)\n",
    "        \n",
    "for i in range(len(algs)):\n",
    "    plt.subplot(2,len(algs),i+1)\n",
    "    plt.axis([ksi2s[0], ksi2s[-1], 0, 1.1*eta2/(eta2+sig2)*x0[0]])\n",
    "    plt.subplot(2,len(algs),i+1+len(algs))\n",
    "    plt.axis([ksi2s[0], ksi2s[-1], 0, 1.1*(eta2 - eta2**2 / (eta2 + sig2))])\n",
    "    plt.xlabel('proposal prior variance / prior variance') \n",
    "    \n",
    "plt.subplot(2,len(algs),1)\n",
    "plt.ylabel('posterior means')\n",
    "plt.legend(['N = ' + str(n) for n in Ns], loc=4)\n",
    "plt.subplot(2,len(algs),len(algs)+1)\n",
    "plt.ylabel('posterior variance')\n",
    "\n",
    "plt.savefig('bias_1000fits_SNPE.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimator standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Ns = [ 50, 100, 500, 1000, 5000 ]\n",
    "n_fits = 1000\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "algs = ['normal', 't_df10', 't_df3']\n",
    "\n",
    "#mkrs, algs = mkrs[:3], algs[:3]\n",
    "\n",
    "\n",
    "for i in range(len(algs)):\n",
    "    \n",
    "    alg = algs[i]\n",
    "    \n",
    "    for n in range(len(Ns)):\n",
    "\n",
    "        N = Ns[n]\n",
    "        clr = clrs[n]\n",
    "        \n",
    "        tmp = np.load('res_analytic_n_fits' + str(n_fits) + '_N' + str(N)+'.npy')[()]        \n",
    "        out_snpe = tmp[alg]\n",
    "\n",
    "        plt.subplot(2, len(algs),i+1)\n",
    "\n",
    "        if alg == 'cdelfi' : \n",
    "            s = analytic_div(out_snpe, eta2, nu*np.ones_like(ksi2s), ksi2s)[:,:,0].std(axis=1)\n",
    "        else:\n",
    "            s = out_snpe[:,:,0].std(axis=1)\n",
    "        plt.semilogx(ksi2s, s, color=clr, linewidth=2)\n",
    "        plt.title(alg + ' proposal prior')\n",
    "        \n",
    "        plt.subplot(2,len(algs),i+1+len(algs))\n",
    "        \n",
    "        if alg == 'cdelfi' : \n",
    "            s = analytic_div(out_snpe, eta2, nu*np.ones_like(ksi2s), ksi2s)[:,:,1].std(axis=1)\n",
    "        else:\n",
    "            s = out_snpe[:,:,1].std(axis=1)\n",
    "        plt.semilogx(ksi2s, s, color=clr, linewidth=2)\n",
    "\n",
    "        \n",
    "for i in range(len(algs)):\n",
    "    plt.subplot(2, len(algs), i+1)\n",
    "    plt.axis([ksi2s[0], ksi2s[-1], 0, 0.3])\n",
    "    plt.subplot(2,len(algs),len(algs)+1+i)\n",
    "    plt.axis([ksi2s[0], ksi2s[-1], 0, 0.033])\n",
    "    plt.xlabel('proposal prior variance / prior variance') \n",
    "    \n",
    "plt.subplot(2,len(algs),1)\n",
    "plt.ylabel('posterior means')\n",
    "plt.legend(['N = ' + str(n) for n in Ns], loc=1)\n",
    "plt.subplot(2,len(algs),len(algs)+1)\n",
    "plt.ylabel('posterior variance')\n",
    "\n",
    "plt.savefig('std_1000fits_SNPE.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another look at CDELFI in 'proposal prior'-space\n",
    "## Gaussian priors, no analytical division, no importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Ns = [ 50, 100, 500, 1000, 5000 ]\n",
    "n_fits = 1000\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "    \n",
    "for n in range(len(Ns)):\n",
    "\n",
    "    N = Ns[n]\n",
    "    clr = clrs[n]\n",
    "\n",
    "    tmp = np.load('res_analytic_n_fits' + str(n_fits) + '_N' + str(N)+'.npy')[()]        \n",
    "    out_snpe = tmp['cdelfi']\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "\n",
    "    m = out_snpe[:,:,0].mean(axis=1)\n",
    "    plt.semilogx(ksi2s, m, color=clr, linewidth=2)\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "\n",
    "    m = out_snpe[:,:,1].mean(axis=1)\n",
    "    plt.semilogx(ksi2s, m, color=clr, linewidth=2)\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "\n",
    "    s = out_snpe[:,:,0].std(axis=1)\n",
    "    plt.semilogx(ksi2s, s, color=clr, linewidth=2)\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    s = out_snpe[:,:,1].std(axis=1)\n",
    "    plt.semilogx(ksi2s, s, color=clr, linewidth=2)\n",
    "    \n",
    "    \n",
    "for n in range(len(Ns)):\n",
    "\n",
    "    N = Ns[n]\n",
    "    clr = clrs[n]\n",
    "\n",
    "    ksi2s = tmp['ksi2s']\n",
    "    eta2, sig2, x0 = tmp['eta2'], tmp['sig2'], tmp['x0']\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "\n",
    "    gt = ksi2s/(ksi2s+sig2)*x0[0]    \n",
    "    plt.semilogx(ksi2s, gt, 'k--', linewidth=2)\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "\n",
    "    gt = ksi2s - ksi2s**2 / (ksi2s + sig2)\n",
    "    plt.semilogx(ksi2s, gt, 'k--', linewidth=2)\n",
    "            \n",
    "plt.subplot(2,2,1)\n",
    "plt.ylabel('posterior means')\n",
    "plt.title('estimator avg')\n",
    "plt.legend(['N = ' + str(n) for n in Ns], loc=4)\n",
    "plt.subplot(2,2,3)\n",
    "plt.ylabel('posterior variance')\n",
    "plt.xlabel('proposal prior variance / prior variance')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('estimator std')\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel('proposal prior variance / prior variance') \n",
    "\n",
    "plt.savefig('bias_std_1000fits_CDELFI.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numberical checks for gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "track_rp = True\n",
    "proposal_form = 'normal'\n",
    "df = None\n",
    "\n",
    "nu = 0.\n",
    "ksi2 = 0.5 * eta2\n",
    "\n",
    "ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                S=ksi2 * np.eye(n_params))\n",
    "s = ds.Identity()\n",
    "g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "\n",
    "seed = 42\n",
    "g.model.seed = seed\n",
    "g.prior.seed = seed\n",
    "g.seed = seed\n",
    "\n",
    "data = g.gen(N, verbose=False)\n",
    "params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\alpha}$\n",
    "\n",
    "- $\\frac{\\partial}{\\partial\\alpha}$ is being difficult here. Analytic solution $\\hat{\\alpha}$ still fails to numberically set the stated partial derivative $\\frac{\\partial\\mathcal{L}}{\\partial{}\\alpha}(\\hat{\\alpha})$ to zero ...\n",
    "- Obtained $\\hat{\\alpha}$ however are pretty much sensible though (correct 'ballpark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_hat = np.array(alpha(params, stats, normals))\n",
    "\n",
    "gamma2_ = post.std**2\n",
    "alphas = np.linspace(-0.09, -0.02, 100000)\n",
    "\n",
    "beta_ = beta(params, stats, normals, alpha_hat)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alpha_hat)/gamma2_).sum(axis=0)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2_).sum(axis=0)\n",
    "plt.plot(alphas, out)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "alphas[np.argmin(np.abs(out))], alpha_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "gamma2_ = post.std**2\n",
    "betas = np.linspace(0., 1., 1000)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - betas.reshape(1,-1) * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "plt.plot(betas, out)\n",
    "plt.show()\n",
    "\n",
    "beta_hat = beta(params, stats, normals, ahat=alpha_)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_hat * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "betas[np.argmin(np.abs(out))], beta_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\gamma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "beta_ = beta_hat\n",
    "gamma2s = np.linspace(0.0009, 0.001, 1000)\n",
    "\n",
    "# something off with below (hard-coded...) gradients now. Outcommenting numerical solution for now!\n",
    "\n",
    "tmp = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2s.reshape(1,-1)\n",
    "out = 1/gamma2s.reshape(-1,) * (normals.reshape(-1,1) * (1 - tmp)).sum(axis=0)\n",
    "\n",
    "plt.plot(gamma2s, out)\n",
    "plt.show()\n",
    "\n",
    "gamma2_hat = gamma2(params, stats, normals, ahat=alpha_, bhat=beta_)\n",
    "tmp_ = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2_hat\n",
    "out_hat = 1/gamma2_hat * (normals.reshape(-1,1) * (1 - tmp_)).sum(axis=0)\n",
    "\n",
    "#gamma2s *= np.nan \n",
    "#out = np.zeros_like(gamma2s)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "gamma2s[np.argmin(np.abs(out))], gamma2_hat, out_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
