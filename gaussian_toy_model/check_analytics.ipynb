{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# analytical approach\n",
    "\n",
    "- analytically tractable problem: Gaussian prior, Gaussian proposal, _linear-Gaussian likelihood_\n",
    "- analytically tractable MDN: linear-affine network\n",
    "- analytically tractable gradients and closed-form solution for MDN parameters for given dataset\n",
    "\n",
    "\n",
    "TO DO:\n",
    "- check gradients again (and again and again...)\n",
    "- analytical division for actual CDELDI posterior estimates (vs. 'proposal-posterior' estimates atm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "prior: \n",
    "\n",
    "$p(\\theta) = \\mathcal{N}(\\theta \\ | \\ 0, \\eta^2)$\n",
    "\n",
    "proposal prior: \n",
    "\n",
    "$\\tilde{p}(\\theta) = \\mathcal{N}(\\theta \\ | \\ \\nu, \\xi^2)$\n",
    "\n",
    "simulator: \n",
    "\n",
    "$p(x \\ | \\ \\theta) =  \\mathcal{N}(x \\ | \\ \\theta, \\sigma^2)$\n",
    "\n",
    "analytic posteriors: \n",
    "\n",
    "$p(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\eta^2}{\\eta^2 + \\sigma^2} x, \\eta^2 - \\frac{\\eta^4}{\\eta^2 + \\sigma^2})$ \n",
    "\n",
    "$\\tilde{p}(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\xi^2}{\\xi^2 + \\sigma^2} x + \\frac{\\sigma^2}{\\xi^2 + \\sigma^2} \\nu, \\xi^2 - \\frac{\\xi^4}{\\xi^2 + \\sigma^2})$\n",
    "\n",
    "Data:\n",
    "\n",
    "$(x_n, \\theta_n) \\sim p(\\theta) p(x \\ | \\ \\theta) = \\mathcal{N}( (x_n, \\theta_n) \\ | \\ (0, \\nu), \n",
    "\\begin{pmatrix}\n",
    "\\xi^{2} + \\sigma^{2} &  \\xi^{2}  \\\\\n",
    "\\xi^{2} & \\xi^{2}  \\\\\n",
    "\\end{pmatrix})$\n",
    "\n",
    "Loss: \n",
    "\n",
    "$ \\mathcal{L}(\\phi) = \\sum_n \\frac{{p}(\\theta_n)}{\\tilde{p}(\\theta_n)} K_\\epsilon(x_n | x_0) \\ \\log q_\\phi(\\theta_n | x_n)$\n",
    "\n",
    "Model: \n",
    "\n",
    "$ q_\\phi(\\theta_n | x_n) = \\mathcal{N}(\\theta_n \\ | \\ \\mu_\\phi(x_n), \\sigma^2_\\phi(x_n))$\n",
    "\n",
    "$ (\\mu_\\phi(x), \\sigma^2_\\phi(x)) = MDN_\\phi(x) = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} x + \\begin{pmatrix} \\alpha \\\\ \\gamma^2 \\end{pmatrix}$\n",
    "\n",
    "Gradients: \n",
    "\n",
    "$\\mathcal{N}_n := \\mathcal{N}(x_n, \\theta_n \\ | \\ \\mu_y, \\Sigma_y)$\n",
    "\n",
    "$\\Sigma_y = \n",
    "\\begin{pmatrix}\n",
    "\\epsilon^2  &  0  \\\\\n",
    "0 & \\left( \\eta^{-2} - \\xi^{-2} \\right)^{-1}  \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "\n",
    "$\\mu_y = \\begin{pmatrix} x_0  \\\\ \\frac{\\eta^2}{\\eta^2 + \\xi^2}\\nu \\end{pmatrix}$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\alpha} = -2 \\sum_n \\mathcal{N}_n \\frac{\\theta_n - \\mu_\\phi(x_n)}{\\sigma^2_\\phi(x_n)}$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\beta} = -2 \\sum_n \\mathcal{N}_n \\frac{\\theta_n - \\mu_\\phi(x_n)}{\\sigma^2_\\phi(x_n)} x_n$\n",
    "\n",
    "$\\frac{\\partial{}\\mathcal{L}}{\\partial{}\\gamma^2} \n",
    "= \\sum_n \\mathcal{N}_n \\left( \\frac{1}{\\sigma^2_\\phi(x_n)} \n",
    "- \\frac{\\left(\\theta_n - \\mu_\\phi(x_n) \\right)^2}{\\sigma^4_\\phi(x_n)} \\right) \n",
    "= \\frac{1}{\\gamma^2} \\sum_n \\mathcal{N}_n \\left( 1 \n",
    "- \\frac{\\left(\\theta_n - \\mu_\\phi(x_n) \\right)^2}{\\gamma^2} \\right) $\n",
    "\n",
    "Optima: \n",
    "\n",
    "$\\hat{\\alpha} = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\left(\\theta_n - \\frac{\\sum_m \\mathcal{N}_m \\theta_m x_m}{\\sum_m \\mathcal{N}_m x_m^2} x_n \\right)}{\\sum_n \\mathcal{N}_n - \\frac{\\left( \\sum_n \\mathcal{N}_n x_n \\right)^2}{\\sum_n \\mathcal{N}_n x_n^2}}$\n",
    "\n",
    "$\\hat{\\beta} = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\theta_n x_n - \\hat{\\alpha} \\sum_n \\mathcal{N}_n x_n}{\\sum_n \\mathcal{N}_n x_n^2}$\n",
    "\n",
    "$\\hat{\\gamma}^2 = \n",
    "\\frac{\\sum_n \\mathcal{N}_n \\left( \\theta_n - \\hat{\\alpha} - \\hat{\\beta} x_n \\right)^2}{\\sum_n \\mathcal{N}_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# define problem setup\n",
    "\n",
    "- proposals narrower than the priors seem to induce some form of bias on MLE estimates (non-SVI fits), especially for posterior variances\n",
    "- up to a certain degree, this can be negated by switching to students-t proposals (with few degrees of freedom)\n",
    "- apart from bias, even switching to students-t distributions results in increased variance of the estimates (probably some ESS thing?)\n",
    "\n",
    "\n",
    "- generally speaking, the real challenge for SNPE arises when $\\sigma^2 << \\eta^2$, i.e. when the likelihood (and hence posterior) are much narrower than the prior\n",
    "- in a multi-round SNPE fit, the proposal will tend to be much narrower than the prior after several rounds\n",
    "- the 'relevant' (relative) proposal width thus is the one that corresponds to the posterior variance\n",
    "\n",
    "\n",
    "- for a simple setup that can be largely 'fixed' with students-t proposals choose $\\sigma^2 = \\eta^2$.\n",
    "\n",
    "- intermediate setup:  $\\sigma^2 = 9 \\cdot \\eta^2$ (i.e. posterior variance is $10\\%$ of prior variance)\n",
    "\n",
    "- to watch Rome burn, try  $\\sigma^2 = 99 \\cdot \\eta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "import util\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.summarystats as ds\n",
    "from delfi.simulator.Gauss import Gauss\n",
    "\n",
    "\n",
    "## problem setup ##\n",
    "\n",
    "n_params = 1\n",
    "\n",
    "assert n_params == 1 # cannot be overstressed: everything in this notebook goes downhill sharply otherwise\n",
    "\n",
    "sig2 = 1.0/9. # likelihood variance\n",
    "eta2 = 1.0     # prior variance\n",
    "eps2 = 1e20    # calibration kernel width (everything above a certain threshold will be treated as 'uniform')\n",
    "\n",
    "# pick observed summary statistics\n",
    "x0 = 0.8 * np.ones(1) #_,obs = g.gen(1) \n",
    "\n",
    "## simulation setup ##\n",
    "\n",
    "n_fits = 200  # number of MLE fits (i.e. dataset draws), each single-round fits with pre-specified proposal!\n",
    "N      = 300  # number of simulations per dataset\n",
    "\n",
    "# set proposal priors (one per experiment)\n",
    "\n",
    "\n",
    "ksi2s = np.exp(np.linspace(-5, -0.01, 10)) * eta2  # proposal variance\n",
    "nus = np.zeros_like(ksi2s) #eta2/(eta2+sig2)*x0[0]* np.ones(len(ksi2s))              # proposal mean\n",
    "\n",
    "n_bins = 50 # number of bins for plotting\n",
    "\n",
    "\n",
    "res = {'normal'  : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       't_df10'  : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       't_df3'   : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       'cdelfi'  : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       'unif_w1' : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       'unif_w6' : np.zeros((len(ksi2s), n_fits,5)),\n",
    "       'sig2' : sig2,\n",
    "       'eta2' : eta2,\n",
    "       'eps2' : eps2,\n",
    "       'ksi2s' : ksi2s,\n",
    "       'nus' : nus,\n",
    "       'x0' : x0,\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# SNPE (Gaussian proposals) \n",
    "## multiple fits, with different proposal widths\n",
    "- n_fits many runs, every run with different seed, i.e. different data set $\\{(x_n, \\theta_n)\\}_{n=1}^N$\n",
    "- collect statistics on mean and std. of estimated posterior mean and variance (bias?)\n",
    "- outer loop over different proposal distribution variances (can in principle also move means)\n",
    "  \n",
    "Keep in mind that for large enough $N$, in theory SNPE should return ground-truth posteriors irrespective of the proposal prior (cf. Kaan's proof in JakobsNotes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 3\n",
    "\n",
    "out_snpe  = res['t_df' + str(df)]\n",
    "ksi2, nu = 0.5, 0.\n",
    "util.test_setting(out_snpe, n_params, N, sig2, eta2, eps2, x0, [ksi2], [nu], proposal_form, track_rp=True, df=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first-order analytic approximation to bias for students-t proposals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "gammaln = sp.special.gammaln\n",
    "\n",
    "def gmom(m, mu, sig2):    \n",
    "    # first few non-central moments of Gaussian distribution \n",
    "    # (generally involves confluent hypergeometric function/Hermite polynomials to solve)\n",
    "    \n",
    "    if m==0:\n",
    "        return 1.\n",
    "    if m ==1:\n",
    "        return mu\n",
    "    if m==2:\n",
    "        return mu**2 + sig2\n",
    "    if m==3:\n",
    "        return mu*(mu**2 + 3*sig2)\n",
    "    if m==4:\n",
    "        return mu**4 +  6*mu**2*sig2 +   3      *sig2**2\n",
    "    if m==5:\n",
    "        return mu**5 + 10*mu**3*sig2 +  15*mu   *sig2**2\n",
    "    if m==6:\n",
    "        return mu**6 + 15*mu**4*sig2 +  45*(mu*sig2)**2  +15*sig2**3\n",
    "    if m==7:\n",
    "        return mu**7 + 21*mu**5*sig2 + 105*mu**3*sig2**2+105*mu*sig2**3\n",
    "    if m==8:\n",
    "        return mu**8 + 28*mu**6*sig2 + 210*mu**4*sig2**2+420*mu**2*sig2**3+105*sig2**4\n",
    " \n",
    "    return None\n",
    "\n",
    "def comp_firstorder_approx_bias(eta2, ksi2, nu, mu, df, N):\n",
    "\n",
    "    k,l = 2,1 # l = order for moments of weights, here we need E[w(x)^l * f(x)], l=1 for Ep[w*f]\n",
    "    c = np.sqrt(2*np.pi*eta2/(k))*\\\n",
    "                np.exp( l*(gammaln(df/2)-gammaln(df/2+0.5)) \n",
    "               +l/2*np.log(ksi2)-k/2*np.log(eta2)\n",
    "               +(l-k)/2*np.log(np.pi)-k/2*np.log(2)+l/2*np.log(df))\n",
    "    m_, s2_,mu_ = -nu, eta2/k, mu-nu\n",
    "    M = int(l*(df+1)/2)\n",
    "    bf = [sp.special.binom(M,m)/(df*ksi2)**m \\\n",
    "          *(gmom(2*m+2,m_,s2_)-2*mu_*gmom(2*m+1,m_,s2_)+mu_**2*gmom(2*m,m_,s2_)) \\\n",
    "          for m in range(M+1)]\n",
    "\n",
    "    Ep_f = nu**2 + eta2\n",
    "    \n",
    "    Ep_wf = c * np.sum(bf)\n",
    "\n",
    "    bias = 1/N * (Ep_f - Ep_wf)\n",
    "    \n",
    "    return Ep_wf\n",
    "\n",
    "N = 500\n",
    "sig2 = 1/9.\n",
    "eta2 = 1.0\n",
    "ksi2, nu = 0.1, 1.0\n",
    "df = 3\n",
    "mu = -2.5\n",
    "\n",
    "\n",
    "bias = comp_firstorder_approx_bias(eta2, ksi2, nu, mu, df, N)\n",
    "\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically verify approx. analytic bias against MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "n_fits = 1000\n",
    "res = np.zeros(n_fits)\n",
    "\n",
    "# compute target solution\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))\n",
    "post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0, \n",
    "                     S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params)) \n",
    "# compute proposal-posterior\n",
    "postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0 + sig2/(ksi2+sig2)*nu), \n",
    "                     S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "\n",
    "\n",
    "# set up importance weight computation\n",
    "eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "mu_y = np.array([ [x0], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "s = ds.Identity()\n",
    "pbar = progressbar(total=n_fits)\n",
    "desc = 'repeated fits'\n",
    "pbar.set_description(desc)\n",
    "with pbar:\n",
    "    for idx_seed in range(n_fits):\n",
    "\n",
    "        #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "\n",
    "        # excessive fixating of random seeds\n",
    "        seed = 42 + idx_seed\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params), # * (df-2.)/df,\n",
    "                           dof=df,\n",
    "                           seed=seed)    \n",
    "        p = dd.Gaussian(m=0 * np.ones(n_params), \n",
    "                        S=eta2 * np.eye(n_params), # * (df-2.)/df,\n",
    "                        seed=seed)    \n",
    "        \n",
    "        \n",
    "        m = Gauss(dim=n_params, noise_cov=sig2, seed=seed)\n",
    "        g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "\n",
    "        # gen data\n",
    "        data = g.gen(N, verbose=False)\n",
    "        params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "        # 'fit' MDN\n",
    "        normals = util.get_weights('studentT', eta2, ksi2, eps2, x0, nu, stats, params, df=df) \n",
    "        Ep_f = ((params-mu)**2).mean() #np.var(params)\n",
    "        Ep_wf = np.mean( normals*(params-mu)**2)\n",
    "        \n",
    "        res[idx_seed] = Ep_wf #(1/N * (Ep_f - Ep_wf))\n",
    "        pbar.update(1)\n",
    "\n",
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full scenario ($\\mu = f(x)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "gammaln = sp.special.gammaln\n",
    "\n",
    "def gmom(m, mu, sig2):    \n",
    "    # first few non-central moments of Gaussian distribution \n",
    "    # (generally involves confluent hypergeometric function/Hermite polynomials to solve)\n",
    "    \n",
    "    if m==0:\n",
    "        return 1.\n",
    "    if m ==1:\n",
    "        return mu\n",
    "    if m==2:\n",
    "        return mu**2 + sig2\n",
    "    if m==3:\n",
    "        return mu*(mu**2 + 3*sig2)\n",
    "    if m==4:\n",
    "        return mu**4 +  6*mu**2*sig2 +   3      *sig2**2\n",
    "    if m==5:\n",
    "        return mu**5 + 10*mu**3*sig2 +  15*mu   *sig2**2\n",
    "    if m==6:\n",
    "        return mu**6 + 15*mu**4*sig2 +  45*(mu*sig2)**2  +15*sig2**3\n",
    "    if m==7:\n",
    "        return mu**7 + 21*mu**5*sig2 + 105*mu**3*sig2**2+105*mu*sig2**3\n",
    "    if m==8:\n",
    "        return mu**8 + 28*mu**6*sig2 + 210*mu**4*sig2**2+420*mu**2*sig2**3+105*sig2**4\n",
    " \n",
    "    return None\n",
    "\n",
    "def comp_firstorder_approx_bias_full(eta2, ksi2, nu, sig2, df, N, alpha, beta):\n",
    "\n",
    "    \n",
    "    k,l = 2,1 # l = order for moments of weights, here we need E[w(x)^l * f(x)], l=1 for Ep[w*f]\n",
    "    \n",
    "    # ratio of prior densities normalizers\n",
    "    c = np.sqrt(2*np.pi*eta2/(k))*\\\n",
    "                np.exp( l*(gammaln(df/2)-gammaln(df/2+0.5)) \n",
    "               +l/2*np.log(ksi2)-k/2*np.log(eta2)\n",
    "               +(l-k)/2*np.log(np.pi)-k/2*np.log(2)+l/2*np.log(df))\n",
    "\n",
    "    # Gaussian moments\n",
    "    m_, s2_,mu_ = -nu, eta2/k, -alpha/(beta-1)-nu\n",
    "    \n",
    "    ll_offset = (beta/(beta-1))**2 * sig2\n",
    "\n",
    "    # order of binomial polynomial \n",
    "    M = int(l*(df+1)/2)\n",
    "    bf = [(beta-1)**2 * sp.special.binom(M,m)/(df*ksi2)**m \\\n",
    "          *(gmom(2*m+2,m_,s2_)-2*mu_*gmom(2*m+1,m_,s2_)+(mu_**2+ll_offset)*gmom(2*m,m_,s2_)) \\\n",
    "          for m in range(M+1)]\n",
    "\n",
    "    Ep_f = (beta-1)**2 * eta2 + beta**2 * sig2  + alpha**2\n",
    "    \n",
    "    Ep_wf = c * np.sum(bf)\n",
    "\n",
    "    bias = 1/N * (Ep_f - Ep_wf)\n",
    "    \n",
    "    return bias\n",
    "\n",
    "N = 500\n",
    "sig2 = 1/9.\n",
    "eta2 = 1.0\n",
    "ksi2, nu = 0.1, 1.0\n",
    "df = 5\n",
    "alpha = 0.\n",
    "beta = 0.5\n",
    "\n",
    "bias = comp_firstorder_approx_bias_full(eta2, ksi2, nu, sig2, df, N, alpha, beta)\n",
    "\n",
    "print(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "n_fits = 1000\n",
    "res = np.zeros(n_fits)\n",
    "\n",
    "# compute target solution\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))\n",
    "post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0, \n",
    "                     S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params)) \n",
    "# compute proposal-posterior\n",
    "postpr = dd.Gaussian(m = np.ones(n_params) * (ksi2/(ksi2+sig2)*x0 + sig2/(ksi2+sig2)*nu), \n",
    "                     S=ksi2 - ksi2**2 / (ksi2 + sig2) * np.eye(n_params))\n",
    "\n",
    "\n",
    "# set up importance weight computation\n",
    "eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "mu_y = np.array([ [x0], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "s = ds.Identity()\n",
    "pbar = progressbar(total=n_fits)\n",
    "desc = 'repeated fits'\n",
    "pbar.set_description(desc)\n",
    "with pbar:\n",
    "    for idx_seed in range(n_fits):\n",
    "\n",
    "        #print( str(idx_seed) + '/' + str(n_fits) )\n",
    "\n",
    "        # excessive fixating of random seeds\n",
    "        seed = 42 + idx_seed\n",
    "        ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                           S=ksi2 * np.eye(n_params), # * (df-2.)/df,\n",
    "                           dof=df,\n",
    "                           seed=seed)    \n",
    "        p = dd.Gaussian(m=0 * np.ones(n_params), \n",
    "                        S=eta2 * np.eye(n_params), # * (df-2.)/df,\n",
    "                        seed=seed)    \n",
    "        \n",
    "        m = Gauss(dim=n_params, noise_cov=sig2, seed=seed)\n",
    "        g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "        # gen data\n",
    "        data = g.gen(N, verbose=False)\n",
    "        params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "        # 'fit' MDN\n",
    "        normals = util.get_weights('studentT', eta2, ksi2, eps2, x0, nu, stats, params, df=df) \n",
    "        Ep_f = ((params-alpha-beta*stats)**2).mean() #np.var(params)\n",
    "        Ep_wf = np.mean( normals*(params-alpha-beta*stats)**2)\n",
    "        \n",
    "        res[idx_seed] = (1/N * (Ep_f - Ep_wf))\n",
    "        pbar.update(1)\n",
    "\n",
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "eta2 = 1.\n",
    "x0 = 0.8\n",
    "dfs = [3,5]\n",
    "sig2s = [1./99, 1./9, 1.] # likelihood variance\n",
    "Ns      = [30, 100, 300, 1000, 3000]  # number of simulations per dataset\n",
    "ksi2s = np.array(np.exp(np.log(10) * np.linspace(-2,-.00001,50))) * eta2  # proposal variance\n",
    "\n",
    "clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "plt.figure(figsize=(12,8))        \n",
    "\n",
    "for l in range(len(dfs)):\n",
    "    df = dfs[l]\n",
    "    biases = np.zeros((len(sig2s), len(ksi2s), len(Ns)))\n",
    "    for k in range(len(sig2s)):\n",
    "\n",
    "        sig2 = sig2s[k]\n",
    "        nu = eta2/(eta2+sig2)*x0\n",
    "\n",
    "        for i in range(len(Ns)):\n",
    "\n",
    "            N = Ns[i]\n",
    "\n",
    "            for j in range(len(ksi2s)):    \n",
    "\n",
    "                ksi2 = ksi2s[j]\n",
    "\n",
    "                biases[k,j,i] = comp_firstorder_approx_bias_full(eta2, ksi2, nu, sig2, df, N, alpha, beta)\n",
    "\n",
    "    ksi2s_MC = np.array([0.01, 0.1, 0.5, 0.999]) * eta2  # proposal variance\n",
    "\n",
    "    for k in range(len(sig2s)):\n",
    "        plt.subplot(len(dfs)+1,len(sig2s),k+1+(l+1)*len(sig2s))\n",
    "        sig2 = sig2s[k]\n",
    "        gt = eta2 - eta2**2 / (eta2 + sig2s[k])\n",
    "        for i in range(len(Ns)):\n",
    "            N = Ns[i]\n",
    "            plt.semilogx(ksi2s, gt + biases[k,:,i], '-', color=clrs[i])\n",
    "\n",
    "        for i in range(len(Ns)):\n",
    "            N = Ns[i]\n",
    "            tmp = np.load('res_analytic_n_fits' + str(1000) + '_N' + str(N) +'_postVar' + str(np.int(np.round(1/sig2)))+'.npy')[()]        \n",
    "            out_snpe = tmp['t_df' + str(int(df))]\n",
    "            m = out_snpe[:,:,1].mean(axis=1)\n",
    "            plt.semilogx(ksi2s_MC, m, 'o--', color=clrs[i], markersize=4, alpha=0.3)    \n",
    "\n",
    "\n",
    "            gt = eta2 - eta2**2 / (eta2 + sig2)\n",
    "            plt.semilogx(ksi2s_MC, gt*np.ones_like(ksi2s_MC), 'k--', linewidth=2)\n",
    "            \n",
    "\n",
    "        plt.axis([0.95*ksi2s[0], 1.05*ksi2s[-1], 0, 1.1*gt ])\n",
    "\n",
    "    plt.subplot(len(dfs)+1,len(sig2s),1+(l+1)*len(sig2s))\n",
    "    plt.ylabel('df =' + str(int(dfs[l])))\n",
    "    \n",
    "for i in range(len(sig2s)):\n",
    "\n",
    "    sig2 = sig2s[i]\n",
    "\n",
    "    clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "    clrs2 = np.linspace(0.7, 0.95, len(sig2s)).reshape(-1,1) * np.array([[1.0,0.4,0.4]])\n",
    "\n",
    "    plt.subplot(len(dfs)+1,len(sig2s),i+1)\n",
    "    th = np.linspace(-3, 3, 300).reshape(-1,1)\n",
    "    plt.plot(th,  p.eval(th, log=False), color='k', linewidth=2.0)\n",
    "    clr = clrs2[i]\n",
    "    post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0, \n",
    "                         S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params)) \n",
    "    l = dd.Gaussian(m=x0 * np.ones(n_params),\n",
    "                    S=sig2 * np.eye(n_params))    \n",
    "    plt.plot(th, l.eval(th, log=False), '--', color=clr, alpha=0.3)\n",
    "    plt.plot(th, post.eval(th, log=False), color=clr)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([-3, -2, -1, 0, x0, 2, 3], [-3, -2, -1, 0, 'x0', 2, 3])\n",
    "    plt.yticks([])\n",
    "    plt.axis([-3.01, 3.01, 0, 3.0])\n",
    "    plt.title('posterior var = ' + str(np.int(np.round(100/(1/sig2+1/eta2)))) + '% prior variance')\n",
    "\n",
    "    \n",
    "    for l in range(len(dfs)):\n",
    "\n",
    "        plt.subplot(len(dfs)+1,len(sig2s),i+1+(l+1)*len(sig2s))\n",
    "        plt.plot(1/(1/sig2+1/eta2)*np.ones(2), [0, 1.1*(eta2 - eta2**2 / (eta2 + sig2))], color=clrs2[i])\n",
    "\n",
    "    if i == len(sig2s)//2: \n",
    "        plt.xlabel('proposal prior variance / prior variance') \n",
    "        \n",
    "plt.subplot(len(dfs)+1,len(sig2s),1+2*len(sig2s))\n",
    "plt.legend(['N = ' + str(n) for n in Ns], loc=3)\n",
    "\n",
    "plt.savefig('bias_linear_approx_studentt_df3_5_withMCE.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make summary figure\n",
    "\n",
    "( loading Monte Carlo fits from disk ! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "eta2 = 1.\n",
    "x0 = 0.8\n",
    "dfs = [3,5]\n",
    "sig2s = [1./99, 1./9, 1.] # likelihood variance\n",
    "Ns    = np.asarray(np.exp(np.log(10)*np.linspace(1, 4, 50)),dtype=int)  # number of simulations per dataset\n",
    "Ns_MC = [30, 100, 300, 1000, 3000]\n",
    "ksi2s = np.array(np.exp(np.log(10) * np.linspace(-2,-.00001,3))) * eta2  # proposal variance\n",
    "\n",
    "ksi2s_MC = np.array([0.01, 0.1, 0.999]) * eta2  # proposal variance\n",
    "idx_ksi2_MC = [0,1,3]\n",
    "\n",
    "clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(ksi2s)).reshape(-1,1)\n",
    "plt.figure(figsize=(12,10))        \n",
    "\n",
    "for l in range(len(dfs)):\n",
    "    df = dfs[l]\n",
    "    biases = np.zeros((len(sig2s), len(ksi2s), len(Ns)))\n",
    "    for k in range(len(sig2s)):\n",
    "\n",
    "        sig2 = sig2s[k]\n",
    "        nu = eta2/(eta2+sig2)*x0\n",
    "\n",
    "        for i in range(len(Ns)):\n",
    "\n",
    "            N = Ns[i]\n",
    "\n",
    "            for j in range(len(ksi2s)):    \n",
    "\n",
    "                ksi2 = ksi2s[j]\n",
    "\n",
    "                biases[k,j,i] = comp_firstorder_approx_bias_full(eta2, ksi2, nu, sig2, df, N, alpha, beta)\n",
    "\n",
    "    for k in range(len(sig2s)):\n",
    "        plt.subplot(len(dfs)+1,len(sig2s),k+1+(l+1)*len(sig2s))\n",
    "        sig2 = sig2s[k]\n",
    "        gt = eta2 - eta2**2 / (eta2 + sig2s[k])\n",
    "        for i in range(len(ksi2s)):\n",
    "            \n",
    "            plt.loglog(Ns, -biases[k,i,:], '-', color=clrs[i], label=r'$\\tilde{\\sigma}^2=' + str(int(100*ksi2s[i]))+ '$')\n",
    "\n",
    "        for i in range(len(ksi2s_MC)):\n",
    "            for j in range(len(Ns_MC)):\n",
    "                N = Ns_MC[j]\n",
    "                try:\n",
    "                    gt = eta2 - eta2**2 / (eta2 + sig2)\n",
    "                    tmp = np.load('res_analytic_n_fits' + str(1000) + '_N' + str(N) +'_postVar' + str(np.int(np.round(1/sig2)))+'.npy')[()]        \n",
    "                    out_snpe = tmp['t_df' + str(int(df))]\n",
    "                    m = out_snpe[:,:,1].mean(axis=1)\n",
    "                    ms = 6 if (2-i)==k else 4\n",
    "                    plt.loglog(N, -(m[idx_ksi2_MC[i]]-gt), 'o', color=clrs[i], markersize=ms, alpha=1.)    \n",
    "                except: \n",
    "                    pass\n",
    "\n",
    "            #plt.semilogx(N, gt*np.ones_like(ksi2s_MC), 'k--', linewidth=2)\n",
    "\n",
    "        #plt.axis([0.95*ksi2s[0], 1.05*ksi2s[-1], 0, 1.1*gt ])\n",
    "\n",
    "    plt.subplot(len(dfs)+1,len(sig2s),1+(l+1)*len(sig2s))\n",
    "    plt.ylabel('df =' + str(int(dfs[l])))\n",
    "    \n",
    "for i in range(len(sig2s)):\n",
    "\n",
    "    sig2 = sig2s[i]\n",
    "\n",
    "    clrs = np.array([[0,1,0]]) * np.linspace(0.1, 0.9, len(Ns)).reshape(-1,1)\n",
    "    clrs2 = np.linspace(0.7, 0.95, len(sig2s)).reshape(-1,1) * np.array([[1.0,0.4,0.4]])\n",
    "\n",
    "    plt.subplot(len(dfs)+1,len(sig2s),i+1)\n",
    "    th = np.linspace(-3, 3, 300).reshape(-1,1)\n",
    "    plt.plot(th,  p.eval(th, log=False), color='k', linewidth=2.0)\n",
    "    clr = clrs2[i]\n",
    "    post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0, \n",
    "                         S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params)) \n",
    "    l = dd.Gaussian(m=x0 * np.ones(n_params),\n",
    "                    S=sig2 * np.eye(n_params))    \n",
    "    plt.plot(th, l.eval(th, log=False), '--', color=clr, alpha=0.3)\n",
    "    plt.plot(th, post.eval(th, log=False), color=clr)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([-3, -2, -1, 0, x0, 2, 3], [-3, -2, -1, 0, 'x0', 2, 3])\n",
    "    plt.yticks([])\n",
    "    plt.axis([-3.01, 3.01, 0, 3.0])\n",
    "    plt.title('posterior var = ' + str(np.int(np.round(100/(1/sig2+1/eta2)))) + '% prior variance')\n",
    "\n",
    "    #for l in range(len(dfs)):\n",
    "    #   plt.subplot(len(dfs)+1,len(sig2s),i+1+(l+1)*len(sig2s))\n",
    "    #   plt.plot(1/(1/sig2+1/eta2)*np.ones(2), [0, 1.1*(eta2 - eta2**2 / (eta2 + sig2))], color=clrs2[i])\n",
    "        \n",
    "plt.subplot(len(dfs)+1,len(sig2s),1+1*len(sig2s))\n",
    "plt.legend(loc=1)\n",
    "\n",
    "plt.subplot(len(dfs)+1,len(sig2s),2+2*len(sig2s))\n",
    "plt.xlabel('N')\n",
    "\n",
    "#plt.savefig('bias_linear_approx_studentt_df3_5_withMCE.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why does the approximation tend to underestimate the variances so much?\n",
    "- $1/(1+x) \\approx 1-x$ only for $|x| \\approx 0$.\n",
    "- in our setup, $x= w(\\theta) > 0$, which can be pretty large as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 3\n",
    "\n",
    "eta2, ksi2 = 1., 0.1 # prior and proposal-prior variances\n",
    "\n",
    "nu,x0  = 0., 0.\n",
    "params = np.linspace(-5,5,100).reshape(-1)\n",
    "normals = util.get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) \n",
    "\n",
    "plt.plot(params, normals)\n",
    "plt.plot(params, 5*dd.Gaussian(m=np.zeros(1),S=eta2*np.eye(1)).eval(params.reshape(-1,1),log=False))\n",
    "plt.legend(['importance weights', 'prior (unnormalized)'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SNPE (students T proposals, df = 10) \n",
    "## multiple fits, with different proposal widths\n",
    "- same as before, but with students T proposals instead of Gaussian proposals (same mean and variance)\n",
    "- df $=10$ in 1D : roughly Gaussian, but with slightly stronger tails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 10\n",
    "\n",
    "out_snpe  = res['t_df' + str(df)]\n",
    "\n",
    "util.test_setting(out_snpe, n_params, N, sig2, eta2, eps2, x0, ksi2s, nus, proposal_form, track_rp=True, df=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SNPE (students T proposals, df = 3) \n",
    "## multiple fits, with different proposal widths\n",
    "- same as before, but with students T proposals instead of Gaussian proposals (same mean and variance)\n",
    "- df $=3$ in 1D : much stronger tails than Gaussian (need df>2 for well-defined student-T variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "proposal_form = 'studentT'\n",
    "df = 3\n",
    "\n",
    "out_snpe  = res['t_df' + str(df)]\n",
    "\n",
    "util.test_setting(out_snpe, n_params, N, sig2, eta2, eps2, x0, ksi2s, nus, proposal_form, track_rp=True, df=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# uniform proposals\n",
    "- uniform proposals 'cut' the prior tails, directly influencing the target posterior\n",
    "- the importance sampling on this narrower prior may however be easier to achieve\n",
    "- can we balance damage to the posterior against better importance sampling and gain something overall ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "proposal_form = 'unif'\n",
    "sds = 1\n",
    "marg = np.sqrt(sds) * np.sqrt(12)\n",
    "out_snpe  = res['unif_w'+str(sds)]\n",
    "\n",
    "util.test_setting(out_snpe, n_params, N, sig2, eta2, eps2, x0, ksi2s, nus, proposal_form, track_rp=True, marg=marg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# compare with CDELFI\n",
    "- simply set track_rp = False to compare MLE solution with mean and variance of proposal-posterior\n",
    "- however requires Gaussian proposals to directly compare with proposal-posterior\n",
    "- if the proposal-posterior is good, so should be the prior-corrected result of the analytical division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_form = 'normal'\n",
    "\n",
    "util.test_setting(out_snpe, n_params, N, sig2, eta2, eps2, x0, ksi2s, nus, proposal_form, track_rp=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('res_analytic_n_fits' + str(n_fits) + '_N' + str(N), res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerical checks for gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "track_rp = True\n",
    "proposal_form = 'normal'\n",
    "df = None\n",
    "\n",
    "nu = 0.\n",
    "ksi2 = 0.5 * eta2\n",
    "\n",
    "ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                S=ksi2 * np.eye(n_params))\n",
    "s = ds.Identity()\n",
    "m = Gauss(noise_cov=sig2)\n",
    "g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "\n",
    "post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0[0], \n",
    "                     S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params)) \n",
    "    \n",
    "seed = 42\n",
    "g.model.seed = seed\n",
    "g.prior.seed = seed\n",
    "g.seed = seed\n",
    "\n",
    "data = g.gen(N, verbose=False)\n",
    "params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "normals = util.get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\alpha}$\n",
    "\n",
    "- $\\frac{\\partial}{\\partial\\alpha}$ is being difficult here. Analytic solution $\\hat{\\alpha}$ still fails to numberically set the stated partial derivative $\\frac{\\partial\\mathcal{L}}{\\partial{}\\alpha}(\\hat{\\alpha})$ to zero ...\n",
    "- Obtained $\\hat{\\alpha}$ however are pretty much sensible though (correct 'ballpark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hat = np.array(util.alpha(params, stats, normals))\n",
    "\n",
    "gamma2_ = post.std**2\n",
    "alphas = np.linspace(-0.09, -0.02, 100000)\n",
    "\n",
    "beta_ = util.beta(params, stats, normals, alpha_hat)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alpha_hat)/gamma2_).sum(axis=0)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2_).sum(axis=0)\n",
    "plt.plot(alphas, out)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "alphas[np.argmin(np.abs(out))], alpha_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "gamma2_ = post.std**2\n",
    "betas = np.linspace(0., 1., 1000)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - betas.reshape(1,-1) * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "plt.plot(betas, out)\n",
    "plt.show()\n",
    "\n",
    "beta_hat = util.beta(params, stats, normals, ahat=alpha_)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_hat * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "betas[np.argmin(np.abs(out))], beta_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\gamma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "beta_ = beta_hat\n",
    "gamma2s = np.linspace(0.03, 0.04, 1000)\n",
    "\n",
    "# something off with below (hard-coded...) gradients now. Outcommenting numerical solution for now!\n",
    "\n",
    "tmp = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2s.reshape(1,-1)\n",
    "out = 1/gamma2s.reshape(-1,) * (normals.reshape(-1,1) * (1 - tmp)).sum(axis=0)\n",
    "\n",
    "plt.plot(gamma2s, out)\n",
    "plt.show()\n",
    "\n",
    "gamma2_hat = util.gamma2(params, stats, normals, ahat=alpha_, bhat=beta_)\n",
    "tmp_ = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2_hat\n",
    "out_hat = 1/gamma2_hat * (normals.reshape(-1,1) * (1 - tmp_)).sum(axis=0)\n",
    "\n",
    "#gamma2s *= np.nan \n",
    "#out = np.zeros_like(gamma2s)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "gamma2s[np.argmin(np.abs(out))], gamma2_hat, out_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
