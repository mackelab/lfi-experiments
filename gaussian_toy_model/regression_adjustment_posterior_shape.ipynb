{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression-adjustment does not capture the posterior shape beyond mean and variance\n",
    "\n",
    "Regression-adjustment fits mean $\\mu(x)$ and variance $\\sigma^2(x)$ of the conditional distribution $p(\\theta |x )$ as a function of $x$.\n",
    "\n",
    "Unlike DELFI methods, it does not return $p(\\theta | x_0) = \\mathcal{N}(\\theta | \\mu(x_0), \\sigma^2(x_0))$ as its posterior estimate.\n",
    "Rather, it adjusts the sampled parameters $\\theta_i$ through a locally linear-affine transformation:\n",
    "\n",
    "$\\hat{\\theta}_i = \\mu(x_0) + (\\theta_i - \\mu(x_i)) \\frac{\\sigma(x_0)}{\\sigma(x)}$\n",
    "\n",
    "The returned posterior is given as the set of adjusted parameters $\\{ \\hat{\\theta}_i \\}_i$. \n",
    "\n",
    "### Question: Is the full shape of the adjusted samples always meaningful? \n",
    "\n",
    "Below is a simple counter-example with binary summary statistics $x \\in \\{0, 1\\}$ to demonstrate that regression-adjustment posteriors cannot necessarily be interpreted beyond their mean and variance. \n",
    "\n",
    "The case of binary summary statistic is nice because every drawn $\\theta_i$ is either a valid draw from the true posterior ($x_i=x_0$) or not ($x_i\\neq x_0$).\n",
    "\n",
    "prior:\n",
    "\n",
    "$p(\\theta) = \\mathcal{N}(0,1)$\n",
    "\n",
    "likelihood: \n",
    "\n",
    "$P(x=1 \\ | \\ \\theta) = 1$   if   $\\theta\\geq 0$ \n",
    "\n",
    "$P(x=1 \\ | \\ \\theta) = 0$   if   $\\theta<0$ \n",
    "\n",
    "The posterior $p(\\theta | x_0)$ is a truncated Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from delfi import distribution as dd\n",
    "\n",
    "# define likelihood\n",
    "\n",
    "def simulator(theta):\n",
    "    return (theta >= 0) * 1.\n",
    "\n",
    "# define prior\n",
    "prior = dd.Gaussian(m = np.zeros(1), \n",
    "                     S =np.eye(1))\n",
    "\n",
    "# draw artificial dataset\n",
    "th = prior.gen(100000)\n",
    "x = simulator(th)\n",
    "\n",
    "# compute conditional probabilities\n",
    "th_x_0 = th[x==0] # all samples from p(\\theta | x=0)\n",
    "th_x_1 = th[x==1] # all samples from p(\\theta | x=1)\n",
    "\n",
    "# get posterior samples \n",
    "x_0 = 0\n",
    "s_post = th[x==x_0].copy()\n",
    "\n",
    "# train linear-Gaussian regression model: \n",
    "def mu(x):\n",
    "    if x == 0:\n",
    "        return th_x_0.mean()\n",
    "    elif x==1:\n",
    "        return th_x_1.mean()\n",
    "    \n",
    "def sig(x): \n",
    "    if x == 0:\n",
    "        return th_x_0.std()\n",
    "    elif x==1:\n",
    "        return th_x_1.std()\n",
    "\n",
    "# do regression-adjustment    \n",
    "th_reg_adj_0 = mu(0) + (th_x_0 - mu(0)) * sig(1)/sig(1) # nothing to do \n",
    "th_reg_adj_1 = mu(0) + (th_x_1 - mu(1)) * sig(0)/sig(1) \n",
    "    \n",
    "# collect regression-adjusted samples\n",
    "th_reg_adj = np.zeros_like(th) \n",
    "th_reg_adj[x==0] = th_reg_adj_0\n",
    "th_reg_adj[x==1] = th_reg_adj_1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# plot raw samples with mean +/- 1 std \n",
    "plt.subplot(2,2,1)\n",
    "x_plot = x+np.random.normal(size=x.size).reshape(x.shape)/50 # add some jitter to x for visualization\n",
    "plt.plot(x_plot, th, 'ko')\n",
    "for x_ in [0,1]:\n",
    "    plt.plot(x_, mu(x_), 'ro')\n",
    "    plt.plot([x_,x_], sig(x_)*np.array([-1,1])+mu(x_), 'r-')\n",
    "plt.axis([-1, 2, -5, 5])\n",
    "plt.xlabel('x (+ jitter for visibility)')\n",
    "plt.ylabel('theta')\n",
    "plt.title('conditional distribution p( theta | x )')\n",
    "plt.xticks([0,1])\n",
    "\n",
    "# plot real posterior\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(s_post, bins=np.linspace(-4, 2, 61))\n",
    "plt.title('actual posterior p( theta | x_0 ) for x_0 = 0')\n",
    "plt.xlabel('theta')\n",
    "plt.yticks([])\n",
    "\n",
    "# plot regression-adjusted samples with mean +/- 1 std\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(x_plot, th_reg_adj, 'ko')\n",
    "for x_ in [0,1]:\n",
    "    plt.plot(x_, mu(0), 'mo')\n",
    "    plt.plot([x_,x_], sig(0)*np.array([-1,1])+mu(0), 'm-')\n",
    "plt.axis([-1, 2, -5, 5])\n",
    "plt.xlabel('x (+ jitter for visibility)')\n",
    "plt.ylabel('theta')\n",
    "plt.title('regression-adjusted samples')\n",
    "plt.xticks([0,1])\n",
    "\n",
    "# plot regression-adjusted posterior\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(th_reg_adj, bins=np.linspace(-4, 2, 61))\n",
    "plt.title('regression-adjusted posterior')\n",
    "plt.xlabel('theta')\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(' mean and variance of the regression-adjusted posterior are correct by construction: ')\n",
    "\n",
    "print('true posterior mean: ', s_post.mean())\n",
    "print('true posterior std.: ', s_post.std())\n",
    "\n",
    "print('regr.adj. posterior mean: ', th_reg_adj.mean())\n",
    "print('regr.adj. posterior std.: ', th_reg_adj.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What went wrong?\n",
    "\n",
    "The real posterior is asymmetric and its support is bounded from above by $\\theta \\leq 0$.\n",
    "\n",
    "The regression-adjusted posterior is symmetric and has infinite support. \n",
    "\n",
    "At heart, regression adjustment assumes that one can transform draws $\\theta_i \\sim p(\\theta | x_i)$ from the conditional distribution for any $x_i$ into valid draws from the real posterior $p(\\theta | x_0)$ simply by ensuring that all these distribtions have the same mean and variance.\n",
    "\n",
    "This is true if $p(\\theta | x)$ for different $x$ all lie in the same location–scale family. \n",
    "\n",
    "Otherwise, it is not true in general. \n",
    "\n",
    "In the above example, $p(\\theta | x=0)$ is (proportional to) the negative half of a Gaussian, and $p(\\theta | x=1)$ the positive half. \n",
    "These two distributions are not within the same location-scale family. \n",
    "\n",
    "In the above case with binary $x$, the regression-adjusted samples are sampled from a mixture of $p(\\theta | x=0)$ and a 'adjusted' version of $p(\\theta | x=1)$ (with mixture weight $E[\\theta > 0] = \\frac{1}{2}$). \n",
    "More generally for continuous $x$, the regression-adjusted samples are drawn from a complex compound distribution that mixes 'adjusted' $p(\\theta |x)$ for all $x$ with $|| x_0 - x || < \\epsilon$. \n",
    "Again, unless all $p(\\theta | x)$ for different $x$ all lie in the same location–scale family, things can go wrong. \n",
    "\n",
    "Note that we are talking about *ground-truth* conditionals $p(\\theta |x)$ here. \n",
    "In general, these are intractable, which makes it very hard to verify whether or not the location-scale family assumption holds or not.\n",
    "\n",
    "Remark: \n",
    "The paper \"Convergence of Regression Adjusted Approximate Bayesian Computation\" (Li \\& Fearnhead 2017) proofs convergence of the regression-adjusted posterior to ground-truth, but it only talks about posterior mean $\\mu(x_0)$ and variance $\\sigma^2(x_0)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun with regression adjustment\n",
    "\n",
    "the whole thing again, but with likelihood  $P(x=1 \\ | \\ \\theta) = 1$   if   $\\theta\\geq -1.5$  (skip to figure !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from delfi import distribution as dd\n",
    "\n",
    "# define likelihood\n",
    "\n",
    "def simulator(theta):\n",
    "    return (theta >= -1.5) * 1.\n",
    "\n",
    "# define prior\n",
    "prior = dd.Gaussian(m = np.zeros(1), \n",
    "                     S =np.eye(1))\n",
    "\n",
    "# draw artificial dataset\n",
    "th = prior.gen(100000)\n",
    "x = simulator(th)\n",
    "\n",
    "# compute conditional probabilities\n",
    "th_x_0 = th[x==0] # all samples from p(\\theta | x=0)\n",
    "th_x_1 = th[x==1] # all samples from p(\\theta | x=1)\n",
    "\n",
    "# get posterior samples \n",
    "x_0 = 0\n",
    "s_post = th[x==x_0].copy()\n",
    "\n",
    "# train linear-Gaussian regression model: \n",
    "def mu(x):\n",
    "    if x == 0:\n",
    "        return th_x_0.mean()\n",
    "    elif x==1:\n",
    "        return th_x_1.mean()\n",
    "    \n",
    "def sig(x): \n",
    "    if x == 0:\n",
    "        return th_x_0.std()\n",
    "    elif x==1:\n",
    "        return th_x_1.std()\n",
    "\n",
    "# do regression-adjustment    \n",
    "th_reg_adj_0 = mu(0) + (th_x_0 - mu(0)) * sig(1)/sig(1) # nothing to do \n",
    "th_reg_adj_1 = mu(0) + (th_x_1 - mu(1)) * sig(0)/sig(1) \n",
    "    \n",
    "# collect regression-adjusted samples\n",
    "th_reg_adj = np.zeros_like(th) \n",
    "th_reg_adj[x==0] = th_reg_adj_0\n",
    "th_reg_adj[x==1] = th_reg_adj_1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# plot raw samples with mean +/- 1 std \n",
    "plt.subplot(2,2,1)\n",
    "x_plot = x+np.random.normal(size=x.size).reshape(x.shape)/50 # add some jitter to x for visualization\n",
    "plt.plot(x_plot, th, 'ko')\n",
    "for x_ in [0,1]:\n",
    "    plt.plot(x_, mu(x_), 'ro')\n",
    "    plt.plot([x_,x_], sig(x_)*np.array([-1,1])+mu(x_), 'r-')\n",
    "plt.axis([-1, 2, -5, 5])\n",
    "plt.xlabel('x (+ jitter for visibility)')\n",
    "plt.ylabel('theta')\n",
    "plt.title('conditional distribution p( theta | x )')\n",
    "plt.xticks([0,1])\n",
    "\n",
    "# plot real posterior\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(s_post, bins=np.linspace(-4, 2, 61))\n",
    "plt.title('actual posterior p( theta | x_0 ) for x_0 = 0')\n",
    "plt.xlabel('theta')\n",
    "plt.yticks([])\n",
    "\n",
    "# plot regression-adjusted samples with mean +/- 1 std\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(x_plot, th_reg_adj, 'ko')\n",
    "for x_ in [0,1]:\n",
    "    plt.plot(x_, mu(0), 'mo')\n",
    "    plt.plot([x_,x_], sig(0)*np.array([-1,1])+mu(0), 'm-')\n",
    "plt.axis([-1, 2, -5, 5])\n",
    "plt.xlabel('x (+ jitter for visibility)')\n",
    "plt.ylabel('theta')\n",
    "plt.title('regression-adjusted samples')\n",
    "plt.xticks([0,1])\n",
    "\n",
    "# plot regression-adjusted posterior\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(th_reg_adj, bins=np.linspace(-4, 2, 61))\n",
    "plt.title('regression-adjusted posterior')\n",
    "plt.xlabel('theta')\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(' mean and variance of the regression-adjusted posterior are correct by construction: ')\n",
    "\n",
    "print('true posterior mean: ', s_post.mean())\n",
    "print('true posterior std.: ', s_post.std())\n",
    "\n",
    "print('regr.adj. posterior mean: ', th_reg_adj.mean())\n",
    "print('regr.adj. posterior std.: ', th_reg_adj.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
