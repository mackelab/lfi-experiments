{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# nonlinear likelihoods & calibration kernels\n",
    "\n",
    "- verifying the intuition that SNPE fits will tend to be more global than CDELFI \n",
    "- exploring usage of calibration kernels to control this behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## background\n",
    "\n",
    "- core DELFI loss derives from fitting MDN $q_\\phi(\\theta|x)$ to joint density $p(\\theta,x)$\n",
    "$$ \\mathcal{L}(\\phi) = D_{KL}( p(\\theta) p(x | \\theta) || q_\\phi(\\theta|x) p(x)) + const. $$\n",
    "defined by prior $p(\\theta)$ and simulator $p(x | \\theta)$.\n",
    "\n",
    "$\\hspace{0.1cm}$\n",
    "\n",
    "- CDELFI (Papamakarios et al.) introduces proposals $\\tilde{p}(\\theta)$ and fits the MDN to modified loss\n",
    "$$ \\tilde{\\mathcal{L}}(\\phi)  = D_{KL}( \\tilde{p}(\\theta) p(x | \\theta) || q_\\phi(\\theta|x) \\tilde{p}(x)) + const. $$\n",
    "with subsequent analytic correction of wrong posterior $ q_\\phi(\\theta | x_0) \\approx \\tilde{p}(\\theta | x_0 ) $ at observed data $x_0$.\n",
    "\n",
    "$\\hspace{0.1cm}$\n",
    "\n",
    "- SNPE introduces importance weighting to avoid analytic correction and get back to $\\mathcal{L}(\\phi)$ through\n",
    "$$ \\mathcal{L}(\\phi)  = D_{KL}( \\tilde{p}(\\theta) \\frac{p(\\theta)}{ \\tilde{p}(\\theta)} p(x | \\theta) || q_\\phi(\\theta|x) p(x)) + const. $$\n",
    "\n",
    "\n",
    "- globally minimizing $D_{KL}( \\tilde{p}(\\theta) \\frac{p(\\theta)}{ \\tilde{p}(\\theta)} p(x | \\theta) || q_\\phi(\\theta|x) p(x))$ may become a problem when the approximator $q_\\phi(\\theta|x)$ is less complex than the actual $p(\\theta | x)$.\n",
    "- in part, that's what the proposal $\\tilde{p}(\\theta)$ was originally introduced for: to locally approximate  $p(\\theta | x)$ around $x = x_0$.\n",
    "- empirical observation: fits with same network size appear poorer for SNPE than CDELFI (mapRF subproject) even for very large data-sets ($N \\geq 50k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question\n",
    "\n",
    "- **does SNPE still support the focusing feature of the proposal?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approach\n",
    "\n",
    "- run SNPE and CDELFI on linear and **non-linear** toy problems with **narrow** ground-truth posteriors. \n",
    "- provide proposal priors that match the actual posterior for different data points $x_0$, i.e. $\\tilde{p}(\\theta) \\approx p(\\theta | x_0 )$, and fit with **linear** MDNs.\n",
    "- known problem for SNPE: importance sampling breaks, hence try **Student-t proposals for SNPE**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "import util\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.summarystats as ds\n",
    "from delfi.simulator.Gauss import Gauss\n",
    "\n",
    "## problem setup ##\n",
    "\n",
    "n_params = 1\n",
    "\n",
    "assert n_params == 1 # cannot be overstressed: everything in this notebook goes downhill sharply otherwise\n",
    "\n",
    "\n",
    "def my_fun(df, x0s, proposal_form, model, eta2, sig2, N, eps2 = 1e20):        \n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    seed = 42\n",
    "    s = ds.Identity()\n",
    "    p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                    S=eta2 * np.eye(n_params))    \n",
    "    if model=='lin':\n",
    "        m = Gauss(dim=n_params, noise_cov=sig2, seed=seed)\n",
    "    elif model=='log':\n",
    "        m = util.LogGauss(dim=n_params, noise_cov=sig2, seed=seed)   \n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "    data = g.gen(1000, verbose=False)\n",
    "    params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot( 0, 0, 'r', linewidth=2.)\n",
    "    plt.plot( 0, 0, 'g', linewidth=2.)\n",
    "    plt.legend(['CDELFI', 'SNPE'], loc=2, frameon=False)\n",
    "    plt.plot( stats, params, 'o' )        \n",
    "    \n",
    "    for idx0 in range(len(x0s)):\n",
    "\n",
    "        x0 = np.array([x0s[idx0]])\n",
    "        sig2_true = 1./(1/eta2+1/sig2)\n",
    "        if model=='lin':\n",
    "            mu_true   = (x0/sig2)*sig2_true\n",
    "        elif model=='log':\n",
    "            mu_true   = (np.log(x0)/sig2)*sig2_true\n",
    "\n",
    "        ksi2 = sig2_true\n",
    "        nu = mu_true\n",
    "\n",
    "        # 'fit CDELFI MDN\n",
    "\n",
    "        ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                         S=ksi2 * np.eye(n_params),\n",
    "                         seed=seed)\n",
    "        g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "        # gen data\n",
    "        data = g.gen(N, verbose=False)\n",
    "        params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "        ahat_CDELFI =       util.alpha(params, stats, np.ones(N)/N)\n",
    "        bhat_CDELFI =        util.beta(params, stats, np.ones(N)/N, ahat_CDELFI)\n",
    "        gamma2hat_CDELFI = util.gamma2(params, stats, np.ones(N)/N, ahat_CDELFI, bhat_CDELFI)\n",
    "\n",
    "        mu_hat_CDELFI   = ahat_CDELFI + bhat_CDELFI * x0\n",
    "        sig2_hat_CDELFI = gamma2hat_CDELFI\n",
    "\n",
    "        # 'fit' SNPE MDN\n",
    "        a,b=None,None\n",
    "        if proposal_form == 'normal':\n",
    "            ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                             S=ksi2 * np.eye(n_params),\n",
    "                             seed=seed)\n",
    "        elif proposal_form == 'studentT':\n",
    "            ppr = dd.StudentsT(m=nu * np.ones(n_params), \n",
    "                               S=ksi2 * np.eye(n_params), # * (df-2.)/df,\n",
    "                               dof=df,\n",
    "                               seed=seed)    \n",
    "        elif proposal_form == 'unif':\n",
    "            a = nu - marg/2. * np.sqrt(ksi2)\n",
    "            b = nu + marg/2. * np.sqrt(ksi2)\n",
    "            ppr = dd.Uniform(lower=a, upper=b, seed=seed)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "        # gen data\n",
    "        data = g.gen(N, verbose=False)\n",
    "        params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "        normals = util.get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df, a=a,b=b) \n",
    "        ahat_SNPE =       util.alpha(params, stats, normals)\n",
    "        bhat_SNPE =        util.beta(params, stats, normals, ahat_SNPE)\n",
    "        gamma2hat_SNPE = util.gamma2(params, stats, normals, ahat_SNPE, bhat_SNPE)\n",
    "\n",
    "\n",
    "        out_ = util.analytic_div(np.array((mu_hat_CDELFI, sig2_hat_CDELFI)).reshape(1,1,2), \n",
    "                          eta2, nu*np.ones(1), ksi2*np.ones(1))\n",
    "\n",
    "        #print('target posterior \\n', (mu_true, sig2_true))\n",
    "        #print('SNPE posterior \\n', (ahat_SNPE + x0 * bhat_SNPE, gamma2hat_SNPE))\n",
    "        #print('CDELFI posterior (raw) \\n', (mu_hat_CDELFI, gamma2hat_CDELFI))\n",
    "        #print('CDELFI posterior (corrected) \\n', (out_[0,0,0], out_[0,0,1]))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        \n",
    "        if model=='log':\n",
    "            x = np.array([0,9])\n",
    "            y = [-1,3.5]\n",
    "        elif model=='lin':\n",
    "            x = np.array([-3,3])            \n",
    "            y = [-3, 3]\n",
    "        f = ahat_SNPE + bhat_SNPE*x\n",
    "        sig = np.sqrt(gamma2hat_SNPE)\n",
    "        plt.plot( x, f, 'g', linewidth=2)\n",
    "        plt.fill_between(x, f-sig,f+sig, \n",
    "                         where=None, facecolor='g', alpha=0.3)\n",
    "        f = ahat_CDELFI + bhat_CDELFI*x\n",
    "        sig = np.sqrt(gamma2hat_CDELFI)\n",
    "        plt.plot( x, f, 'r', linewidth=2)\n",
    "        plt.fill_between(x, f-sig,f+sig, \n",
    "                         where=None, facecolor='r', alpha=0.3)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('theta')\n",
    "        plt.plot(x0[0]*np.ones(2), [y[0],y[1]], 'k--')\n",
    "        plt.axis([x[0],x[1],y[0],y[1]])\n",
    "\n",
    "        plt.subplot(2,2,2)\n",
    "        if model=='lin':\n",
    "            plt.loglog(x0, mu_true, 'ks', markersize=7)\n",
    "            plt.loglog(x0, out_[0,0,0], 'rd')\n",
    "            plt.loglog(x0, ahat_SNPE + x0 * bhat_SNPE, 'gd')\n",
    "            plt.axis([0.9*x0s[0],1.1*x0s[-1],1/10,10.])\n",
    "        elif model=='log':\n",
    "            plt.semilogx(x0, mu_true, 'ks', markersize=7)\n",
    "            plt.semilogx(x0, out_[0,0,0], 'rd')\n",
    "            plt.semilogx(x0, ahat_SNPE + x0 * bhat_SNPE, 'gd')\n",
    "            plt.axis([0.9*x0s[0],1.1*x0s[-1],-0.8,0.8])            \n",
    "        plt.ylabel('posterior mean')\n",
    "        \n",
    "        plt.subplot(2,2,4)\n",
    "        plt.loglog(x0, sig2_true, 'ks', markersize=7)\n",
    "        plt.loglog(x0, out_[0,0,1], 'ro')\n",
    "        plt.loglog(x0, gamma2hat_SNPE, 'go')\n",
    "        if model=='lin':\n",
    "            plt.axis([0.9*x0s[0],1.1*x0s[-1],4/100,0.2])\n",
    "        elif model=='log':\n",
    "            plt.axis([0.9*x0s[0],1.1*x0s[-1],0.09,1.0])        \n",
    "        plt.xlabel('x0')\n",
    "        plt.ylabel('posterior variance')\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        \n",
    "        if model == 'lin':\n",
    "            plt.title('$p( theta|x) = N( theta | x, \\sigma^2)$')\n",
    "        elif model=='log':\n",
    "            plt.title('$p( theta|x) = N( theta | \\log(x), \\sigma^2)$')\n",
    "\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.title('proposal ' +str(proposal_form))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## simulation setup ##\n",
    "\n",
    "eta2 = 1.0   # prior variance\n",
    "sig2 = 1.0/9 # likelihood variance, 1/9 here means posterior 10x tighter than prior\n",
    "\n",
    "N    = 10000  # number of simulations per fit (can go large here)\n",
    "# this setup should allow narrow-enough proposals to see a clear difference between full fit local approximation.\n",
    "\n",
    "df   = 3     # degree of freedom for student-t proposal (choose df=3 for best importance sampling)\n",
    "x0s = [0.5, 1., 1.5, 2.] # x0 should be covered by prior*likelihood p(x0|theta)p(\\theta). \n",
    "                         # Choosing only positive values for pain-free plotting under non-linear (= log) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit and plot one MDN/conditional density estimation per x0\n",
    "print('number of x0s: ', len(x0s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results\n",
    "\n",
    "- each figure below plots several distinct fits of CDELFI (red) and SNPE (green) to either a linear (scenario#1) or a non-linear (scenario#2) toy problem. \n",
    "- on left pannel, plotting example data $(x,\\theta)$ in joint-space with overlaid MDN fit for $\\mu_\\phi(x) = \\alpha + \\beta x$ plus/minus one learned standard deviation $\\sigma_\\phi(x) = \\gamma$. Shown data points are drawn from the (broad) real prior. Individual fits for different $x_0$ actually use different data-sets drawn from proposals $\\tilde{p}(\\theta) \\approx p(\\theta|x_0)$ close to the respective $x_0$ (not plotted for clarity). \n",
    "- on right pannel, showing final estimate for posterior $p(\\theta|x_0) = \\mathcal{N}(\\theta | \\mu, \\sigma^2)$. For SNPE, $\\mu = \\mu_\\phi(x_0)$, and $\\sigma^2 = \\sigma_\\phi(x_0)^2$. For CDELFI, final posterior is obtained from an additional analytic correction step of the MDN outputs $\\mu_\\phi(x_0), \\sigma_\\phi(x_0)^2$. Black $=$ ground-truth, red $=$ CDELFI, green $=$ SNPE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario 1: linear toy problem\n",
    "\n",
    "- MDN matches problem-complexity (can solve problem perfectly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario #1a: Gaussian proposals\n",
    "\n",
    "- Gaussian proposals both for CDELFI and SNPE (expecting importance sampling to be in trouble)\n",
    "- CDELFI: **local** approximations of $p(\\theta|x)$, which are **corrected** for analyically\n",
    "- SNPE: **roughly global** fitting of MDN to $p(\\theta|x)$, but poor fits for some $x_0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'normal', 'lin', eta2, sig2, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario #1b: Student-t proposals for SNPE\n",
    "\n",
    "- Student-t proposals SNPE to help importance sampling, giving better-quality results overall\n",
    "- SNPE now **perfectly global** fit of MDN to $p(\\theta|x)$ (as targeted by SNPE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'studentT', 'lin', eta2, sig2, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario #2: non-linear toy problem\n",
    "\n",
    "- still closed-form solution to posterior:  $p(\\theta|x) = \\mathcal{N}(\\theta \\ | \\ \\log(x), \\ \\gamma^2)$\n",
    "- MDN now too simple for problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario #2a: Gaussian proposals\n",
    "\n",
    "- CDELFI: **local** approximations of $p(\\theta|x)$, which are **corrected** for analyically. Overall, some clear errors now (model missmatch!)\n",
    "- SNPE: somewhat **local** fitting of MDN to $p(\\theta|x)$, somewhat okayish results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'normal', 'log', eta2, sig2, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario #2b: Student-t proposals for SNPE\n",
    "\n",
    "- SNPE: **almost-perfectly global** fit of MDN to $p(\\theta|x)$, here leading to vastly overestimated variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'studentT', 'log', eta2, sig2, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary of findings\n",
    "\n",
    "- CDELFI learns local approximations to $p(\\theta | x)$ as seen when comparing across different $x_0$. \n",
    "- SNPE tends to learn globally, i.e. MDN fits are more similar across different $x_0$. \n",
    "- differences in SNPE MDN fits almost vanish when using Student-t proposals. \n",
    "- on non-linear problems with simple linear MDNs, global fits can turn out much poorer than local approximations. \n",
    "\n",
    "\n",
    "## interpretation / (tentative) conclusion\n",
    "\n",
    "- SNPE learns globally unless importance sampling breaks down. \n",
    "- complex interplay between importance sampling and local vs. global fitting \n",
    "- incomplete importance sampling allows local approximations of $p(\\theta|x)$, but correction for proposal priors is incomplete ? \n",
    "\n",
    "\n",
    "- global fitting of SNPE should be awesome for amortization. \n",
    "- questions of model complexity $q_\\phi(\\theta | x)$ however seemingly more pressing for SNPE than for CDELFI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibration kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in Appendix A of Lueckmann et al. (2018), a calibration-kernel $K(x, x_0)$ can be used to manipulate the loss into a local approximation around $x_0$ without changing the target posterior and solution around $x_0$. For $N \\to \\infty$, the kernel-calibrated loss becomes\n",
    "\n",
    "$$\\mathcal{L}_{K}(\\phi) = D_{KL}(p(\\theta|x) p_K(x) || q_\\phi(\\theta|x) p_K(x))$$\n",
    "\n",
    "For the Gaussian toy case, we can actually compute the kernel $K$ that induces the same marginal $p(x)$ as otherwise the proposal prior $\\tilde{p}(\\theta)$ would have induced. For $K(x,x_0) = \\mathcal{N}(x_K, \\sigma_K^2)$, we in the case of a simple Gaussian likelihood $p(x | \\theta)$ have that\n",
    "$$ p_K(x) = \\frac{1}{Z_k} \\int K(x, x_0) p(\\theta) p(x |\\theta) d\\theta = \\mathcal{N}(x | \\mu_\\epsilon,\\sigma_\\epsilon^2), $$\n",
    "\n",
    "i.e. $p_K(x)$ is another Gaussian. Additionally, for proposal priors $\\tilde{p}(\\theta) = \\mathcal{N}(\\frac{\\eta^2}{\\eta^2+\\sigma^2}x_0, \\eta^2 - \\frac{\\eta^4}{\\eta^2 + \\sigma^2})$ we find that \n",
    "$$p_K(x) = \\tilde{p}(x) = \\int \\tilde{p}(\\theta) p(x|\\theta) d\\theta$$ \n",
    "for $x_K = \\frac{\\eta^2}{\\eta^2 - \\xi^2} x_0$, $\\sigma_K^2 = \\frac{(\\sigma^2 + \\xi^2)(\\sigma^2 + \\eta^2)}{\\eta^2 - \\xi^2}$. Thus for such Kernels $K(x, x_0) \\propto \\mathcal{N}(x | x_K, \\sigma_K^2)$, we have \n",
    "\n",
    "$$\\mathcal{L}_{K}(\\phi) = D_{KL}(p(\\theta|x) p_K(x) || q_\\phi(\\theta|x) p_K(x))\n",
    "=  D_{KL}(p(\\theta|x) \\tilde{p}(x) || q_\\phi(\\theta|x) \\tilde{p}(x))$$\n",
    "\n",
    "i.e. we learn the density $p(\\theta|x)$ over the same range of $x$ as CDELFI learns from its loss\n",
    "\n",
    "$$\\tilde{\\mathcal{L}}(\\phi) = D_{KL}( \\tilde{p}(\\theta) p(x|\\theta) || q_\\phi(\\theta|x) \\tilde{p}(x))$$\n",
    "\n",
    "with the major difference being that the CDELFI results from $\\tilde{\\mathcal{L}}(\\phi)$ need to be correct for $\\tilde{p}(\\theta)$ while those from $\\mathcal{L}_{K}(\\phi)$ do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple calibration kernel for non-linear toy example \n",
    "- Gaussian-shaped calibration $\\mathcal{N}(\\mu_K = x_0, \\sigma_K^2 = \\epsilon^2)$ re-establishes locality of SNPE fits. \n",
    "- small $\\sigma_K^2$ effectively discard a lot of data, meaning we can run into estimator variance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'studentT', 'lin', eta2, sig2, N, eps2=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SNPE can fit **better** than CDELFI on nonlinear problems, potentially because the kernel is focusing the approximation onto $x_0$ (whereas the proposal $\\tilde{p}(\\theta)$ here and in Papamakarios et al. focuses on $\\frac{\\sigma^2}{\\xi^2 + \\sigma^2} x_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fun(df, x0s, 'studentT', 'log', eta2, sig2, N, eps2=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbd\n",
    "- compute calibration kernel effectively equal to proposal prior also for log-gaussian likelihood. \n",
    "- rigorously compare locality of results between CDELFI and SNPE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "dtype = theano.config.floatX\n",
    "ndim_x = 1\n",
    "\n",
    "class KernelLayer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, B=lasagne.init.Normal(0.01), **kwargs):\n",
    "        super(KernelLayer, self).__init__(incoming, **kwargs)\n",
    "        num_inputs = self.input_shape[1]\n",
    "        self.eye = T.eye(num_inputs)\n",
    "        self.B = self.add_param(B, (num_inputs, ), name='B')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        D = T.dot(self.B*self.eye, self.B*self.eye.T)\n",
    "        inner = (input.dot(D)*input).sum(axis=1)\n",
    "        return T.exp(-inner)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0],)\n",
    "\n",
    "    \n",
    "class KernelLayer_offset(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, B=lasagne.init.Normal(0.01), Z=lasagne.init.Normal(0.01), **kwargs):\n",
    "        super(KernelLayer_offset, self).__init__(incoming, **kwargs)\n",
    "        num_inputs = self.input_shape[1]\n",
    "        self.eye = T.eye(num_inputs)\n",
    "        self.B = self.add_param(B, (num_inputs, ), name='B')\n",
    "        self.Z = self.add_param(Z, (1,), name='Z')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        D = T.dot(self.B*self.eye, self.B*self.eye.T)\n",
    "        inner = (input.dot(D)*input).sum(axis=1)\n",
    "        return T.exp(-inner + self.Z)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0],)    \n",
    "    \n",
    "input_var = T.fmatrix('inputs')\n",
    "target_var = T.fvector('targets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "df = 3\n",
    "proposal_form = 'normal'\n",
    "model = 'lin'\n",
    "\n",
    "x0s = np.array([0.5, 1.0, 1.5, 2.0])\n",
    "eta2 = 1.0   # prior variance\n",
    "sig2 = 1.0/1 # likelihood variance, 1/9 here means posterior 10x tighter than prior\n",
    "eps2 = 1e20\n",
    "N    = 10000  # number of simulations per fit (can go large here)eps2 = 1e20\n",
    "\n",
    "seed = 42\n",
    "s = ds.Identity()\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))    \n",
    "if model=='lin':\n",
    "    m = Gauss(dim=n_params, noise_cov=sig2, seed=seed)\n",
    "elif model=='log':\n",
    "    m = util.LogGauss(dim=n_params, noise_cov=sig2, seed=seed)   \n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "data = g.gen(1000, verbose=False)\n",
    "params_p, stats_p = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "for idx0 in range(len(x0s)):\n",
    "\n",
    "    x0 = np.array([x0s[idx0]])\n",
    "    sig2_true = 1./(1/eta2+1/sig2)\n",
    "    if model=='lin':\n",
    "        mu_true   = (x0/sig2)*sig2_true\n",
    "    elif model=='log':\n",
    "        mu_true   = (np.log(x0)/sig2)*sig2_true\n",
    "\n",
    "    ksi2 = sig2_true\n",
    "    nu = mu_true\n",
    "\n",
    "    # 'fit CDELFI MDN\n",
    "    ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                     S=ksi2 * np.eye(n_params),\n",
    "                     seed=seed)\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    # gen data\n",
    "    data = g.gen(N, verbose=False)\n",
    "    params, stats = data[0], data[1]\n",
    "\n",
    "    dx = (stats - x0).astype(np.float32)\n",
    "        \n",
    "    iws = util.get_weights(proposal_form, eta2, ksi2, 1e20, x0, nu, \n",
    "                           stats.reshape(-1,1), params.reshape(-1,1), df=df, a=None,b=None).astype(np.float32)\n",
    "    w = iws\n",
    "    w = w / np.sum(w)\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (1 - prediction*target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params_k = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params_k, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    mu_th =   np.dot(w, params )\n",
    "    sig2_th = np.dot(w,(params-mu_th)**2)\n",
    "\n",
    "    print('raw and importance sampled data means')\n",
    "    print(params_p.mean(axis=0),params.mean(axis=0),mu_th.flatten())\n",
    "\n",
    "    print('raw and importance sampled data variances')\n",
    "    print(params_p.var(axis=0), params.var(axis=0), sig2_th.flatten())\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogy(stats, iws, 'ro')\n",
    "    plt.semilogy(stats, 1./pred, 'bo')\n",
    "    plt.xlabel('summary statistic')\n",
    "    plt.ylabel('log')\n",
    "    plt.plot([x0,x0], [np.min(iws), np.max(1./pred)], 'k--')\n",
    "    plt.legend(['weights', 'inv. kernel', 'xo'], frameon=True)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(stats, pred*iws, 'go')\n",
    "    plt.xlabel('summary statistic')\n",
    "    plt.ylabel('log')\n",
    "    plt.plot([x0,x0], [np.min(iws*pred), np.max(iws*pred)], 'k--')\n",
    "    plt.legend(['kernel * weights', 'xo'], frameon=True)\n",
    "\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.hist(np.log10(iws), normed=True, bins=np.linspace(-3,2,50), color='r')\n",
    "    m_M = np.max(np.histogram(np.log10(iws), normed=True, bins=np.linspace(-3,2,50))[0])\n",
    "    w = iws/np.sum(iws)\n",
    "    plt.text(-2.9, 0.7*m_M, 'ESS : ' +str(np.round(100/np.sum(w**2))/100))\n",
    "    plt.text(-2.9, 0.5*m_M, 'Var : ' +str(np.round(100*np.var(iws))/100))\n",
    "    plt.xticks(np.arange(-3,3), ['10^'+str(i) for i in np.arange(-3,3)])\n",
    "    plt.title('weights')\n",
    "    plt.yticks([])    \n",
    "    \n",
    "    plt.subplot(2,3,6)\n",
    "    plt.hist(np.log10(pred*iws), normed=True, bins=np.linspace(-3,2,50), color='g')\n",
    "    m_M = np.max(np.histogram(np.log10(pred*iws), normed=True, bins=np.linspace(-3,2,50))[0])\n",
    "    w = (iws*pred)/np.sum(iws*pred)\n",
    "    plt.text(-2.9, 0.7*m_M, 'ESS : ' +str(np.round(100/np.sum(w**2))/100))\n",
    "    plt.text(-2.9, 0.5*m_M, 'Var : ' +str(np.round(100*np.var(iws*pred))/100))\n",
    "    plt.xticks(np.arange(-3,3), ['10^'+str(i) for i in np.arange(-3,3)])\n",
    "    plt.title('weights*kernel')\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log-model\n",
    "- summary statistics only positive !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "df = 3\n",
    "proposal_form = 'normal'\n",
    "model = 'lin'\n",
    "\n",
    "x0s = np.array([0.5, 1.0, 1.5, 2.0])\n",
    "eta2 = 1.0   # prior variance\n",
    "sig2 = 1.0/9 # likelihood variance, 1/9 here means posterior 10x tighter than prior\n",
    "eps2 = 1e20\n",
    "N    = 10000  # number of simulations per fit (can go large here)eps2 = 1e20\n",
    "\n",
    "seed = 42\n",
    "s = ds.Identity()\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))    \n",
    "if model=='lin':\n",
    "    m = Gauss(dim=n_params, noise_cov=sig2, seed=seed)\n",
    "elif model=='log':\n",
    "    m = util.LogGauss(dim=n_params, noise_cov=sig2, seed=seed)   \n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "data = g.gen(1000, verbose=False)\n",
    "params_p, stats_p = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "for idx0 in range(len(x0s)):\n",
    "\n",
    "    x0 = np.array([x0s[idx0]])\n",
    "    sig2_true = 1./(1/eta2+1/sig2)\n",
    "    if model=='lin':\n",
    "        mu_true   = (x0/sig2)*sig2_true\n",
    "    elif model=='log':\n",
    "        mu_true   = (np.log(x0)/sig2)*sig2_true\n",
    "\n",
    "    ksi2 = sig2_true\n",
    "    nu = mu_true\n",
    "\n",
    "    # 'fit CDELFI MDN\n",
    "    ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                     S=ksi2 * np.eye(n_params),\n",
    "                     seed=seed)\n",
    "    g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "    # gen data\n",
    "    data = g.gen(N, verbose=False)\n",
    "    params, stats = data[0], data[1]\n",
    "\n",
    "    dx = (stats - x0).astype(np.float32)\n",
    "        \n",
    "    iws = util.get_weights(proposal_form, eta2, ksi2, 1e20, x0, nu, \n",
    "                           stats.reshape(-1,1), params.reshape(-1,1), df=df, a=None,b=None).astype(np.float32)\n",
    "    w = iws\n",
    "    w = w / np.sum(w)\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (1 - prediction*target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params_k = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params_k, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    mu_th =   np.dot(w, params )\n",
    "    sig2_th = np.dot(w,(params-mu_th)**2)\n",
    "\n",
    "    print('raw and importance sampled data means')\n",
    "    print(params_p.mean(axis=0),params.mean(axis=0),mu_th.flatten())\n",
    "\n",
    "    print('raw and importance sampled data variances')\n",
    "    print(params_p.var(axis=0), params.var(axis=0), sig2_th.flatten())\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogy(stats, iws, 'ro')\n",
    "    plt.semilogy(stats, 1./pred, 'bo')\n",
    "    plt.xlabel('summary statistic')\n",
    "    plt.ylabel('log')\n",
    "    plt.plot([x0,x0], [np.min(iws), np.max(1./pred)], 'k--')\n",
    "    plt.legend(['weights', 'inv. kernel', 'xo'], frameon=True)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(stats, pred*iws, 'go')\n",
    "    plt.xlabel('summary statistic')\n",
    "    plt.ylabel('log')\n",
    "    plt.plot([x0,x0], [np.min(iws*pred), np.max(iws*pred)], 'k--')\n",
    "    plt.legend(['kernel * weights', 'xo'], frameon=True)\n",
    "\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.hist(np.log10(iws), normed=True, bins=np.linspace(-3,2,50), color='r')\n",
    "    m_M = np.max(np.histogram(np.log10(iws), normed=True, bins=np.linspace(-3,2,50))[0])\n",
    "    w = iws/np.sum(iws)\n",
    "    plt.text(-2.9, 0.7*m_M, 'ESS : ' +str(np.round(100/np.sum(w**2))/100))\n",
    "    plt.text(-2.9, 0.5*m_M, 'Var : ' +str(np.round(100*np.var(iws))/100))\n",
    "    plt.xticks(np.arange(-3,3), ['10^'+str(i) for i in np.arange(-3,3)])\n",
    "    plt.title('weights')\n",
    "    plt.yticks([])    \n",
    "    \n",
    "    plt.subplot(2,3,6)\n",
    "    plt.hist(np.log10(pred*iws), normed=True, bins=np.linspace(-3,2,50), color='g')\n",
    "    m_M = np.max(np.histogram(np.log10(pred*iws), normed=True, bins=np.linspace(-3,2,50))[0])\n",
    "    w = (iws*pred)/np.sum(iws*pred)\n",
    "    plt.text(-2.9, 0.7*m_M, 'ESS : ' +str(np.round(100/np.sum(w**2))/100))\n",
    "    plt.text(-2.9, 0.5*m_M, 'Var : ' +str(np.round(100*np.var(iws*pred))/100))\n",
    "    plt.xticks(np.arange(-3,3), ['10^'+str(i) for i in np.arange(-3,3)])\n",
    "    plt.title('weights*kernel')\n",
    "    plt.yticks([])\n",
    "    \n",
    "    #plt.savefig('kernel_learning_logGaussianToy_example_basicLoss_offsetKernel_gaussianProposals.pdf')\n",
    "    \n",
    "    plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
