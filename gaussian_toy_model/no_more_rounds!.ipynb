{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No more rounds!\n",
    "\n",
    "\n",
    "### SNPE, CDELFI and rounds\n",
    "\n",
    "- SNPE and CDELFI are methods to fit a facotrizing model $q_\\phi(\\theta|x)p(x)$ for the joint density $p(x,\\theta)$ of parameters $\\theta$ and summary statistics $x$ defined by a stochastic simulation model $p(x|\\theta)$ and prior $p(\\theta)$. \n",
    "- both methods support proposal distributions $\\tilde{p}(\\theta)$ to substitute the prior $p(\\theta)$ during fitting, allowing to focus the density learning on regions of $p(x,\\theta)$ where $x \\approx x_0$ for actually observed data $x_0$. \n",
    "- coming up with such proposal distributions $\\tilde{p}(\\theta)$ is cumbersome - the standard approach of both methods is to run a sequence of 'rounds' - each being a full model fit of its own that initiates $\\tilde{p}(\\theta) \\leftarrow q_\\phi(\\theta \\ | \\ x_0)$ with the posterior estimate from the previous round.\n",
    "\n",
    "\n",
    "- rounds suck.\n",
    "\n",
    "\n",
    "- The simple assignment $\\tilde{p}(\\theta) \\leftarrow q_\\phi(\\theta \\ | \\ x_0)$ introduces hard-to-control dynamics across rounds.\n",
    "- initial $\\tilde{p}(\\theta)$ might be poor and not focus well on $x_0$, but early rounds nonetheless need to be run with large-enough simulated data-sets $\\{(x_n, \\theta_n)\\}_{n=1}^N$ to generate a better proposal for next round - this in practice means sampling enough data to fit a full mixture density network!\n",
    "\n",
    "- a different approach is to parametrize the proposal and try to learn both proposal $p_\\psi(\\theta)$ and conditional density $q_\\phi(\\theta \\ | \\ x)$ jointly. \n",
    "\n",
    "- but what loss term should we use to choose a good proposal parameter $\\psi$?\n",
    "\n",
    "### parametrized proposal distributions\n",
    "\n",
    "- another problem here however also comes already from the SNPE/CDELFI loss, \n",
    "$$\\mathcal{L}(\\theta,\\psi) = - \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p(x)) = \\int p_\\psi(\\theta) p(x|\\theta) \\log q_\\phi(\\theta | x) dx d\\theta + \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta $$\n",
    "When only optimizing for $\\phi$, treating $\\psi$ constant, this loss allows to fully avoid evaluating the unknown densities $p(x | \\theta)$ and especially $p(x) =  \\int p(x|\\theta) p_\\psi(\\theta) = f(x, \\psi)$ through MC-approximations of the integrals. It however no longer does so for variable $\\psi$. Likelihood and marginal evaluations need to be estimated, which requires costly simulations. This is particularly true for the marginal density $p(x)$ that would in practice require many likelihood estimates $p(x|\\theta_i), \\theta_i \\sim p_\\psi(\\theta)$ to approximate well with Monte Carlo. \n",
    "\n",
    "- it is interesting to note that the rhs term that is constant in $\\phi$ (but not in $\\psi$), \n",
    "$$ - \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta := H[p(X,\\theta),p(X)]$$\n",
    "is the cross-entropy between target joint distribution $p(\\theta)p(x|\\theta)$ and the problematic marginal $p(x)$, meaning that this term can provide information to fit a parametrized model $p_\\omega(x)$ to approximate the marginal $p(x)$ induced by $p_\\psi(\\theta)$!\n",
    "- $H[p_\\psi(X,\\theta),p_\\omega(X)] = - \\int p_\\psi(\\theta,x) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta|x)p(x)} dx d\\theta = D_{KL}(p(x) ||  p_\\omega(x)) + \\int p_\\psi(\\theta,x) \\log p_\\psi(\\theta|x) dx d\\theta$ is minimized w.r.t. $\\omega$ if $p_\\omega(x) = p(x)$ almost everywhere. \n",
    "\n",
    "- thanks to this link between proposal $p_\\psi$ and marginal $p_\\omega$, having a good marginal estimate in turn can now help us choose a good proposal prior: the basis of most variational approximations is indeed to try to maximimize the marginal data-likelihood $p_\\omega(x_0)$.  \n",
    "\n",
    "### adaptive proposal loss\n",
    "\n",
    "$$\\mathcal{L}(\\theta,\\psi,\\omega) = - \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p_\\omega(x)) + \\log p_\\omega(x_0) \\\\ \n",
    "= \\int p_\\psi(\\theta) p(x|\\theta) \\log q_\\phi(\\theta | x) dx d\\theta + \\int p_\\psi(\\theta) p(x|\\theta) \\log \\frac{p_\\omega(x)}{p_\\psi(\\theta)p(x|\\theta)} dx d\\theta + \\log p_\\omega(x_0)$$\n",
    "\n",
    "- only unparametrized part of the loss is the likelihood $p(x|\\theta)$, the rest can be jointly optimized with (stochastic) gradient descent!\n",
    "- gradient w.r.t. $\\phi$ is virtually unchanged relative to SNPE / CDELFI\n",
    "- gradient w.r.t. $\\psi$ tries to balance maximizing conditional probabilities $q_\\phi(\\theta|x)$ while keeping the induced marginal $p(x)$ close to $p_\\omega(x)$.  \n",
    "- gradient w.r.t. $\\omega$ tries to balance staying close to $p(x)$ and maximizing the observed-data likelihood $p_\\omega(x_0)$.  \n",
    "- remember $\\frac{\\partial}{\\partial{}\\psi} \\int p_\\psi(\\theta) f(\\theta) d\\theta = \\int p_\\psi(\\theta) f(\\theta)\\frac{\\partial}{\\partial{}\\psi} \\log p_\\psi(\\theta) d\\theta$ under mild constraints, i.e. we can use Monte Carlo for gradients wrt. the proposal\n",
    "- we are left with an evaluation of the log-likelihood in the new loss! In practice, we will need the conditional entropy $H_{x|\\theta}[X|\\theta_n]$ over summary statistics at sampled parameters $\\theta_n$. Since $H_{x|\\theta}$ maps only from parameters $\\theta$ into the reals (whereas $p(x|\\theta)$ is defined over the joined set of all $(x,\\theta)$), this quantity may be easier to aquire than the likelihood, esp. when there are much fewer parameters than summary statistics. \n",
    "\n",
    "\n",
    "### questions\n",
    "- do the three major loss terms ($q_\\phi$, $H[p(X,\\theta)|p(X)], \\log p_\\omega(x_0)$) work together in concert as desired, or will one dominate (e.g. will the proposal overadapt to the current MDN $\\phi$ rather than try to follow and give 'wiggle-space' $p_\\omega(x_0)$)? Will we need to introduce weights, e.g. $\\lambda \\log p_\\omega(x_0)$?  \n",
    "- what are good model assumptions for $p_\\omega$? How important is capturing the data marginal $p(x)$, how important is matching proposal*likelihood ?\n",
    "- how to best estimate the conditional entropies $H[X|\\theta]$ from simulations? Repeated sampling for each $\\theta$? Fit another network?\n",
    "- can this be extended with importance sampling? The above scheme requires analytical correction for the final proposal distribution (after convergence), but can we relax this when re-introducing importance weights? If yes, does the ability to choose another proposal distribution after each individual batch-gradient allow us to get more stable importance sampling schemes?\n",
    "- can this be extended to SVI? Note that $p_\\omega(x)$ essentially just completes the parametrization of the joint-data likelihood $q_\\phi(\\theta_n|x_n)p_\\omega(x_n) = p(\\theta_n,x_n | \\phi, \\omega)$, but that a constantly updated proposal $p_\\psi(\\theta)$ destroys the notion of having a single fixed data-set. How do we do Bayesian online-learning when the data-distribution is known to be a (stochastically) moving target? \n",
    "- no more rounds? \n",
    "\n",
    "No more rounds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy problem setup\n",
    "- how much can be done analytical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prior: \n",
    "\n",
    "$p(\\theta) = \\mathcal{N}(\\theta \\ | \\ 0, \\eta^2)$\n",
    "\n",
    "proposal prior: \n",
    "\n",
    "$p_\\psi(\\theta) = \\mathcal{N}(\\theta \\ | \\ \\nu, \\xi^2)$\n",
    "\n",
    "simulator: \n",
    "\n",
    "$p(x \\ | \\ \\theta) =  \\mathcal{N}(x \\ | \\ \\theta, \\sigma^2)$\n",
    "\n",
    "analytic posteriors: \n",
    "\n",
    "$p(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\eta^2}{\\eta^2 + \\sigma^2} x, \\eta^2 - \\frac{\\eta^4}{\\eta^2 + \\sigma^2})$ \n",
    "\n",
    "$\\tilde{p}(\\theta \\ | \\ x) = \\mathcal{N}(\\theta \\ | \\frac{\\xi^2}{\\xi^2 + \\sigma^2} x + \\frac{\\sigma^2}{\\xi^2 + \\sigma^2} \\nu, \\xi^2 - \\frac{\\xi^4}{\\xi^2 + \\sigma^2})$\n",
    "\n",
    "variational marginal:\n",
    "\n",
    "$p_\\omega(x) = \\mathcal{N}(x \\ | \\ \\tau, \\rho^2)$\n",
    "\n",
    "Data:\n",
    "\n",
    "$(x_n, \\theta_n) \\sim p(\\theta) p(x \\ | \\ \\theta) = \\mathcal{N}( (x_n, \\theta_n) \\ | \\ (\\nu, \\nu), \n",
    "\\begin{pmatrix}\n",
    "\\xi^{2} + \\sigma^{2} &  \\xi^{2}  \\\\\n",
    "\\xi^{2} & \\xi^{2}  \\\\\n",
    "\\end{pmatrix})$\n",
    "\n",
    "Model: \n",
    "\n",
    "$ q_\\phi(\\theta_n | x_n) = \\mathcal{N}(\\theta_n \\ | \\ \\mu_\\phi(x_n), \\sigma^2_\\phi(x_n))$\n",
    "\n",
    "$ (\\mu_\\phi(x), \\sigma^2_\\phi(x)) = MDN_\\phi(x) = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} x + \\begin{pmatrix} \\alpha \\\\ \\gamma^2 \\end{pmatrix}$\n",
    "\n",
    "Loss: \n",
    "\n",
    "$\\mathcal{L}(\\phi, \\psi, \\omega) = - \\lambda \\ D_\\mbox{KL}(p_\\psi(\\theta) \\ p(x|\\theta) \\ || \\ q_\\phi(\\theta|x) p_\\omega(x)) + \\log p_\\omega(x_0)$\n",
    "\n",
    "Gradients: \n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\phi} = \\frac{\\partial}{\\partial\\phi} \\int p(x|\\theta) p_\\psi(\\theta) \\ \\log q_\\phi(\\theta\\ | \\ x) \\ dx d\\theta \\approx \\frac{1}{N} \\sum_n \\frac{\\partial}{\\partial\\phi} \\log q_\\phi(\\theta_n | x_n)$\n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\omega} = \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_0) + \\lambda \\ \\frac{\\partial}{\\partial\\omega}  \\int p(x|\\theta) p_\\psi(\\theta) \\ \\log p_\\omega(x) \\ dx d\\theta \\approx  \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_0) + \\frac{\\lambda}{N} \\sum_n \\frac{\\partial}{\\partial\\omega} \\log p_\\omega(x_n)$\n",
    "\n",
    "$\\frac{\\partial\\mathcal{L}}{\\partial\\psi} \n",
    "%= \\frac{\\partial}{\\partial\\psi} \\int p(x|\\theta) p_\\psi(\\theta) \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) - \\log p(x|\\theta) p_\\psi(\\theta) \\right) dx d\\theta \n",
    "= \\int p(x|\\theta) p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] \\left( \\log q_\\phi(\\theta|x) + \\log p_\\omega(x) \\right) dx d\\theta - \\int p_\\psi(\\theta) \\left[ \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta)\\right] H[X|\\theta] d\\theta - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta] \\\\ \n",
    "\\approx \\frac{1}{N}\\sum_n \\left( \\log q_\\phi(\\theta_n|x_n) + \\log p_\\omega(x_n) - H[X|\\theta_n] \\right) \\frac{\\partial}{\\partial\\psi} \\log p_\\psi(\\theta_n) - \\frac{\\partial}{\\partial\\psi} H_{\\psi}[\\theta]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stuff below is old and mainly in for recycling gradients w.r.t. $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.summarystats as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "\n",
    "from delfi.simulator.Gauss import Gauss\n",
    "    \n",
    "def gauss_weights(params, stats, mu_y, Sig_y):\n",
    "    \n",
    "    y = np.hstack((stats, params))\n",
    "    return mvn.pdf(x=y, mean=mu_y.reshape(-1), cov=Sig_y, allow_singular=True)   \n",
    "\n",
    "\n",
    "def gauss_weights_eps0(params, stats, mu_y, Sig_y):\n",
    "    \"\"\" stable version in case eps^2 is giant - stats.mvn return nonsense here \"\"\"\n",
    "    # Note: making use of the fact that covariances are zero for normal weights in SNPE/CDELFI MLE solutions\n",
    "    \n",
    "    x = -0.5 * (params-mu_y[1])**2 / Sig_y[1,1] # would like to use mvn.pdf, but that one freaks  \n",
    "    return np.exp( x.reshape(-1) )              # out for 1D problems with negative (co-)variance\n",
    "\n",
    "\n",
    "def sel_gauss_implementation(eps2, thresh=1000): \n",
    "    \n",
    "    return gauss_weights_eps0 if eps2 > thresh else gauss_weights\n",
    "\n",
    "\n",
    "#def studentT_weights(params, stats, mu_y, Sig_y):\n",
    "#    \n",
    "#    raise NotImplementedError\n",
    "#    \n",
    "#    \n",
    "#def studentT_weights_eps0(params, stats, mu_y, Sig_y, df=3):\n",
    "#    \"\"\" stable version in case eps^2 is giant - stats.mvn return nonsense here \"\"\"\n",
    "#    # Note: making use of the fact that covariances are zero for normal weights in SNPE/CDELFI MLE solutions\n",
    "#    \n",
    "#    exponent = -(df+1)/2 \n",
    "#    return (1 + (params-mu_y[1])**2/(df*Sig_y[1,1]))**exponent\n",
    "#\n",
    "#\n",
    "#def sel_studentT_implementation(eps2, thresh=1000): \n",
    "#    \n",
    "#    return studentT_weights_eps0 if eps2 > thresh else studentT_weights\n",
    "#\n",
    "\n",
    "def get_weights_fun(eps2, thresh=1000, proposal_form='normal'):\n",
    "    \n",
    "    if proposal_form=='normal':\n",
    "        selector = sel_gauss_implementation \n",
    "    elif proposal_form=='studentT':\n",
    "        selector = sel_studentT_implementation \n",
    "        \n",
    "    return selector(eps2, thresh)\n",
    "\n",
    "def get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=3): \n",
    "    \n",
    "    assert proposal_form in ('normal', 'studentT')\n",
    "    \n",
    "    if proposal_form == 'normal':\n",
    "        \n",
    "        eta2p = 1/(1/eta2 - 1/ksi2)\n",
    "        Sig_y = np.array([[eps2,0], [0,eta2p]])    \n",
    "        mu_y = np.array([ [x0[0]], [eta2/(eta2-ksi2)*nu]])\n",
    "\n",
    "        comp_weights =get_weights_fun(eps2, thresh=1000, proposal_form=proposal_form)\n",
    "        \n",
    "        normals = comp_weights(data[0], data[1], mu_y, Sig_y)\n",
    "        \n",
    "    if proposal_form == 'studentT':\n",
    "\n",
    "        exponent = -(df+1)/2 \n",
    "        proposal_pdf = (1 + (params-nu)**2/(df*ksi2))**exponent\n",
    "        prior_pdf    = mvn.pdf(x=params, mean=0., cov=eta2)\n",
    "        normals = prior_pdf / proposal_pdf\n",
    "        if eps2 < 1000:\n",
    "            calibration_kernel_pdf = mvn.pdf(x=stats, mean=x0, cov=eps2)\n",
    "            normals *= calibration_kernel_pdf\n",
    "            \n",
    "    return normals\n",
    "\n",
    "\n",
    "\n",
    "def dL_dalpha( params, stats, normals, beta, gamma2, alphas):\n",
    "\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2 * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "\n",
    "def dL_dbeta( params, stats, normals, alpha, gamma2, betas):\n",
    "\n",
    "    return -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2).sum(axis=0)\n",
    "\n",
    "def dL_dgamma2( params, stats, normals, alpha, beta, gamma2s):\n",
    "\n",
    "    tmp = (params.reshape(-1,1) - beta*stats.reshape(-1,1) - alpha)**2 / gamma2s.reshape(1,-1)\n",
    "    return 1/gamma2s.reshape(1,-1) * (normals.reshape(-1,1) * (1 - out)).sum(axis=0)\n",
    "    \n",
    "def alpha(params, stats, normals):\n",
    "    \n",
    "    N = normals.size    \n",
    "\n",
    "    Eo  = (normals * params).sum()\n",
    "    Eox = (normals * stats * params).sum()\n",
    "    Ex2 = (normals * stats**2).sum()\n",
    "    Ex  = (normals * stats).sum()\n",
    "    E1  = normals.sum()\n",
    "    \n",
    "    #ahat = (normals * (Ex2 * params - Eox * stats)).sum()\n",
    "    #ahat /= (E1 * Ex2 - Ex**2)\n",
    "    \n",
    "    ahat = (Eo - Eox/Ex2 * Ex) / (E1 - Ex**2/Ex2)\n",
    "    \n",
    "    return ahat\n",
    "\n",
    "def beta(params, stats, normals, ahat=None):\n",
    "\n",
    "    ahat = alpha(params, stats, normals) if ahat is None else ahat\n",
    "\n",
    "    Eox = (normals * stats * params).sum()\n",
    "    Ex2 = (normals * stats**2).sum()\n",
    "    Ex  = (normals * stats).sum()\n",
    "    \n",
    "    bhat = (Eox - ahat * Ex) / Ex2\n",
    "    \n",
    "    return bhat\n",
    "    \n",
    "def gamma2(params, stats, normals, ahat=None, bhat=None):\n",
    "\n",
    "    ahat = alpha(params, stats, normals) if ahat is None else ahat\n",
    "    bhat = beta(params, stats, normals, ahat) if bhat is None else bhat\n",
    "\n",
    "    gamma2hat = (normals*(params - ahat - bhat * stats )**2).sum() / normals.sum()\n",
    "    \n",
    "    return gamma2hat\n",
    "\n",
    "\n",
    "def analytic_div(out, eta2, nus, ksi2s):\n",
    "    \"\"\" analytic correction of onedimensional Gaussians for proposal priors\"\"\"\n",
    "    # assumes true prior to have zero mean!\n",
    "    # INPUTS:\n",
    "    # - out: 3D-tensor: \n",
    "    #        1st axis gives ksi2s (proposal variances), \n",
    "    #        2nd axis gives number of experiments/runs/fits\n",
    "    #        3nd axis is size 2: out[i,j,0] Gaussian mean, out[i,j,0] Gaussian variance\n",
    "    # - eta2:  prior variance (scalar)\n",
    "    # - nus:   vector of proposal prior means\n",
    "    # - ksi2s: vector of proposal prior variances\n",
    "    \n",
    "    # OUTPUTS\n",
    "    # - out_: 3D tensor of proposal-corrected posterior means and variances\n",
    "    \n",
    "    out_ = np.empty_like(out)\n",
    "    for i in range(out_.shape[0]):\n",
    "        \n",
    "        # precision and precision*mean\n",
    "        P = 1/out[i,:,1]\n",
    "        Pm = P * out[i,:,0]\n",
    "\n",
    "        # multiply with prior\n",
    "        P = P + 1/eta2\n",
    "        Pm = Pm + 0/eta2\n",
    "\n",
    "        # divide by proposal\n",
    "        P = P - 1/ksi2s[i]\n",
    "        Pm = Pm - nu/ksi2s[i]\n",
    "\n",
    "        out_[i,:,:] = np.vstack((Pm/P, 1/P)).T\n",
    "\n",
    "    return out_\n",
    "\n",
    "n_bins = 50 # number of bins for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define problem setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## problem setup ##\n",
    "\n",
    "n_params = 1\n",
    "\n",
    "assert n_params == 1 # cannot be overstressed: everything in this notebook goes downhill sharply otherwise\n",
    "\n",
    "sig2 = 1.0/9. # likelihood variance\n",
    "eta2 = 1.0     # prior variance\n",
    "eps2 = 1e20    # calibration kernel width (everything above a certain threshold will be treated as 'uniform')\n",
    "\n",
    "# pick observed summary statistics\n",
    "x0 = 0.8 * np.ones(1) #_,obs = g.gen(1) \n",
    "\n",
    "# prior and analytic (!) likelihood & posterior\n",
    "m = Gauss(dim=n_params, noise_cov=sig2)\n",
    "p = dd.Gaussian(m=0. * np.ones(n_params), \n",
    "                S=eta2 * np.eye(n_params))\n",
    "post   = dd.Gaussian(m = np.ones(n_params) * eta2/(eta2+sig2)*x0[0], \n",
    "                     S=eta2 - eta2**2 / (eta2 + sig2) * np.eye(n_params))    \n",
    "\n",
    "## simulation setup ##\n",
    "\n",
    "n_fits = 500  # number of MLE fits (i.e. dataset draws), each single-round fits with pre-specified proposal!\n",
    "N      = 500  # number of simulations per dataset\n",
    "\n",
    "# set proposal priors (one per experiment)\n",
    "ksi2s = np.array([0.01, 0.1, 0.5, 0.999]) * eta2  # proposal variance\n",
    "nus = eta2/(eta2+sig2)*x0[0]* np.ones(len(ksi2s))              # proposal mean\n",
    "\n",
    "\n",
    "res = {'normal' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       't_df10' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       't_df3'  : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       'cdelfi' : np.zeros((len(ksi2s), n_fits,2)),\n",
    "       'sig2' : sig2,\n",
    "       'eta2' : eta2,\n",
    "       'eps2' : eps2,\n",
    "       'ksi2s' : ksi2s,\n",
    "       'nus' : nus,\n",
    "       'x0' : x0,\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerical checks for gradients\n",
    "- tbd. lots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "track_rp = True\n",
    "proposal_form = 'normal'\n",
    "df = None\n",
    "\n",
    "nu = 0.\n",
    "ksi2 = 0.5 * eta2\n",
    "\n",
    "ppr = dd.Gaussian(m=nu * np.ones(n_params), \n",
    "                S=ksi2 * np.eye(n_params))\n",
    "s = ds.Identity()\n",
    "g = dg.Default(model=m, prior=ppr, summary=s)\n",
    "\n",
    "seed = 42\n",
    "g.model.seed = seed\n",
    "g.prior.seed = seed\n",
    "g.seed = seed\n",
    "\n",
    "data = g.gen(N, verbose=False)\n",
    "params, stats = data[0].reshape(-1), data[1].reshape(-1)\n",
    "\n",
    "normals = get_weights(proposal_form, eta2, ksi2, eps2, x0, nu, stats, params, df=df) if track_rp else np.ones(N)/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\alpha}$\n",
    "\n",
    "- $\\frac{\\partial}{\\partial\\alpha}$ is being difficult here. Analytic solution $\\hat{\\alpha}$ still fails to numberically set the stated partial derivative $\\frac{\\partial\\mathcal{L}}{\\partial{}\\alpha}(\\hat{\\alpha})$ to zero ...\n",
    "- Obtained $\\hat{\\alpha}$ however are pretty much sensible though (correct 'ballpark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_hat = np.array(alpha(params, stats, normals))\n",
    "\n",
    "gamma2_ = post.std**2\n",
    "alphas = np.linspace(-0.09, -0.02, 100000)\n",
    "\n",
    "beta_ = beta(params, stats, normals, alpha_hat)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alpha_hat)/gamma2_).sum(axis=0)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_ * stats.reshape(-1,1) - alphas.reshape(1,-1))/gamma2_).sum(axis=0)\n",
    "plt.plot(alphas, out)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "alphas[np.argmin(np.abs(out))], alpha_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "gamma2_ = post.std**2\n",
    "betas = np.linspace(0., 1., 1000)\n",
    "out = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - betas.reshape(1,-1) * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "plt.plot(betas, out)\n",
    "plt.show()\n",
    "\n",
    "beta_hat = beta(params, stats, normals, ahat=alpha_)\n",
    "out_hat = -2*(normals.reshape(-1,1) * (params.reshape(-1,1) - beta_hat * stats.reshape(-1,1) - alpha_)/gamma2_ * stats.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "betas[np.argmin(np.abs(out))], beta_hat, out_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerically check $\\frac{\\partial}{\\partial{}\\gamma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_ = alpha_hat\n",
    "beta_ = beta_hat\n",
    "gamma2s = np.linspace(0.0009, 0.001, 1000)\n",
    "\n",
    "# something off with below (hard-coded...) gradients now. Outcommenting numerical solution for now!\n",
    "\n",
    "tmp = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2s.reshape(1,-1)\n",
    "out = 1/gamma2s.reshape(-1,) * (normals.reshape(-1,1) * (1 - tmp)).sum(axis=0)\n",
    "\n",
    "plt.plot(gamma2s, out)\n",
    "plt.show()\n",
    "\n",
    "gamma2_hat = gamma2(params, stats, normals, ahat=alpha_, bhat=beta_)\n",
    "tmp_ = (params.reshape(-1,1) - beta_*stats.reshape(-1,1) - alpha_)**2 / gamma2_hat\n",
    "out_hat = 1/gamma2_hat * (normals.reshape(-1,1) * (1 - tmp_)).sum(axis=0)\n",
    "\n",
    "#gamma2s *= np.nan \n",
    "#out = np.zeros_like(gamma2s)\n",
    "\n",
    "# (numerical solution, analytical solution, derivate evaluated at analytical solution (should be zero-ish) ) = \n",
    "gamma2s[np.argmin(np.abs(out))], gamma2_hat, out_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
