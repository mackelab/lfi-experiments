{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from brian2tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_connection_probs(REE, k, pee):\n",
    "    p_out = pee * k/(REE + k -1)\n",
    "    p_in = REE * p_out\n",
    "    return p_in, p_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simulation_time = 3 * second\n",
    "alpha = 1.\n",
    "\n",
    "# seed the random number generator\n",
    "np.random.seed(10)\n",
    "\n",
    "# create simulation network\n",
    "net = Network()\n",
    "\n",
    "# cluster parameters \n",
    "# cluster size \n",
    "k = 50 \n",
    "# cluster coef \n",
    "ree = 1.0\n",
    "# average ee sparseness\n",
    "pee = 0.2 \n",
    "cluster_weight_factor = 1.9 \n",
    "p_in, p_out = get_cluster_connection_probs(ree, k, pee)\n",
    "\n",
    "# neuron parameters\n",
    "vt = 1\n",
    "vr = 0\n",
    "tau_e = 15*ms\n",
    "tau_i = 10*ms\n",
    "tau1 = 1 * ms\n",
    "tau2_e = 3 * ms\n",
    "tau2_i = 2 * ms\n",
    "refrac = 5 * ms\n",
    "tau_scale = 1 * ms\n",
    "\n",
    "# network parameters \n",
    "NE = 4000\n",
    "NI = 1000\n",
    "N = NE + NI\n",
    "\n",
    "# sparseness\n",
    "pie = 0.5 \n",
    "pei = 0.5 \n",
    "pii = 0.5 \n",
    "\n",
    "# weights \n",
    "wee = 0.024\n",
    "wei = -0.045\n",
    "wie = 0.014\n",
    "wii = -0.057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define neuron equation \n",
    "eqs = '''\n",
    "dv/dt = (mu-v)/tau + (I_e + I_i) / tau_scale : 1\n",
    "dI_e/dt = -(I_e - x_e)/tau2_e : 1\n",
    "dI_i/dt = -(I_i - x_i)/tau2_i : 1\n",
    "dx_e/dt = -x_e / tau1 : 1\n",
    "dx_i/dt = -x_i / tau1 : 1\n",
    "mu : 1\n",
    "tau : second\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the network \n",
    "G = NeuronGroup(N, eqs, threshold='v>vt', reset='v=vr', method='euler', refractory=refrac)\n",
    "net.add(G)\n",
    "Pe = G[:NE]\n",
    "Pi = G[NE:]\n",
    "# set E I specific membrane time constants \n",
    "Pe.tau = tau_e\n",
    "Pi.tau = tau_i\n",
    "\n",
    "# create clusters\n",
    "Nc = int(NE/k)\n",
    "PeCluster = [Pe[i*Nc:(i+1)*Nc] for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up connectivity, except for ee \n",
    "Sii = Synapses(Pi, Pi, 'w : 1', on_pre='''x_i += w''')\n",
    "Sii.connect(p=pii)\n",
    "Sii.w = wii\n",
    "net.add(Sii)\n",
    "\n",
    "Sei = Synapses(Pi, Pe, 'w : 1', on_pre='''x_i += w''')\n",
    "Sei.connect(p=pei)\n",
    "Sei.w = wei\n",
    "net.add(Sei)\n",
    "\n",
    "Sie = Synapses(Pe, Pi, 'w : 1', on_pre='''x_e += w''')\n",
    "Sie.connect(p=pie)\n",
    "Sie.w = wie\n",
    "net.add(Sie)\n",
    "\n",
    "if ree == 1: # uniform case \n",
    "    print('uniform case')\n",
    "    See = Synapses(Pe, Pe, 'w : 1', on_pre='''x_e += w''')\n",
    "    See.connect(p=pee)\n",
    "    See.w = wee\n",
    "    net.add(See)\n",
    "    \n",
    "elif ree < 0.:  \n",
    "    # list of synapse objects \n",
    "    # do the cluster connection like cross validation: cluster neuron := test idx; other neurons := train idx\n",
    "    kf = KFold(n_splits=k)\n",
    "    for idx_out, idx_in in kf.split(range(NE)):  # idx_out holds all other neurons; idx_in holds all cluster neurons\n",
    "        \n",
    "        current_cluster = G[idx_in[0]:idx_in[-1]]\n",
    "        other_exc_neurons = G[idx_out[0]:idx_out[-1]]\n",
    "        \n",
    "        # connect current cluster to itself        \n",
    "        Syn_in = Synapses(current_cluster, current_cluster, 'w : 1', on_pre='''x_e += w''')\n",
    "        Syn_in.connect(p=p_in)\n",
    "        Syn_in.w = wee * cluster_weight_factor\n",
    "        net.add(Syn_in)\n",
    "        \n",
    "        # connect current cluster to other exc neurons\n",
    "        Syn_out = Synapses(current_cluster, other_exc_neurons, 'w : 1', on_pre='''x_e += w''')\n",
    "        Syn_out.connect(p=p_out)\n",
    "        Syn_out.w = wee\n",
    "        net.add(Syn_out)        \n",
    "else: \n",
    "    for i in range(k):\n",
    "        SeeIn = Synapses(PeCluster[i], PeCluster[i], 'w : 1', on_pre='''x_e += w''')\n",
    "        SeeIn.connect(p=p_in)\n",
    "        SeeIn.w = wee * cluster_weight_factor\n",
    "        net.add(SeeIn)\n",
    "\n",
    "    # cluster-external excitatory connections (cluster only)\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if (i == j): continue\n",
    "            SeeOut = Synapses(PeCluster[i], PeCluster[j], 'w : 1', on_pre='''x_e += w''')\n",
    "            SeeOut.connect(p=p_out)\n",
    "            SeeOut.w = wee\n",
    "            net.add(SeeOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set initial values of the membrane voltage\n",
    "Pe.v = np.random.rand(NE) * (vt - vr) + vr\n",
    "Pi.v = np.random.rand(NI) * (vt - vr) + vr\n",
    "\n",
    "# set uniform resting potential around the threshold \n",
    "Pe.mu = 2 * np.random.uniform(1.1, 1.2, NE) * (vt - vr) + vr\n",
    "Pi.mu = 2 * np.random.uniform(1.0, 1.05, NI) * (vt - vr) + vr\n",
    "\n",
    "# set up monitors \n",
    "Mn = StateMonitor(G, ['v', 'I_e', 'I_i', 'x_e', 'x_i'], record=[0, NE])\n",
    "sme = SpikeMonitor(Pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally, run the network \n",
    "run(simulation_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "brian_plot(sme, markersize=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spike counts from spikemonitor for certain time window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_spiketimes(spiketimes, t, delta_t): \n",
    "    spiketimes = np.asarray(spiketimes)\n",
    "    timemask = np.logical_and(spiketimes >= t, spiketimes <= t + delta_t)\n",
    "    return spiketimes[timemask]\n",
    "\n",
    "\n",
    "def get_spike_times_for_time_window(spiketimedict, t, delta_t): \n",
    "    return {k: select_spiketimes(v, t, delta_t) for k, v in spiketimedict.items()}\n",
    "\n",
    "\n",
    "def get_spike_counts_for_time_window(spiketimedict, t, delta_t): \n",
    "    # get spike times for \n",
    "    spiketimedict = get_spike_times_for_time_window(spiketimedict, t, delta_t)\n",
    "    spikecounts = [spiketime_array.size for spiketime_array in spiketimedict.values()]\n",
    "    return np.array(spikecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spiketimedict = sme.spike_trains()\n",
    "spike_counts = get_spike_counts_for_time_window(spiketimedict, t=1., delta_t=2.)\n",
    "rates = spike_counts / 2.\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(rates, bins='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spike counts for sliding time window to calculate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_spike_counts_over_windows(spiketime_dict, t, delta_t, window_length): \n",
    "    n_neurons = len(spiketime_dict.keys())\n",
    "    length_of_recording = delta_t \n",
    "    n_time_windows = int(length_of_recording / window_length)\n",
    "    spike_counts_windows = np.zeros((n_neurons, n_time_windows))\n",
    "\n",
    "    for window_idx in range(n_time_windows): \n",
    "        wt = t + window_idx * window_length\n",
    "        spike_counts_windows[:, window_idx] = get_spike_counts_for_time_window(spiketime_dict, \n",
    "                                                                               t=wt, \n",
    "                                                                               delta_t=window_length)\n",
    "    return spike_counts_windows\n",
    "\n",
    "def calculate_correlation_matrix(spikecount_matrix_windows): \n",
    "    n_trials, n_neurons, n_time_windows = spikecount_matrix_windows.shape\n",
    "    \n",
    "    # prelocate the cov matrix \n",
    "    cov = np.zeros((n_neurons, n_neurons))\n",
    "    for trial in range(n_trials): \n",
    "        # just add them up over trials \n",
    "        cov += np.cov(spikecount_matrix_windows[trial, ...])\n",
    "    # average across trials \n",
    "    cov /= n_trials\n",
    "    \n",
    "    # get the mask of spiking neurons idx\n",
    "    spiking_mask = np.logical_not(np.diag(cov).copy() == 0)\n",
    "    \n",
    "    # remove silent neurons from the analysis \n",
    "    temp_cov = cov[spiking_mask, :]\n",
    "    new_cov = temp_cov[:, spiking_mask]\n",
    "    var = np.diag(new_cov).copy()\n",
    "    # use the outer product over the variance vector to do it vectorized \n",
    "    rho = new_cov / np.sqrt(np.outer(var, var))\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spike_counts_windows = calculate_spike_counts_over_windows(spiketimedict, t=1., delta_t=2., window_length=0.1)\n",
    "s = spike_counts_windows[np.newaxis, :, : ]\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = calculate_correlation_matrix(spikecount_matrix_windows=s)\n",
    "assert (corr.T == corr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_corr = corr.copy()\n",
    "temp_corr[np.diag_indices_from(temp_corr)] = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rho = temp_corr[np.isfinite(temp_corr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(corr.flatten(), bins='auto')\n",
    "plt.hist(rho, bins='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate CV \n",
    "\n",
    "CV if defined as the std of the ISI over the mean of the ISI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spikemonitor = sme\n",
    "# get the spike trains from the monitor: a dict with {'neuron_idx' : spike times in sec}\n",
    "sdict = spikemonitor.spike_trains()\n",
    "\n",
    "# for every neuron extract spike times between t and t + delta_t and count\n",
    "cv = []\n",
    "for idx, n_key in enumerate(sdict):\n",
    "    spike_times = sdict[n_key]\n",
    "    spike_times = np.logical_and(spike_times >= t, spike_times <= (t + delta_t))\n",
    "    if spike_times.size > 10:         \n",
    "        isi = np.diff(spike_times)\n",
    "        cv.append(np.std(isi) / np.mean(isi))\n",
    "        \n",
    "def calculate_cv(spikemonitor, t=1.0, delta_t=1.5): \n",
    "    # get the spike trains from the monitor: a dict with {'neuron_idx' : spike times in sec}\n",
    "    sdict = spikemonitor.spike_trains()\n",
    "    t *= second\n",
    "    delta_t *= second\n",
    "    \n",
    "    # for every neuron extract spike times between t and t + delta_t and count\n",
    "    cv = []\n",
    "    for idx, n_key in enumerate(sdict):\n",
    "        spike_times = sdict[n_key]\n",
    "        spike_times = spike_times[np.logical_and(spike_times >= t, spike_times <= (t + delta_t))]\n",
    "        if spike_times.size > 5:         \n",
    "            isi = np.diff(spike_times)\n",
    "            cv.append(np.std(isi) / np.mean(isi))    \n",
    "    return np.array(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvs = calculate_cv(sme)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(cvs, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_firing_rate(spikemonitor, t=1.0, delta_t=2.0):\n",
    "    t *= second\n",
    "    delta_t *= second\n",
    "    # get the spike trains from the monitor: a dict with {'neuron_idx' : spike times in sec}\n",
    "    sdict = spikemonitor.spike_trains()\n",
    "\n",
    "    # prelocate\n",
    "    spike_counts = np.zeros(len(sdict.keys()))\n",
    "    # for every neuron extract spike times between t and t + delta_t and count\n",
    "    for idx, n_key in enumerate(sdict):\n",
    "        spike_times = sdict[n_key]\n",
    "        spike_counts[idx] = np.sum(np.logical_and(spike_times >= t, spike_times <= (t + delta_t)))\n",
    "\n",
    "    # return the firing rate in spikes per second, as an array over neurons \n",
    "    return spike_counts / delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rates = calculate_firing_rate(sme, 1.0, 2.0)\n",
    "plt.hist(rates, bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdict = sme.spike_trains()\n",
    "t = 1.5 * second \n",
    "delta_t = 1.5 * second \n",
    "dt = 0.002\n",
    "t0 = 1.5 \n",
    "T = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs = []\n",
    "rates = []\n",
    "\n",
    "# sdict holds the neuron idx as key and the spiketimes as values. \n",
    "for idx, n_key in enumerate(sdict): \n",
    "    # get the spiketimes of the current neuron \n",
    "    spiketimes = sdict[n_key]\n",
    "    # restrict to the times we are interested in: 1.5s - 3.0s \n",
    "    spiketimes = spiketimes[np.logical_and(spiketimes >= t, spiketimes <=(t + delta_t))]\n",
    "    \n",
    "    # use a histogram to convert it to an array spike counts in bins of dt=2ms width\n",
    "    spikeTrain, edges = np.histogram(spiketimes, bins=int(T/dt), range=[1.5, 3.0])\n",
    "\n",
    "    # get the rate as the total number of spikes over time\n",
    "    rate = np.sum(spikeTrain) / 1.5\n",
    "    rates.append(rate)\n",
    "    \n",
    "    # calculate autocorrelation of the spike train \n",
    "    Q = np.correlate(spikeTrain, spikeTrain, mode='same') \n",
    "\n",
    "    # remove central peak \n",
    "    #Q[np.argmax(Q)] = 0\n",
    "    #Q = Q / np.max(Q)\n",
    "    # save result\n",
    "    if Q.max(): \n",
    "        corrs.append(Q )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = np.array(corrs).mean(axis=0)\n",
    "plt.hist(rates)\n",
    "plt.xlabel('rate in spikes/s')\n",
    "plt.ylabel('count'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs = np.array(corrs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(211)\n",
    "for c in corrs[:100]: \n",
    "    plt.plot(np.linspace(-750, 750, 1500), c, alpha=0.2, color='grey')\n",
    "plt.plot(np.linspace(-750, 750, 1500), corrs.mean(axis=0), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the mean correlation \n",
    "plt.figure(figsize=(15, 5))\n",
    "#c = corrs.mean(axis=0)\n",
    "#c[np.argmax(c)] = np.mean(c)\n",
    "cm = corrs.mean(axis=0)\n",
    "cm[np.argmax(cm)] = cm.mean()\n",
    "plt.plot(cm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bccn_programming]",
   "language": "python",
   "name": "conda-env-bccn_programming-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
