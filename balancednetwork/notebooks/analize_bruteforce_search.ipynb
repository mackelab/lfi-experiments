{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import delfi.distribution as dd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import scipy.stats as st\n",
    "import os \n",
    "from lfimodels.balancednetwork.BalancedNetworkSimulator import BalancedNetwork\n",
    "from lfimodels.balancednetwork.BalancedNetworkStats import BalancedNetworkStats, Identity\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "mpl_params = {'legend.fontsize': 14,\n",
    "                      'axes.titlesize': 20,\n",
    "                      'axes.labelsize': 17,\n",
    "                      'xtick.labelsize': 12,\n",
    "                      'ytick.labelsize': 12,\n",
    "             'figure.figsize' : (15, 5)}\n",
    "\n",
    "mpl.rcParams.update(mpl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_figure = True\n",
    "\n",
    "fileformat = '.png'\n",
    "dpi = 300\n",
    "\n",
    "# set name to find the folder \n",
    "simulation_name = '1513042700676306_bruteforce_n6561'\n",
    "path_to_save_folder = os.path.join('results', simulation_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_str = simulation_name[:simulation_name.find('_')]\n",
    "fullname = os.path.join(path_to_save_folder, simulation_name + '.p')\n",
    "\n",
    "\n",
    "# load data \n",
    "with open(fullname, 'rb') as handle:\n",
    "    result_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_params, stats, data, params = result_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate forward and calculate true stats \n",
    "m = BalancedNetwork(inference_params=['wxy'], n_servers=1, duration=3., first_port=8010,\n",
    "                    calculate_stats=True, dim=4)\n",
    "params_list = [true_params]\n",
    "true_stats = m.gen(params_list)[0][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmat = np.array(params).reshape(9, 9, 9, 9, 4)\n",
    "smat = np.array(stats).reshape(9, 9, 9, 9, 19)\n",
    "# calculate mean squared error between simulated and true summary stats\n",
    "mse = ((smat - true_stats)**2).mean(axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wee, wei, wie, wii = true_params\n",
    "idx_wee, idx_wei, idx_wie, idx_wii = 3, 5, 1, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_idx = np.unravel_index(np.argmin(mse), mse.shape)\n",
    "opt_stats = smat[opt_idx]\n",
    "truly_opt_stats = smat[idx_wee, idx_wei, idx_wie, idx_wii]\n",
    "opt_params = pmat[opt_idx]\n",
    "print(opt_params, opt_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at all 6 combinations of 2D inference problems and there error landscapes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses_2D = np.zeros((9, 9, 6))\n",
    "xlabels = ['wei', 'wie', 'wii', 'wie', 'wii', 'wii']\n",
    "ylabels = ['wee', 'wee', 'wee', 'wei', 'wei', 'wie']\n",
    "mses_2D[:, :, 0] = mse[:, :, opt_idx[2], opt_idx[3]]\n",
    "mses_2D[:, :, 1] = mse[:, opt_idx[1], :, opt_idx[3]]\n",
    "mses_2D[:, :, 2] = mse[:, opt_idx[1], opt_idx[2], :]\n",
    "mses_2D[:, :, 3] = mse[opt_idx[0], :, :, opt_idx[3]]\n",
    "mses_2D[:, :, 4] = mse[opt_idx[0], :, opt_idx[2], :]\n",
    "mses_2D[:, :, 5] = mse[opt_idx[0], opt_idx[1], :, :]\n",
    "\n",
    "# common color norm \n",
    "norm = colors.LogNorm(vmin=np.min(mse), vmax=np.max(mse))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx in range(6): \n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "    plt.imshow(mses_2D[:, :, idx], norm = norm)\n",
    "    plt.ylabel(ylabels[idx])\n",
    "    plt.xlabel(xlabels[idx])\n",
    "    plt.colorbar()\n",
    "    \n",
    "#plt.suptitle('MSE landscapes of all six combinations of 2D inference problems.'); \n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/2d_mse_landscapes.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is a clear minimum - at a cliff\n",
    "\n",
    "We see that there is a clear minimun in the brute force search in the 2D space for each combination of weight pairs. . However, it is often right at a cliff in the mse landscape indicating that for slightly different parameter combinations the network produced largely different summary statistics. This could make the inference problem very hard. The flattest landscape around the minimum seems to occur for the combination of $w_{ei}$ and $w_{ii}$. So it might be a good start to try the inference on this combination of weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to visualize 3 dimensions at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X = np.linspace(0.009, 0.049, n_steps)\n",
    "Y = np.linspace(0.02, 0.06, n_steps)\n",
    "Z = np.linspace(0.009, 0.049, n_steps)\n",
    "\n",
    "X, Y, Z = np.meshgrid(X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.scatter(X, Y, Z, c=mse[:, :, :, 7], cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mse[:, :, 2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiis = np.linspace(0.032, 0.072, n_steps)\n",
    "# define a common norm \n",
    "norm = colors.LogNorm(vmin=np.min(mse[:, :, 2, :]), vmax=np.max(mse[:, :, 2, :]))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for idx, w in enumerate(jiis): \n",
    "    plt.subplot(3, 3, idx + 1)\n",
    "    plt.imshow(mse[:, :, 2, idx], norm=norm)\n",
    "    plt.title('w={}'.format(round(jiis[idx], 3)))\n",
    "    plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_opt = (opt_stats - true_stats)**2\n",
    "se_truly_opt = (truly_opt_stats - true_stats)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_stats, 'o-', label='opt')\n",
    "plt.plot(opt_stats, 'o-', label='true')\n",
    "plt.plot(truly_opt_stats, 'o-', label='truly_opt')\n",
    "plt.legend()\n",
    "plt.title('Summary statistics of observed data, the MSE optimum and the theoretical optimum');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(19), se_opt, label='opt', alpha=.5)\n",
    "plt.bar(np.arange(19), se_truly_opt, label='theoretical opt', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Squared error')\n",
    "plt.title('SE of the actual minimum vs. that of the true parameter idx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory the squared error between the observed stats and the entry in the stats matrix at the true parameters should be zero because they are based on the same simulation parameter. However, due to randomness in the simulation this is not the case. The small differences in the indices of the optimum are therefore explainable by the noise in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the optimum along the $w_{ie}$ dimension\n",
    "This is the inference problem that seems to be very difficult in 1D already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('MSE')\n",
    "plt.xlabel(r'$w_{ie}$')\n",
    "plt.semilogy(mse[idx_wee, idx_wei, :, idx_wii], 'o-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a clear minimum at the true value of the parameter. However, one can already see the strong MSE at index 0. During inference the prior reached down to 0.008 covering a whole range of values for which the network goes crazy. This might result in the strong uncertainty in the resulting posteriors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mackelab]",
   "language": "python",
   "name": "conda-env-mackelab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
