{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import scipy.integrate as integrate\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import *\n",
    "from model_comparison.models import BaseModel\n",
    "\n",
    "from delfi.distribution.mixture import MoG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl_params = {'legend.fontsize': 18,\n",
    "              'legend.frameon': False,\n",
    "                      'axes.titlesize': 21,\n",
    "                      'axes.labelsize': 19,\n",
    "                      'xtick.labelsize': 15,\n",
    "                      'ytick.labelsize': 15,\n",
    "             'figure.figsize' : (18, 5)}\n",
    "\n",
    "mpl.rcParams.update(mpl_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian model comparison with Gaussian vs. Laplace model \n",
    "\n",
    "The task is to decide whether the observed data comes from Normal or from a Laplace distribution. \n",
    "\n",
    "The difficulty in this task comes only from the amount of data available. The priors should be chosen in a way that they do not favor one or the other model\n",
    "\n",
    "### Visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thetas to evaluate \n",
    "thetas = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# example background model \n",
    "example_m0 = scipy.stats.norm(0, 2)\n",
    "# signal model \n",
    "example_m1 = scipy.stats.laplace(0, 2)\n",
    "# draw example observed data \n",
    "xo = example_m1.rvs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas, example_m0.pdf(thetas), label='background')\n",
    "plt.plot(thetas, example_m1.pdf(thetas), label='signal')\n",
    "plt.axvline(xo, label='data', color='r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModel(BaseModel):\n",
    "    def __init__(self, mu, dim_param=1, sample_size=10, n_workers=1, seed=None):\n",
    "        super().__init__(dim_param=dim_param, sample_size=sample_size, n_workers=n_workers, seed=seed)\n",
    "        self.mu = mu\n",
    "        self.posterior = None\n",
    "\n",
    "    def gen_single(self, params):\n",
    "        # in multiprocessing the parameter vector additionally contains a seed\n",
    "        if self.run_parallel:\n",
    "            scale, seed = params\n",
    "            self.rng.seed(int(seed))\n",
    "        else:\n",
    "            scale = params\n",
    "        return self.rng.normal(loc=self.mu, scale=scale, size=self.sample_size)\n",
    "    \n",
    "class LaplaceModel(BaseModel): \n",
    "    def __init__(self, mu, dim_param=1, sample_size=10, n_workers=1, seed=None):\n",
    "        super().__init__(dim_param=dim_param, sample_size=sample_size, n_workers=n_workers, seed=seed)\n",
    "        self.mu = mu\n",
    "        self.posterior = None\n",
    "\n",
    "    def gen_single(self, params):\n",
    "        # in multiprocessing the parameter vector additionally contains a seed\n",
    "        if self.run_parallel:\n",
    "            scale, seed = params\n",
    "            self.rng.seed(int(seed))\n",
    "        else:\n",
    "            scale = params\n",
    "        return self.rng.laplace(loc=self.mu, scale=scale, size=self.sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "ntrain = 100000\n",
    "ntest = 100\n",
    "\n",
    "# background model prior \n",
    "prior_m0 = scipy.stats.invgamma(2)\n",
    "# signal model prior \n",
    "prior_m1 = scipy.stats.invgamma(2)\n",
    "\n",
    "# models \n",
    "m0 = GaussianModel(mu=0, sample_size=sample_size)\n",
    "m1 = LaplaceModel(mu=.3, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters from the priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ntrain + ntest\n",
    "params_m0 = prior_m0.rvs(size=int(n / 2))\n",
    "params_m1 = prior_m1.rvs(size=int(n / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from models and calculate summary stats, prepare test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m0 = m0.gen(params_m0)\n",
    "data_m1 = m1.gen(params_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and set up model index target vector \n",
    "x_all = np.vstack((data_m0, data_m1))\n",
    "\n",
    "# define model indices\n",
    "m_all = np.hstack((np.zeros(data_m0.shape[0]), np.ones(data_m1.shape[0]))).squeeze().astype(int)\n",
    "\n",
    "# get shuffled indices \n",
    "shuffle_indices = np.arange(n)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "# shuffle the data \n",
    "x_all = x_all[shuffle_indices, ]\n",
    "m_all = m_all[shuffle_indices].tolist()\n",
    "\n",
    "x, xtest = x_all[:ntrain, :], x_all[ntrain:, :]\n",
    "m, mtest = m_all[:ntrain], m_all[ntrain:]\n",
    "\n",
    "# calculate summary stats\n",
    "sx = x #np.var(x, axis=1).reshape(ntrain, 1) #calculate_stats_toy_examples(x)\n",
    "sx_test = xtest #np.var(xtest, axis=1).reshape(ntest, 1) # calculate_stats_toy_examples(xtest)\n",
    "# use training norm to normalize test data \n",
    "sx_zt, training_norm = normalize(sx)\n",
    "sx_test_zt, training_norm = normalize(sx_test, training_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the NN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationMDN(n_input=sample_size, n_hidden_units=200, n_hidden_layers=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "trainer = Trainer(model, optimizer, verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 10\n",
    "n_minibatch = int(ntrain / 100)\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx_zt, m, n_epochs=n_epochs, n_minibatch=n_minibatch)\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the NN input output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize \n",
    "stats_space = np.linspace(0, 10, 100)\n",
    "sx_vis, training_norm = normalize(stats_space, training_norm)\n",
    "# predict probs of entire range\n",
    "probs_vis = model.predict(sx_vis.reshape((stats_space.size, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "ax.plot(stats_space, prior_m0.pdf(stats_space), label='m0 prior')\n",
    "ax.plot(stats_space, prior_m1.pdf(stats_space), label='m1 prior')\n",
    "ax.plot(stats_space, probs_vis[:, 0], label='NN output: $p(m0 \\;|\\; x))$')\n",
    "ax.plot(stats_space, probs_vis[:, 1], label='NN output: $p(m1 \\;|\\; x))$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate the exact posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marli_lapace(scale, data, model_mean, prior): \n",
    "    likelihood = np.prod(scipy.stats.laplace.pdf(data, loc=model_mean, scale=scale))\n",
    "    return likelihood * prior.pdf(scale)\n",
    "\n",
    "def marli_normal(scale, data, model_mean, prior): \n",
    "    likelihood = np.prod(scipy.stats.norm.pdf(data, loc=model_mean, scale=scale))\n",
    "    return likelihood * prior.pdf(scale)\n",
    "\n",
    "# def marginal_likelihood_integrant(mu, xo, model_distr, model_std, prior_distr): \n",
    "#     return np.prod(model_distr.pdf(xo, loc=mu, scale=model_distr.std()) * prior_distr.pdf(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marli0 = np.array([integrate.quad(marli_normal, 0, 4, \n",
    "                                  args=(xo, m0.mu, prior_m0))[0] for xo in xtest])\n",
    "marli1 = np.array([integrate.quad(marli_lapace, 0, 4, \n",
    "                                  args=(xo, m1.mu, prior_m1))[0] for xo in xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sx_test[:, 0], np.log(marli0), 'o')\n",
    "plt.plot(sx_test[:, 0], np.log(marli1), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pphat = model.predict(sx_test_zt)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pp0, pphat, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pp0);\n",
    "plt.hist(pphat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.array(mtest) - pphat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mtest, '-o')\n",
    "plt.plot(pphat, '-o')\n",
    "plt.plot(pp0, '-o')\n",
    "# plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp0 = marli0 / (marli0 + marli1)\n",
    "pp1 = marli1 / (marli0 + marli1)\n",
    "mask = np.array(mtest)==1\n",
    "# plt.scatter(x=sx_test[mask, 0], y=np.log(sx_test[mask, 1]), c=pp0[mask], marker='o')\n",
    "plt.scatter(x=sx_test[:, 0], y=(sx_test[:, 1]), c=pp1, marker='o')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xos, marli0[:, 0])\n",
    "plt.plot(xos, marli1[:, 0])\n",
    "plt.plot(xos, p_m1_given_xo)\n",
    "plt.plot(xos, 1 - p_m1_given_xo)\n",
    "# plt.plot(thetas, probs_vis[:, 1], label='NN output: $p(m1 \\;|\\; x))$')\n",
    "# plt.plot(thetas, probs_vis[:, 0], label='NN output: $p(m1 \\;|\\; x))$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mackelab]",
   "language": "python",
   "name": "conda-env-mackelab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
