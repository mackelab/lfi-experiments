{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import likelihoodfree.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from likelihoodfree.io import first, last, nth\n",
    "from likelihoodfree.viz import plot_pdf_marginals\n",
    "from mackelab.plot import saverep\n",
    "from tqdm import tqdm\n",
    "\n",
    "def yaml_parse(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            return list(yaml.load_all(stream))\n",
    "        except yaml.YAMLError as exc:\n",
    "            return exc\n",
    "        \n",
    "\n",
    "y = yaml_parse(experiment)\n",
    "header = y[0].copy()\n",
    "model = header['model']\n",
    "prefix = header['prefix']\n",
    "\n",
    "del y[0]['comment']\n",
    "display(y)\n",
    "#display(Markdown(header['body']))\n",
    "#display(body)\n",
    "\n",
    "netsdir = '/home/jm/repos/likelihoodfree-models/results/' + model +'/nets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_runs(prefix):\n",
    "    def list_of_runs(input_list):\n",
    "        runs = []\n",
    "        for k in input_list:\n",
    "            run = k.split('_')[1]\n",
    "            if run not in runs:\n",
    "                runs.append(run)\n",
    "        return runs \n",
    "\n",
    "    runs = os.listdir(netsdir)\n",
    "\n",
    "    runs = [x for x in runs if prefix in x]\n",
    "    loruns = list_of_runs(runs)  # to iterate over\n",
    "\n",
    "    print('# of runs : {}'.format(len(loruns)))\n",
    "    \n",
    "    return loruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:\n",
    "- one entry per run in df\n",
    "- accumulate data info field\n",
    "- function to show loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df(prefix, loruns):\n",
    "    path = netsdir\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for r in tqdm(range(len(loruns))):\n",
    "        name = loruns[r]\n",
    "        wildcard = prefix + '_' + name + '*'\n",
    "        \n",
    "        info_dicts = io.load_wildcard(path, wildcard + '_info')\n",
    "        if len(info_dicts) == 0:\n",
    "            print('{} : empty info_dicts'.format(name))\n",
    "            continue    \n",
    "        \n",
    "        info_first = first(info_dicts)\n",
    "        info_last = last(info_dicts)\n",
    "        \n",
    "        for j in range(len(info_dicts)):\n",
    "            data = [[name]]\n",
    "            labels = ['name']\n",
    "\n",
    "            data.append([info_first['seed']])\n",
    "            labels.append('seed_start')\n",
    "            \n",
    "            data.append([int(info_last['load_trn'] is not None and info_last['load_trn'] != '')])\n",
    "            labels.append('accumulate_data')\n",
    "            \n",
    "            data.append([int('Gaussian' in str(type(info_last['prior_true'])))])\n",
    "            labels.append('gauss_prior')\n",
    "            \n",
    "            info = nth(info_dicts, j)\n",
    "            \n",
    "            data.append([int(info['postfix'][-3:])])\n",
    "            labels.append('round')\n",
    "\n",
    "            for k, v in info.items():\n",
    "                labels.append(k)\n",
    "                data.append([v])\n",
    "\n",
    "            sim_dicts = io.load_wildcard(path, wildcard + '_sim')\n",
    "            if len(sim_dicts) == 0:\n",
    "                print('{} : empty sim_dicts'.format(name))\n",
    "                continue\n",
    "            sim = nth(sim_dicts, j)\n",
    "            labels.append('sim')\n",
    "            data.append([sim])\n",
    "\n",
    "            posterior_dicts = io.load_wildcard(path, wildcard + '_posterior')\n",
    "            if len(posterior_dicts) == 0:\n",
    "                print('{} : empty posterior_dicts'.format(name))\n",
    "                continue\n",
    "            approx_posterior = nth(posterior_dicts, j)\n",
    "            labels.append('posterior')\n",
    "            data.append([approx_posterior])\n",
    "\n",
    "            #dist_dicts = io.load_wildcard(path, wildcard + '_dist')\n",
    "            #loss_dicts = io.load_wildcard(path, wildcard + '_loss')\n",
    "            #net_dicts = io.load_wildcard(path, wildcard + '_net')\n",
    "\n",
    "            if model == 'gauss':\n",
    "\n",
    "                posterior_mu = approx_posterior.xs[0].m[0]\n",
    "                posterior_cov = approx_posterior.xs[0].S[0,0]\n",
    "                true_mu = sim.posterior.m[0]\n",
    "                true_cov = sim.posterior.S[0,0]\n",
    "                std1 = np.sqrt(true_cov)\n",
    "                std2 = np.sqrt(posterior_cov)\n",
    "                mu1 = true_mu\n",
    "                mu2 = posterior_mu\n",
    "                kltp = np.log(std2/std1) + (std1**2 + (mu1-mu2)**2)/(2*std2**2) - 0.5\n",
    "                abs_err_mu = np.abs(posterior_mu - true_mu)\n",
    "                abs_err_cov = np.abs(posterior_cov - true_cov)\n",
    "\n",
    "                data += [[posterior_mu], \n",
    "                         [posterior_cov], \n",
    "                         [true_mu], \n",
    "                         [true_cov], \n",
    "                         [std1], \n",
    "                         [std2], \n",
    "                         [mu1], \n",
    "                         [mu2], \n",
    "                         [kltp], \n",
    "                         [abs_err_mu], \n",
    "                         [abs_err_cov]]\n",
    "                labels += ['posterior_mu', \n",
    "                           'posterior_cov', \n",
    "                           'true_mu', \n",
    "                           'true_cov', \n",
    "                           'std1', \n",
    "                           'std2', \n",
    "                           'mu1', \n",
    "                           'mu2', \n",
    "                           'kltp', \n",
    "                           'abs_err_mu', \n",
    "                           'abs_err_cov']\n",
    "            elif model == 'hh':\n",
    "                labels.append('duration')\n",
    "                data.append([sim.duration])\n",
    "                pass\n",
    "            elif model == 'mog':\n",
    "                try:\n",
    "                    posterior_mua = approx_posterior.xs[0].m[0]  # 1\n",
    "                    posterior_cova = approx_posterior.xs[0].S[0,0]  # 1\n",
    "                    posterior_mub = approx_posterior.xs[1].m[0]  # 1\n",
    "                    posterior_covb = approx_posterior.xs[1].S[0,0]  # 1\n",
    "                except:\n",
    "                    print('nf')\n",
    "                    continue\n",
    "\n",
    "                if posterior_covb > posterior_cova:\n",
    "                    posterior_mu = posterior_mub\n",
    "                    posterior_cov = posterior_covb\n",
    "                    posterior_mu2 = posterior_mua\n",
    "                    posterior_cov2 = posterior_cova\n",
    "                else:\n",
    "                    posterior_mu = posterior_mua\n",
    "                    posterior_cov = posterior_cova\n",
    "                    posterior_mu2 = posterior_mub\n",
    "                    posterior_cov2 = posterior_covb\n",
    "\n",
    "                true_mu = sim.posterior.xs[0].m[0]  # wide\n",
    "                true_cov = sim.posterior.xs[0].S[0,0]  # wide\n",
    "                true_mu2 = sim.posterior.xs[1].m[0]  # wide\n",
    "                true_cov2 = sim.posterior.xs[1].S[0,0]  # wide\n",
    "\n",
    "            data = pd.DataFrame(data).T\n",
    "            data.columns = labels\n",
    "\n",
    "            df = df.append(data)    \n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', 1)\n",
    "\n",
    "    print('# df : {}'.format(len(df)))\n",
    "    print('head of df')\n",
    "    display(df.head())\n",
    "\n",
    "    print('first entry')\n",
    "    display(df.loc[0])\n",
    "\n",
    "    subset_err = df.copy().query('errors != \"\"')\n",
    "    subset_err = subset_err.reset_index()\n",
    "    subset_err = subset_err.drop('index', 1)\n",
    "    print('runs with errors (subset_err) : {}'.format(len(subset_err)))\n",
    "    \n",
    "    return df, subset_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subset(dataframe, subset_str):\n",
    "    subset = dataframe.query(subset_str).copy()\n",
    "    subset = subset.reset_index()\n",
    "    subset = subset.drop('index', 1)\n",
    "    print('length of subset : {}'.format(len(subset)))\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basenames = []\n",
    "filepaths = []\n",
    "\n",
    "for filename in os.listdir(netsdir):\n",
    "    if filename.endswith(\".pkl\") and \"iter_1_info\" in filename and prefix in filename:\n",
    "        basenames.append(os.path.splitext(filename)[0][:-len(\"iter_1_info\")-1])\n",
    "        filepaths.append(os.path.join(netsdir, filename))\n",
    "        \n",
    "print('Found {} matching runs'.format(len(basenames)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def plot(**kwargs):\n",
    "    ax = plt.gca()\n",
    "    data = kwargs.pop(\"data\")\n",
    "    \n",
    "    plt.plot([0.5, 5.5], [data['trueparam'].values[0], data['trueparam'].values[0]], color='g')\n",
    "\n",
    "    plt.errorbar(data[\"iteration\"], data[\"mean\"], data[\"variance\"], marker=\"o\", linestyle=\"\", color='b')\n",
    "    plt.errorbar(data[\"iteration\"]+0.25, data[\"true_mean\"], data[\"true_variance\"], marker=\"o\", linestyle=\"\", color='r')\n",
    "\n",
    "g = sns.FacetGrid(df, row=\"parameter\", col=\"N\", sharey=\"row\", margin_titles=True)  #duration\n",
    "g.map_dataframe(plot) \n",
    "g.set_axis_labels(\"iteration\", \"value\")\n",
    "plt.legend(['x0','epsilonfree', 'ground truth'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "subset = dfc.query('dim == 1 & iw_loss ==  True & svi == False').copy()  #  & prior_true == False & keep == True').copy()\n",
    "subset = subset.reset_index()\n",
    "subset = subset.drop('index', 1)\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "g = sns.factorplot(x=\"prior_alpha\", y=\"posterior_cov2\", hue=\"norm\", data=subset,\n",
    "                   capsize=.1, palette=\"YlGnBu_d\", size=10, aspect=1., markers='.',\n",
    "                   linestyles='', legend=True, row=\"keep\", col=\"ess\", sharey=True)\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 0.02])\n",
    "#ax.set_ylim([0, 0.05])\n",
    "#ax.semilogy()\n",
    "#saverep('loss_calib')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
