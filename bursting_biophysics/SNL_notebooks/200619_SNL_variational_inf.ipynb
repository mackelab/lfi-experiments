{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNL VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyknos/nflows/nn/nde\n"
     ]
    }
   ],
   "source": [
    "from sbi.utils.get_nn_models import get_vi_net\n",
    "from sbi.utils.vi import train_vi, train_mle\n",
    "import torch\n",
    "import sbi.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../bFlows')\n",
    "from bflows.utils.neural_net.get_bounded_flows import get_bflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_net = get_vi_net(parameter_dim=3)\n",
    "\n",
    "class dummy_class():\n",
    "    def log_prob(theta):\n",
    "        mydist = torch.distributions.MultivariateNormal(torch.ones(3), .1*torch.eye(3))\n",
    "        return mydist.log_prob(theta) - 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior1 = dummy_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/standard.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"_shift\", torch.tensor(shift if (shift is not None) else 0.0)\n",
      "/home/michael/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/standard.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"_scale\", torch.tensor(scale if (scale is not None) else 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tensor([ -8.9667, -61.6220, -19.9755, -70.0490, -72.3318, -42.0787, -30.8278,\n",
      "        -35.4676, -49.2727, -22.9792, -57.7836, -19.0398, -60.5484,  -7.8454,\n",
      "        -21.3485, -69.9522, -22.8000, -34.1831, -43.1107,  -6.1226, -30.2684,\n",
      "        -76.8719, -26.3404, -39.5387, -37.9557, -32.0401, -23.8826, -25.1808,\n",
      "        -36.1280, -31.0395, -90.6358, -26.5928, -10.9091, -37.1253, -77.2948,\n",
      "        -26.3838, -45.6929, -33.2024, -71.6843, -22.5552,  -5.8224, -26.8880,\n",
      "        -58.9209, -25.7377, -50.5978,  -7.1971, -58.6178, -32.0478,  -5.7910,\n",
      "        -13.7283, -65.2645, -32.0369,  -4.6620, -30.0400, -43.4396, -54.5101,\n",
      "        -53.8062, -56.7713, -32.0527, -47.5454, -10.0855, -40.7858, -40.4373,\n",
      "        -36.9870, -13.6898, -24.3189,  -7.4545,  -4.8011, -29.0870, -10.1768,\n",
      "        -50.0443, -33.8692,  -5.1669, -36.5144, -23.0304, -22.7412, -30.8839,\n",
      "        -36.7702, -76.4423, -44.2283, -29.0639, -22.8605, -23.4625, -24.9362,\n",
      "        -31.2813, -23.2015, -74.6918, -13.8989,  -6.5840, -38.8568, -66.0815,\n",
      "        -21.4675, -68.4589, -58.1785, -26.2279, -40.0835, -24.8786, -24.0408,\n",
      "         -9.8338, -66.8077], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(31.7739, grad_fn=<MeanBackward0>)\n",
      "this tensor([-73.5118, -75.8930, -12.3654, -57.1059, -37.8062, -36.1164, -23.2051,\n",
      "        -18.9936, -22.6307, -61.7356, -52.6694,  -7.7250, -25.6630, -17.4700,\n",
      "        -15.4034, -82.9310, -14.7957,  -9.7474, -18.7055, -33.9063, -38.6033,\n",
      "         -7.3435, -45.3691, -38.7360, -28.1804, -41.9368, -72.1243, -48.9178,\n",
      "        -56.6694, -36.9547, -22.2844, -46.5553, -34.1426, -54.9386, -39.2755,\n",
      "        -73.0505, -18.0410, -24.6846, -13.8944, -38.8067,  -7.4632, -28.1134,\n",
      "        -15.8395, -80.1582, -42.0261, -39.6820,  -6.6197, -48.3534, -26.5355,\n",
      "        -31.4228, -46.7566, -12.3154, -40.5456, -59.3300, -39.0906, -65.2746,\n",
      "        -32.7618, -26.5622,  -9.5470, -15.4159, -15.6775, -36.4789, -80.9686,\n",
      "        -32.0393, -49.0895, -41.2758, -26.8861, -33.5494, -30.8098, -61.0645,\n",
      "        -20.8808, -17.9899, -48.7788, -50.0898, -18.9082, -18.2666, -40.9843,\n",
      "        -28.9355, -75.2120, -42.4379, -48.0768, -32.9988, -50.2229, -27.7262,\n",
      "        -25.0173, -59.9628, -26.4313, -55.2819, -17.0047, -15.0751, -47.4445,\n",
      "         -8.9339, -15.0031, -30.4716, -12.5751, -47.8212, -50.0496, -10.6310,\n",
      "        -43.2322, -25.1057], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(32.1544, grad_fn=<MeanBackward0>)\n",
      "this tensor([-38.6405, -26.4437, -62.9942, -26.1069, -42.0799, -42.7542, -27.8775,\n",
      "        -43.1593, -31.2109, -68.8526, -21.4996, -34.7910, -29.3277, -35.3030,\n",
      "         -6.0238, -34.7332, -41.3665, -15.0863, -22.1616, -50.5103, -12.9187,\n",
      "        -10.8230, -35.9517, -14.8484, -44.8342, -57.0727, -40.3192, -40.7223,\n",
      "        -12.7590, -31.6055, -41.8098, -12.4867, -48.0608, -23.0544, -21.1910,\n",
      "        -47.7992, -41.1203, -27.8323, -28.4044, -65.5143, -33.4591, -12.1473,\n",
      "        -26.9789,  -8.2319, -73.4816,  -9.2412, -22.1432, -57.2421, -37.6919,\n",
      "        -69.8649,  -4.9816, -47.2171, -41.0418,  -6.3665, -26.0151, -34.9733,\n",
      "        -10.6827, -30.0588, -24.7627, -47.3318, -21.7933, -40.7276, -26.9914,\n",
      "         -9.0878, -10.9883, -19.2213, -27.0908, -19.4135, -54.1772, -17.9898,\n",
      "        -68.1318, -33.6187, -75.2756, -26.4237, -21.0967, -30.4915, -17.6688,\n",
      "        -42.5790, -45.8158, -44.4008, -52.5172, -10.0177, -47.2379, -63.4683,\n",
      "        -21.4963, -13.0521, -38.6144, -15.2483, -10.8853, -24.9764, -29.2178,\n",
      "        -16.6930, -46.7227, -18.8667, -42.0358, -17.0539, -22.6970, -36.0264,\n",
      "         -8.2808, -32.1131], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(28.6791, grad_fn=<MeanBackward0>)\n",
      "this tensor([-30.7397, -29.9015, -53.3149, -24.6785,  -8.3668, -12.5356, -35.9133,\n",
      "        -48.5111, -48.4060, -20.0345, -21.5479, -63.1019, -35.1481, -38.7606,\n",
      "        -24.7808, -58.6583,  -8.3817,  -6.3960,  -4.6997, -18.7639,  -6.8658,\n",
      "        -25.3266, -22.0952, -17.8571, -54.5518, -21.0643, -48.9136, -35.0567,\n",
      "        -15.6129, -52.5419, -24.4934, -29.3444,  -6.2467, -47.4563, -23.4324,\n",
      "        -34.7351, -50.9396, -26.3537, -19.5019, -30.1531, -69.1867, -12.4398,\n",
      "        -11.9331, -58.9778, -20.3917, -30.5953, -59.9132,  -8.9029, -28.7173,\n",
      "        -30.6917, -14.9325, -74.5852, -49.6630, -27.9680, -50.8135, -38.9332,\n",
      "        -52.3904, -55.1912, -62.2118, -22.6573, -23.9250, -27.2795, -33.7879,\n",
      "        -61.8637, -30.9896,  -6.3446, -37.6413, -22.9368, -40.8584, -50.7585,\n",
      "        -41.5358,  -9.4213, -41.5939, -30.7819, -77.4452, -25.8934,  -8.9162,\n",
      "         -9.4084, -52.3822, -25.2320, -48.9254, -26.7168, -33.3604, -50.4491,\n",
      "        -16.6570, -40.4709, -18.0914, -14.0174, -43.8398, -54.9598, -48.6186,\n",
      "        -22.0846, -89.3431, -29.7556, -41.4650, -27.7823, -29.5317, -83.6016,\n",
      "        -95.8437, -53.4299], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(31.5317, grad_fn=<MeanBackward0>)\n",
      "this tensor([-10.8670, -34.2606, -56.6132, -57.4161, -12.6657, -40.7254, -17.9100,\n",
      "        -42.5205, -15.5280,  -8.0756, -72.7932, -41.4984, -41.8652, -31.6729,\n",
      "        -12.2412, -30.4054,  -6.1537, -11.3209,  -5.9122, -45.6506, -34.1804,\n",
      "        -34.1893,  -8.1895, -10.4976, -28.3667, -22.6707, -69.3980, -37.7507,\n",
      "        -24.7446, -35.5003, -61.1397, -41.8042, -33.3080, -50.7989, -33.9429,\n",
      "        -68.1139, -15.9504, -33.9321, -53.1328, -44.8631, -29.8481, -29.2698,\n",
      "        -19.9735, -64.5133, -37.0285, -17.5405, -23.6600, -15.7541,  -6.9321,\n",
      "        -88.5888, -65.8741, -14.7875, -62.3492,  -6.5901,  -8.2725, -10.6985,\n",
      "         -8.9059, -37.4657, -28.7989,  -8.4420, -73.9147, -27.9770, -22.4382,\n",
      "        -32.0406, -34.0205, -20.4808, -31.0397, -23.9585, -38.0972, -19.1122,\n",
      "         -8.0731, -74.9239, -46.4803, -52.4204, -50.4958, -33.7549, -26.7770,\n",
      "        -19.8136, -10.5586, -46.4794, -79.4138, -12.8249, -34.5364, -37.0124,\n",
      "        -34.1727,  -8.6297, -27.1661,  -8.9576, -26.6697,  -8.7681, -53.7529,\n",
      "        -40.9799, -65.9474, -32.4967, -68.3834, -37.9433, -10.3544, -11.0539,\n",
      "        -35.6612, -28.7385], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(29.2713, grad_fn=<MeanBackward0>)\n",
      "this tensor([-42.8801, -51.3157, -33.0112, -18.0637, -22.3857, -35.6460, -32.6056,\n",
      "        -27.5851, -22.0692,  -9.5273, -27.9394,  -8.5384, -22.1032, -46.2775,\n",
      "        -36.3884, -29.2729, -40.3712, -33.0645, -36.6086, -29.3182, -31.1327,\n",
      "        -59.9749, -84.2287, -58.2181, -84.9980, -74.3128, -21.4364, -29.3707,\n",
      "        -11.7021, -46.2163, -49.6659,  -8.7754, -32.7734, -23.8651, -18.8102,\n",
      "        -61.0284, -62.5802, -24.6145, -23.2380,  -9.3789, -10.9000, -52.6318,\n",
      "        -26.7061, -75.3129,  -9.3422, -27.3216, -15.9637, -16.9384, -38.6945,\n",
      "        -52.5404, -29.1226, -76.1367, -29.4503, -47.4837, -46.1304, -40.4172,\n",
      "        -36.3552, -23.9418,  -6.6569, -33.7856, -28.9465, -17.1103, -18.7341,\n",
      "        -35.5625, -47.7631, -31.8275, -32.6471, -26.1485, -60.1163, -26.5010,\n",
      "         -8.6135, -53.7470, -17.9229, -30.8460, -55.1360, -15.3623, -41.3252,\n",
      "         -7.9713, -61.7756, -42.1679, -19.7743, -16.3165,  -7.6243, -10.5901,\n",
      "        -33.1000, -19.2102, -36.9762, -17.1258, -43.7857, -29.8022, -47.6297,\n",
      "        -38.1098,  -8.8167, -30.6333, -47.3663, -41.7700, -42.4497, -61.6503,\n",
      "         -7.3885, -17.2686], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(30.0212, grad_fn=<MeanBackward0>)\n",
      "this tensor([-28.1188, -23.3271, -21.4097, -11.9896, -33.4846, -26.3367, -37.5209,\n",
      "        -18.2052, -40.5959, -26.5304, -23.4467, -45.2457, -62.6288, -54.5738,\n",
      "        -40.8648, -49.2709, -41.9417, -41.1357, -35.7133, -22.5212, -25.9187,\n",
      "        -34.4813, -37.5670,  -7.4095, -56.7221, -32.9472, -74.9789, -43.9018,\n",
      "        -16.8824, -27.9695, -35.1313, -53.7469, -29.3625, -38.8591,  -7.5808,\n",
      "        -40.6664, -62.2260,  -8.3128, -20.6861, -46.2386, -16.9685, -47.1323,\n",
      "        -23.0337, -59.7097,  -6.6941,  -9.7913, -41.3435, -73.3922, -31.6867,\n",
      "        -24.2927, -28.0092,  -6.6229, -18.5219, -35.4253, -18.0670, -51.8839,\n",
      "        -22.5733, -43.9158, -42.5858, -14.3266, -30.6122, -27.5346, -37.9896,\n",
      "        -24.1705, -72.0388, -11.3977, -35.6530, -14.6621, -13.9319, -16.3877,\n",
      "        -74.3449, -10.6632, -15.3551, -22.3493, -59.7288, -21.8295, -31.3824,\n",
      "        -49.7339,  -9.0214,  -5.4251,  -7.7968, -12.6724, -69.9264, -13.6160,\n",
      "        -15.5852, -58.8230, -43.4480, -45.0616, -27.7689, -37.7176, -32.2309,\n",
      "        -36.3940, -21.3324, -33.5879, -10.2457, -48.8747, -22.1605, -45.2498,\n",
      "        -54.1293, -58.9633], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(29.3414, grad_fn=<MeanBackward0>)\n",
      "this tensor([-49.1888,  -6.0056,  -9.7617, -19.0518, -18.4515, -35.8930, -46.3108,\n",
      "        -65.0308, -21.8599, -37.8652, -36.6458,  -6.9616, -45.9302, -13.2831,\n",
      "         -8.7231, -22.8901, -34.5278, -37.6536, -59.1797, -29.6394, -32.0718,\n",
      "        -41.2539, -20.1377, -26.3064, -70.0813, -58.2631, -70.8867, -38.4912,\n",
      "        -38.6939, -42.1460, -32.8543, -14.1181, -26.7882, -32.4423, -17.6251,\n",
      "        -27.7125, -59.0727, -17.5925, -46.3702, -50.5655, -28.7857, -33.2881,\n",
      "        -25.8458, -37.4847, -46.9845, -24.8572, -24.4224, -16.3588, -59.7638,\n",
      "        -50.8141, -52.3623, -42.0862, -58.0516, -62.5690, -34.2476, -18.8016,\n",
      "        -11.2049, -31.5100, -43.8629, -20.0914, -67.6980,  -7.4879, -10.4973,\n",
      "        -14.2040,  -6.7916,  -9.3489, -27.2572, -52.3786, -23.3991,  -7.5296,\n",
      "        -25.9279, -55.7075, -29.8717, -42.7515, -48.8659, -40.4787, -50.2935,\n",
      "        -19.7186,  -5.6549, -20.2878, -14.6110, -20.9405, -29.8268, -35.2681,\n",
      "        -23.2567, -61.6301,  -4.6205, -47.0121,  -5.3798, -33.9820, -27.6172,\n",
      "        -31.9323, -67.1178, -36.9201, -13.3923, -16.4199, -52.2863, -28.6475,\n",
      "        -15.1759, -13.5278], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(28.9257, grad_fn=<MeanBackward0>)\n",
      "this tensor([-20.7303, -65.3351, -49.0084, -43.0020, -43.3196,  -7.9943,  -4.9165,\n",
      "        -30.7896, -36.4962, -34.0183,  -5.5692, -30.3485, -17.9811, -18.3270,\n",
      "        -28.3827, -41.0854, -15.6539, -50.3334, -28.3102, -19.2367, -17.2142,\n",
      "        -17.9179,  -4.9795, -13.3653, -20.3037, -29.5626, -24.5297, -10.2376,\n",
      "        -18.1852, -41.4470, -33.8165, -38.9174, -22.5469, -26.6756, -64.1532,\n",
      "        -28.0302, -36.5766, -11.0112, -11.3073, -17.9064, -66.5862, -37.5750,\n",
      "        -36.9829, -58.9943, -34.3173,  -8.4135, -32.7169, -29.3590, -27.4002,\n",
      "        -19.6371, -49.3878,  -6.5623, -29.4551,  -5.3053, -11.9653, -22.1225,\n",
      "        -22.5992, -13.1415, -15.9326, -63.5036, -57.4477, -29.6955, -13.2291,\n",
      "        -15.7439, -53.7908, -36.0133, -12.0693, -47.1988, -24.4958, -56.1043,\n",
      "        -35.5773,  -8.5098, -32.1654,  -8.3635, -22.4495, -40.2852, -37.3178,\n",
      "        -17.8396, -26.0157, -25.9269, -21.9310, -18.8536, -24.2900, -16.6226,\n",
      "        -34.7736, -70.5154, -40.5039, -27.8428, -28.3346, -38.2363, -57.4062,\n",
      "        -81.2580, -16.8666, -32.5815, -10.4124,  -7.6131,  -9.0612, -20.5715,\n",
      "        -30.4127, -10.0827], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(25.2583, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tensor([-17.1319, -37.9686, -23.7468, -57.5708, -61.5073, -12.9562,  -7.2986,\n",
      "        -26.3333, -30.2237,  -6.4164, -53.7123, -88.6343, -62.5909, -13.8325,\n",
      "        -53.8242, -14.7566, -31.3517, -12.2119, -43.4118, -11.3563, -21.9839,\n",
      "        -16.8128, -55.9349, -28.4598, -25.7696, -18.9703, -55.1670, -38.1660,\n",
      "        -48.1802, -14.8733,  -9.6760, -31.7618, -64.2709, -29.3498, -23.1470,\n",
      "        -16.6770, -43.0907, -28.5903, -40.6942, -50.0671, -23.7046, -27.0160,\n",
      "        -17.7340, -39.9138,  -8.9129, -11.5068, -60.6776, -27.2106, -30.4307,\n",
      "        -57.2834, -35.3040, -53.7105, -31.7383, -21.3298, -31.7467, -78.4499,\n",
      "        -28.8845, -31.1329, -64.5028, -45.3407, -20.2523, -28.5895, -14.4975,\n",
      "        -33.9567, -39.1858, -18.6832,  -7.4754,  -9.5440, -26.7440, -37.7082,\n",
      "        -16.8786, -19.8717, -41.0647, -17.0896,  -5.9962, -25.4573, -33.4379,\n",
      "        -48.2334, -13.2355, -46.7117,  -7.3439, -29.3420, -37.7042, -10.0704,\n",
      "        -34.5983, -16.5726, -39.3103, -10.1818, -32.7885, -12.4005, -35.8245,\n",
      "         -7.8096, -42.7880, -57.2443,  -9.4573, -41.0218, -54.3732,  -8.3904,\n",
      "        -15.0392, -47.3544], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(27.6287, grad_fn=<MeanBackward0>)\n",
      "this tensor([-18.7095, -21.7591, -60.0080, -44.1428, -38.3491, -43.0649, -22.1985,\n",
      "        -30.9880,  -5.8232, -31.9440, -28.3057, -31.2861, -14.2749, -27.0604,\n",
      "        -27.2456,  -7.0836, -40.9900, -11.5678, -41.5134, -10.5058, -10.3881,\n",
      "        -15.4612, -16.9236, -70.7272, -28.0908,  -7.1572, -71.3731,  -9.1405,\n",
      "        -31.7861, -22.1653, -17.4523, -57.0247, -41.5287,  -8.2295, -37.4111,\n",
      "         -5.8481, -25.8414, -42.2214, -16.1620, -13.3929, -69.5037, -25.6671,\n",
      "        -68.8920, -32.0107, -32.1485, -29.8112, -25.5298, -26.6183, -35.7787,\n",
      "        -33.1030,  -8.0438, -24.1442, -14.5646, -75.6370, -11.1211, -82.3506,\n",
      "         -5.4306, -15.1151, -16.4790,  -7.0583, -17.3207,  -5.8931, -22.4195,\n",
      "        -38.4677, -26.8430, -12.8366, -19.5037, -44.8253, -77.3568, -33.9081,\n",
      "        -30.8966, -38.8452, -26.5070, -82.2515,  -8.4684, -25.5594, -33.0467,\n",
      "         -7.4904, -32.0531, -40.8195, -40.1991, -64.9725, -57.4224, -27.2058,\n",
      "        -39.9714, -45.3510, -18.9570, -17.2977, -11.1303, -47.7408, -28.9701,\n",
      "        -29.3079, -49.1166, -21.0790, -26.5907, -41.1017, -34.2565, -24.8574,\n",
      "        -31.6887,  -4.9260], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(26.8053, grad_fn=<MeanBackward0>)\n",
      "this tensor([-23.2416, -19.0891, -37.0305, -32.9931, -62.9877, -28.5426, -51.0124,\n",
      "        -21.5259, -37.2260, -36.8180, -32.5008, -26.7710, -36.0651,  -9.9607,\n",
      "         -6.7928,  -6.8056, -21.6231, -31.6013, -52.3068, -36.6181, -16.0729,\n",
      "         -8.1223, -47.6042, -29.6854, -18.8474,  -7.1326, -42.3688, -41.3942,\n",
      "        -12.5135,  -6.4652,  -6.4773, -42.6580, -24.9931, -14.8484, -15.4837,\n",
      "        -21.3898,  -8.6288, -20.2609, -41.4874, -38.9659, -58.6665, -19.0740,\n",
      "        -45.0404, -11.9700, -37.7407, -25.1862,  -7.6093, -40.1131, -12.2318,\n",
      "        -28.7136, -38.8167, -32.6626, -12.9121, -91.8615, -41.2278, -47.2853,\n",
      "         -8.6940, -11.7751, -28.6392,  -8.3207, -13.8628, -35.7144, -34.3484,\n",
      "        -41.7809, -40.9768, -10.9435, -17.4697, -57.5036, -44.1330, -47.4680,\n",
      "        -26.4525, -33.6190, -18.3682, -23.9672, -10.2754, -51.3300, -66.2599,\n",
      "        -15.6469, -50.6708,  -7.5779, -30.5015,  -7.3694, -26.0122, -37.2043,\n",
      "        -58.0898,  -6.9483, -52.1738, -32.4121, -55.0277, -43.4479, -31.8990,\n",
      "        -55.6751, -35.7646, -40.0264, -41.3590, -28.1540,  -7.9824, -46.8740,\n",
      "         -9.4875, -40.3395], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(26.7111, grad_fn=<MeanBackward0>)\n",
      "this tensor([-41.9928, -38.2352, -22.0111, -28.0122, -26.5304, -54.3825, -24.8748,\n",
      "        -12.2748, -24.5896, -22.0259, -58.3591, -27.6649,  -8.7221,  -8.4848,\n",
      "         -8.9005, -11.1858, -52.9328, -46.8452, -66.7373, -22.9435, -27.0471,\n",
      "        -24.4953, -17.6964, -19.2648, -11.6155, -61.2251, -41.0302, -14.5966,\n",
      "        -66.7276, -38.0438, -31.2109, -76.7185, -41.2612, -35.3036, -21.5987,\n",
      "        -11.3402, -38.3736, -46.2487, -37.9712, -10.2793,  -9.1149, -20.2866,\n",
      "        -27.5175, -62.5171, -47.0740, -27.8056, -29.5117, -31.4117, -10.8106,\n",
      "        -40.8126,  -7.0856, -44.5346, -11.1780, -13.2123, -44.3709,  -4.9604,\n",
      "         -9.6026, -55.6301, -56.6279, -58.7507, -14.5243, -10.0627, -41.0969,\n",
      "         -5.9608, -13.3720, -37.5595, -42.0631, -36.5480, -25.6104, -48.2247,\n",
      "        -17.1925, -22.1344, -40.3970, -24.6965,  -9.0451,  -9.3044, -11.3835,\n",
      "        -11.0206, -10.8638, -49.4922, -30.6502, -29.3213, -11.9873, -16.2305,\n",
      "         -7.2552, -22.8717, -52.5073, -35.5705,  -8.4913, -32.6733, -48.2963,\n",
      "        -47.9631, -29.4567,  -5.4488, -31.8376, -28.8234, -53.5695, -72.0345,\n",
      "        -61.1811, -16.4928], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(26.7286, grad_fn=<MeanBackward0>)\n",
      "this tensor([-14.2028, -46.1448, -30.5098, -38.7724,  -8.6225, -33.8154, -14.6475,\n",
      "        -30.0989, -45.5832, -27.1064, -39.5387, -12.2265, -46.9928, -48.4264,\n",
      "        -27.9841, -35.3713, -52.5865, -53.4382, -19.3276, -41.7305, -25.6552,\n",
      "        -19.6479, -21.2296, -27.9464, -15.8150, -16.4346, -45.4105, -24.9305,\n",
      "        -61.3368, -35.2229, -17.8725, -79.6136, -11.2129,  -7.3861, -12.3828,\n",
      "         -5.9286,  -6.8879, -41.1302, -32.8693, -27.3246,  -6.5754, -49.6019,\n",
      "        -27.6124, -16.8996, -54.7591, -35.5976,  -9.1803, -10.6587, -21.9016,\n",
      "        -40.0278, -41.3528, -37.4776, -35.4143,  -8.7346, -40.5013, -21.0267,\n",
      "         -5.4779, -29.7578, -11.0462, -37.3713, -14.7585, -17.9731, -26.6006,\n",
      "        -15.3193, -23.5778, -29.0746, -24.2156, -25.6108,  -5.3613, -39.4321,\n",
      "         -6.8631, -16.9725, -21.5689, -48.5141, -22.4514, -10.0345, -69.1564,\n",
      "         -5.7185, -38.5957, -36.2660, -36.7920, -20.6821, -23.9924, -28.8924,\n",
      "        -12.4338, -33.2935, -45.7032, -16.8232, -16.2451, -11.4372,  -8.2071,\n",
      "        -53.6799, -61.3303,  -4.5957, -38.4850,  -5.9955, -35.8417, -55.6839,\n",
      "        -27.0987, -49.0293], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(24.7633, grad_fn=<MeanBackward0>)\n",
      "this tensor([-21.8671, -49.6482, -22.6379, -48.3623, -30.6492, -27.5884,  -8.3253,\n",
      "        -17.3607, -11.8405, -13.2876, -45.0207, -15.1715, -37.9836, -15.2660,\n",
      "        -52.9215, -18.1747, -41.7747, -34.8357,  -5.3984, -35.0485, -10.9308,\n",
      "        -40.1819, -22.3421, -22.3153, -56.2491, -28.4111, -27.4379, -20.1820,\n",
      "        -20.2472, -57.5649, -17.3940, -22.4152,  -5.1263, -32.0160, -39.7335,\n",
      "        -38.8818, -41.7218, -64.4064, -32.6983, -21.4793, -37.4197, -18.3899,\n",
      "        -39.3651, -32.7269, -55.9929, -26.0597, -24.3439, -20.2184, -44.3845,\n",
      "        -45.2863, -47.3749, -53.3938, -23.9632, -25.6637, -36.8285, -39.4892,\n",
      "        -28.5428, -32.5650,  -6.5068, -40.5756, -17.3350, -25.8034, -44.2474,\n",
      "        -20.9533, -40.0563, -52.2884, -40.9184,  -6.0731, -23.4969, -21.0310,\n",
      "        -21.7347, -28.1805, -18.6156, -14.2741, -22.3139, -11.5249, -34.6565,\n",
      "        -14.1327, -15.5620,  -8.2243, -91.6177, -19.8411, -56.7911, -27.5028,\n",
      "        -41.8437, -24.4890, -56.7857, -85.9482, -28.9758, -28.1273, -80.2519,\n",
      "        -27.6605, -43.7019, -30.3506, -63.3615, -13.9322, -20.2907, -29.5961,\n",
      "        -36.7441,  -5.3644], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(28.0239, grad_fn=<MeanBackward0>)\n",
      "this tensor([-42.5242, -44.7765, -23.9319, -29.7859, -31.2725, -26.9234, -30.2221,\n",
      "        -29.6489, -35.0076, -55.0975, -34.8668, -58.2241, -25.1688, -37.6910,\n",
      "        -17.6131,  -9.4197, -11.0187, -33.4977, -30.5898, -25.5045, -59.8108,\n",
      "        -16.8658, -15.7070, -69.0455, -12.8261,  -7.0151, -24.2252, -29.9625,\n",
      "        -35.6834, -30.7691,  -9.2676, -60.5931, -19.7721, -59.5738, -26.0966,\n",
      "        -39.7405, -37.9229, -28.8901,  -9.1660, -41.5869, -37.6790, -30.3001,\n",
      "        -10.4166, -38.3371, -49.9490, -53.4637, -15.3222,  -8.3021, -43.7244,\n",
      "        -20.2169, -25.4647, -36.7085, -44.6536, -11.4040, -34.7221, -56.2837,\n",
      "        -43.8861, -73.5148, -12.7360, -12.8175, -49.2998, -28.2179, -60.1834,\n",
      "        -61.5224, -26.9989, -17.1681, -23.1892, -50.1039, -33.7096, -22.2356,\n",
      "        -72.1301, -12.2048, -28.4240, -41.8386,  -9.5566, -37.6996, -68.7351,\n",
      "        -28.4681, -25.9765, -40.9053, -13.2820, -32.1858, -34.1252, -25.7654,\n",
      "        -22.1801, -11.7067, -13.9125, -34.4177, -47.4907, -28.0489,  -9.3241,\n",
      "         -6.1926, -13.1425, -29.8045, -63.7070, -40.7362, -32.0272, -17.8961,\n",
      "        -45.6236, -15.0241], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(28.5096, grad_fn=<MeanBackward0>)\n",
      "this tensor([-47.8900, -38.4186,  -9.8512, -39.2017, -38.9897, -29.7199, -38.7107,\n",
      "        -43.9686, -25.5955,  -4.8900, -18.1469, -46.1334, -18.0248, -17.0126,\n",
      "        -30.8629, -27.7417, -84.3879,  -9.9441, -60.4953, -56.3614, -14.1685,\n",
      "        -51.0166, -10.8107,  -7.3407, -18.0351, -56.8805, -29.6356, -16.4899,\n",
      "        -54.1035,  -5.7872, -41.8916, -75.2605, -11.3825, -17.7324,  -7.0102,\n",
      "        -39.3477,  -7.2527, -27.2268, -19.4384,  -7.0775, -12.4141, -26.4553,\n",
      "         -9.7845, -30.5113,  -6.7616, -11.2267, -17.1362, -23.1704, -39.8615,\n",
      "         -9.2416, -47.4596, -23.3349, -42.3742, -58.4523, -10.0446, -23.3205,\n",
      "        -21.8372, -15.5388, -53.5281, -16.9745, -34.6957, -76.3637, -16.6280,\n",
      "        -25.8145, -42.3797, -38.9009, -31.1402, -20.6170, -38.8040, -21.2278,\n",
      "        -19.7708, -50.0207, -72.8832, -19.6777,  -6.5901, -42.9591, -26.9869,\n",
      "        -49.1039, -29.4469, -84.3833, -24.0650, -14.2192, -10.1595, -21.6210,\n",
      "         -6.5779, -10.8515, -43.6820, -13.0152, -35.3876, -23.2982,  -7.2387,\n",
      "        -19.8593, -32.5038, -16.3308, -30.0063, -41.2550, -27.4490, -37.6622,\n",
      "        -36.5554, -25.3348], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(25.6623, grad_fn=<MeanBackward0>)\n",
      "this tensor([-41.7235, -22.6618, -38.2228,  -7.9541,  -8.5451, -29.5306,  -7.5225,\n",
      "        -70.4111, -46.9394, -40.0985,  -6.6909, -15.8033,  -7.2624, -18.8987,\n",
      "        -24.6065, -23.6067, -14.9056,  -8.3127, -33.4991, -23.5246, -41.8477,\n",
      "         -8.2315, -44.1521, -12.5089, -25.5716, -29.9843,  -6.7276,  -7.6959,\n",
      "         -6.4902,  -5.2103, -30.2028,  -5.0216,  -6.0454, -26.9784, -11.9955,\n",
      "         -9.2844, -28.6370,  -5.7723, -35.9576, -13.9786,  -9.3673, -25.7030,\n",
      "        -12.0281, -18.1464, -54.4394,  -6.2024, -19.0542,  -6.2655,  -7.7338,\n",
      "        -25.9516, -51.3871,  -7.0349, -31.9876,  -7.0630, -13.3674, -33.2711,\n",
      "        -22.2069, -32.9121,  -9.1288, -39.0510, -48.5845, -32.1160, -37.8519,\n",
      "        -26.1367, -24.2180, -14.2322, -58.5041,  -6.5068,  -6.9155, -37.0930,\n",
      "        -37.4701, -11.5997, -40.5532, -15.7708, -27.6964,  -9.6354, -38.2404,\n",
      "         -4.4323, -46.1602, -36.6968,  -7.5856, -13.9389, -21.2965, -19.3685,\n",
      "        -26.9321,  -7.7073, -19.5967, -55.5100, -22.7789,  -5.4494,  -4.9716,\n",
      "        -53.1747, -27.0521, -24.3358, -42.5742, -65.5806, -22.8335, -31.5527,\n",
      "        -19.0543, -43.2515], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(20.3852, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tensor([-38.0125,  -6.1918, -19.5702,  -9.4801, -15.4868,  -6.7093, -31.4096,\n",
      "        -44.6686,  -7.1527, -23.0848,  -6.8631, -23.9869, -13.9613, -58.7773,\n",
      "        -25.9218,  -6.7589, -21.8528, -13.5420, -16.0467, -26.9958, -21.6759,\n",
      "        -17.1950, -21.5464, -21.2380, -48.4278, -48.3625, -25.5397, -22.8935,\n",
      "         -9.1981, -13.0416, -25.1055, -44.3734, -38.5306, -12.7355, -24.1248,\n",
      "        -60.9537,  -7.8045, -43.0704, -29.3783, -43.8390, -27.8442, -38.9083,\n",
      "        -14.8863, -38.6915, -15.8404, -41.7010, -17.2441, -19.3045, -38.3838,\n",
      "        -41.4899, -40.5343, -54.0169,  -6.1006, -51.1811, -30.5717, -39.1391,\n",
      "        -41.0531, -59.7634, -23.5739, -38.0695, -23.7803, -45.6925, -20.4499,\n",
      "        -38.1809, -38.3485, -20.4183, -28.6621, -30.9179, -45.9816, -39.2484,\n",
      "        -40.7631, -11.2057, -31.4482, -21.5850, -14.7414, -28.4611, -32.2663,\n",
      "         -5.7349, -16.1197, -32.3776,  -8.0621, -21.2859, -35.6941, -32.2065,\n",
      "        -29.7036, -36.6599, -18.4728, -29.1654, -10.9246, -22.4048, -47.8636,\n",
      "         -6.9533, -13.6992,  -9.2559, -11.6388,  -5.9770,  -7.4487, -31.4484,\n",
      "        -14.3107,  -6.4494], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(22.8733, grad_fn=<MeanBackward0>)\n",
      "this tensor([-24.5464, -42.3408, -65.2172, -41.4040, -36.6785,  -4.8556, -58.4897,\n",
      "        -40.9624, -38.4200, -21.9375, -32.7417, -16.6448, -32.8307, -13.9681,\n",
      "         -5.0870, -41.9387, -44.8641, -53.2865,  -9.4954, -17.0450, -13.5750,\n",
      "        -42.8649, -21.6122,  -8.3274, -17.2426, -53.3024, -14.2552, -14.0835,\n",
      "        -36.5036,  -8.1868, -26.5292, -29.3859,  -9.6556, -39.0556, -26.2081,\n",
      "        -30.6420, -30.1508, -41.4433, -52.5330, -24.3845, -15.6185, -10.4305,\n",
      "        -76.6334, -74.9533, -32.3431, -27.1743, -19.0742, -38.1594, -17.3782,\n",
      "        -15.8230, -17.0107, -31.6991, -19.5233, -58.5175,  -6.4347, -50.4982,\n",
      "        -33.2059, -36.2583,  -6.8156, -39.3926, -17.1524, -23.0806, -37.1137,\n",
      "        -30.4079, -40.8915, -38.4226,  -9.5588, -10.0515, -62.1854,  -7.9744,\n",
      "        -13.1266, -11.8112, -24.4850, -53.0897, -13.9982, -15.0212, -16.2462,\n",
      "        -49.4834, -55.7508, -32.2546, -11.6630, -41.9577, -19.9155, -29.4516,\n",
      "        -23.6747, -27.4315,  -5.2665, -46.4797,  -9.8249,  -5.9265, -24.2117,\n",
      "        -38.3490, -27.2697, -23.3917,  -5.4567, -43.0833, -10.2629,  -8.1033,\n",
      "        -37.1702, -14.2672], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(24.7229, grad_fn=<MeanBackward0>)\n",
      "this tensor([-26.5022, -37.8681, -16.8254, -17.9938, -32.2170,  -6.0136, -23.2095,\n",
      "         -7.4491, -21.7090, -42.8271, -25.1623,  -9.3573, -57.0754, -33.1810,\n",
      "        -11.3220,  -6.3867, -14.5430,  -6.7175, -27.4288, -32.6590,  -5.8074,\n",
      "        -24.3706, -23.9870, -29.0827, -12.0135, -52.5857,  -8.1671,  -7.3182,\n",
      "        -59.2930, -10.5472, -49.5022,  -8.8649, -42.6881, -35.7669, -36.3842,\n",
      "        -28.5239, -42.3104, -32.2031, -26.1295, -25.8169, -17.7986, -25.6847,\n",
      "        -12.4098, -62.7891, -37.2683, -35.0817, -16.8545, -24.9667,  -5.6785,\n",
      "        -33.8900, -51.7642, -34.5769, -30.2428, -40.2101, -56.1473, -74.1551,\n",
      "        -39.1760, -24.6553, -20.3697, -27.6715, -34.1850, -16.8431, -42.4510,\n",
      "        -34.4349, -13.3586, -42.2916,  -8.3292, -12.8513, -12.9763, -15.7750,\n",
      "         -6.9776, -15.9043, -58.0077, -75.2068, -42.9807, -15.3621, -11.7166,\n",
      "        -45.3679, -13.3716, -47.9630,  -5.4315, -12.1663, -32.5240,  -6.8521,\n",
      "        -31.3049, -14.8806, -35.1659, -15.9709, -23.3763, -34.5718, -52.9138,\n",
      "        -24.8534, -15.1925,  -8.6773,  -8.0522, -19.8787, -23.0162,  -9.5495,\n",
      "        -39.7374, -27.9640], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(23.5827, grad_fn=<MeanBackward0>)\n",
      "this tensor([-27.1364,  -5.1552, -25.6245, -17.2026, -17.3632, -23.2024, -36.6308,\n",
      "        -12.5239, -43.5043, -32.1332, -16.7193, -16.9881, -12.3980,  -6.0036,\n",
      "        -12.8099, -59.3104, -12.4094,  -4.9248, -22.8995, -48.4803, -47.9033,\n",
      "        -30.6080, -22.2688, -40.3543, -23.1092, -44.1103, -57.6731,  -5.3777,\n",
      "        -12.9554, -25.4799, -39.1595,  -9.4868, -19.7920, -27.3240, -24.9018,\n",
      "        -47.7239, -33.6421, -34.3197,  -6.4540, -29.7688, -26.9751,  -6.0744,\n",
      "        -34.4444, -73.0234,  -9.7576,  -8.9944,  -8.8255, -60.2904, -13.9636,\n",
      "        -37.3595, -28.4172,  -8.4303, -58.7915, -12.3052, -44.1454, -24.0971,\n",
      "         -5.4415,  -5.7415, -54.2931, -72.8711,  -6.5872, -39.8045,  -4.5007,\n",
      "        -30.8402, -12.7733, -42.2194, -18.4729, -24.4354, -50.2499, -41.2449,\n",
      "        -34.2444, -24.2701, -43.2524, -14.5942, -10.2884, -25.8758, -10.4213,\n",
      "         -8.3837,  -5.0032, -39.6525, -36.0216, -46.8541, -58.2313, -60.5373,\n",
      "        -19.0209, -47.5905, -16.1280, -27.4929, -31.1993, -35.3844, -22.0436,\n",
      "        -52.3094,  -6.0384, -20.7163, -31.9558, -39.7935, -10.4347, -60.4266,\n",
      "        -25.7421,  -7.9730], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(24.1736, grad_fn=<MeanBackward0>)\n",
      "this tensor([-37.1680,  -8.0933, -23.0561, -10.1975,  -5.1157,  -9.7713, -18.6970,\n",
      "         -8.2471, -39.0098,  -4.6397,  -7.3023,  -6.0705, -41.6521, -31.8243,\n",
      "         -6.4734, -45.8910, -32.8380, -21.2206, -43.2542, -23.6673, -27.5302,\n",
      "         -5.1808, -23.2862, -34.5682, -40.5592, -25.2354, -24.9737, -10.4574,\n",
      "        -26.8171, -10.4024, -40.4511, -33.4807, -38.3295, -51.3445, -24.1689,\n",
      "        -55.5885, -11.6241,  -8.1099, -36.5956, -15.5684, -18.1977, -14.0481,\n",
      "        -19.2331, -36.0930,  -8.6386, -14.4026,  -8.1373, -35.0884, -10.4345,\n",
      "        -35.9125, -60.4488, -10.1474, -37.3319, -69.6143, -28.9501, -56.5255,\n",
      "        -16.5088, -24.6878, -36.5241, -29.7991,  -9.0699, -39.0158,  -5.9275,\n",
      "        -12.4610, -15.6989, -11.2187, -12.0896, -29.1268,  -8.8971, -16.6537,\n",
      "        -20.4184, -16.1771, -32.0641, -40.0219, -30.1957, -18.4385, -44.7217,\n",
      "         -8.1562, -13.6830, -40.8829, -42.7112, -39.1787, -24.5352,  -9.4884,\n",
      "        -18.1141, -18.9248,  -5.3024, -21.5755,  -5.4780, -16.5102, -18.6733,\n",
      "        -20.6969, -34.9636, -32.7966, -36.5845, -23.0370, -31.7929, -26.8513,\n",
      "        -25.2659, -51.1858], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(21.3153, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -7.6734, -24.7956, -19.7914, -37.8213, -43.5029, -11.4005,  -8.4347,\n",
      "        -32.3772,  -8.3361, -22.7070, -23.2216,  -5.2670,  -9.8866, -29.3080,\n",
      "        -22.2560, -15.1548, -19.9560, -17.9655, -26.6113, -40.3825, -32.1114,\n",
      "         -9.4862, -33.8088, -47.2418, -22.7476, -13.9666, -43.9035, -31.9131,\n",
      "         -5.3153, -44.6331,  -4.7193, -21.7357, -38.5240, -11.2837, -21.0823,\n",
      "        -53.2441,  -4.4310, -21.5651,  -4.8362, -36.8991, -11.3984,  -8.2227,\n",
      "        -24.1872,  -7.4625, -42.2358, -26.7497, -10.7624,  -6.2494, -19.8982,\n",
      "        -10.5451, -27.7640,  -6.7625, -30.3682, -14.4206, -45.8018, -15.7387,\n",
      "        -26.3559, -18.5694, -22.6416, -22.9121,  -9.6257,  -9.1661,  -7.6080,\n",
      "         -8.4273, -34.0415,  -7.7959, -12.0426, -17.7312, -36.6480, -21.5650,\n",
      "        -36.0012, -38.8489, -34.4370, -27.6615, -14.4424,  -9.5909, -25.3195,\n",
      "        -21.7865, -28.6296, -12.4115,  -7.9350, -14.0377, -45.4122, -25.4705,\n",
      "        -11.5238, -17.2026,  -9.2335, -51.6103, -59.9257, -13.8852, -72.4266,\n",
      "        -12.6779, -41.1377, -37.9529, -15.6692, -15.2424, -17.6370, -42.1214,\n",
      "        -53.7502, -10.6152], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(19.8824, grad_fn=<MeanBackward0>)\n",
      "this tensor([-25.2145,  -5.9166, -40.2592,  -9.7103, -23.5051, -24.3357, -14.1131,\n",
      "        -21.0273, -22.7283,  -6.3000, -12.8774, -41.1547, -12.3149,  -7.7405,\n",
      "         -4.6713, -12.3386, -73.6387,  -9.4354, -43.7827, -13.0698,  -6.7268,\n",
      "        -28.7069, -18.4573, -45.1159, -44.9444, -40.5985,  -7.5746, -51.2716,\n",
      "        -31.0489, -23.8118, -13.0347, -72.7334, -25.0976,  -6.2502, -20.3716,\n",
      "        -10.2989, -63.4067, -21.6683, -24.5232, -23.4664,  -9.0875, -35.7680,\n",
      "        -20.1943, -15.0618, -43.6966, -14.5020, -55.9140, -36.7588,  -6.7585,\n",
      "        -33.4997, -14.1134, -21.4854,  -8.2199, -21.1345,  -5.1025, -41.2593,\n",
      "        -12.4164, -26.8630, -37.3638, -20.1732, -17.3438, -36.0641, -12.0634,\n",
      "        -37.6007,  -9.3567,  -8.0659, -50.6220, -31.6691, -31.0360, -13.4151,\n",
      "        -36.1281, -37.7628, -10.8401, -49.9867, -42.1912, -40.8330,  -7.2539,\n",
      "        -21.4635, -16.3081, -23.6931, -40.2999, -27.2386, -14.6838, -17.0945,\n",
      "        -29.5266, -18.5703, -22.9946, -20.4064, -23.1714, -24.1084, -54.2761,\n",
      "        -54.8933, -11.8193, -42.4434,  -9.4108, -30.2914, -28.1926, -30.5918,\n",
      "        -29.9992, -26.8043], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(22.4336, grad_fn=<MeanBackward0>)\n",
      "this tensor([-39.0061, -31.8750,  -4.4915,  -9.7147, -14.1969,  -7.1555, -32.6505,\n",
      "        -33.0431, -11.7988, -23.0719, -29.0536,  -7.8420,  -6.1748, -14.3179,\n",
      "        -22.1781, -26.8143, -11.4658, -42.7630, -54.3428, -31.0453, -41.3571,\n",
      "         -7.3360, -74.4276, -40.3209, -15.6773, -26.9480, -31.8331, -21.5614,\n",
      "        -19.6760, -17.7390, -14.4786, -15.7993, -31.5327, -19.5582, -56.1267,\n",
      "        -14.7168, -19.4035, -41.5341, -17.4528, -13.9628, -13.3669,  -6.4991,\n",
      "        -25.0216,  -7.4096,  -4.5135, -25.2985, -61.3381, -32.3619,  -6.5182,\n",
      "        -41.9712,  -9.1627, -29.0928,  -5.8931, -13.4456, -27.1927,  -4.9587,\n",
      "        -19.6531,  -8.9162,  -6.3482, -17.2825, -35.7141, -44.0880, -19.4706,\n",
      "        -35.7403,  -8.7201, -14.7540, -31.3946,  -5.8493, -31.8282, -12.1044,\n",
      "         -8.5264, -65.7187, -53.6035,  -5.9490, -57.9792, -35.7090,  -5.3754,\n",
      "         -9.0036,  -5.7210, -10.5803,  -6.6181, -12.2374, -45.2817, -21.1861,\n",
      "        -11.2132, -42.2863,  -9.2418, -25.5777,  -7.0712,  -6.0562, -14.9556,\n",
      "        -12.8287,  -6.3174,  -9.9587, -25.2691, -24.9962, -43.1443,  -8.1885,\n",
      "        -40.1356,  -6.3525], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(19.1462, grad_fn=<MeanBackward0>)\n",
      "this tensor([-32.0167, -13.3697, -35.8001, -26.9586, -21.5649, -13.9641, -25.3797,\n",
      "        -36.4653, -19.8417, -33.9070, -18.0226, -40.1586, -18.9250, -26.6548,\n",
      "        -33.5172, -55.2842, -12.1475, -11.1864,  -6.6881,  -7.0839, -29.9599,\n",
      "        -18.5458, -11.2846, -28.4574, -33.4802, -14.3759, -37.8775, -41.2962,\n",
      "        -36.6251, -27.9959, -17.0847,  -4.4538, -31.6897,  -9.1538, -22.5301,\n",
      "        -31.5117, -74.8141, -33.5591, -11.0090, -28.1926, -20.5418, -26.9305,\n",
      "        -41.3288, -44.7737, -12.5385, -10.4926, -41.1659, -24.4249, -17.3830,\n",
      "        -54.7786, -12.4631, -15.5314,  -8.5923, -14.7306, -22.4135, -21.2421,\n",
      "        -22.6423, -32.4598, -25.0482, -14.5981, -46.3909,  -9.4602, -31.3040,\n",
      "         -8.9514, -35.0502, -30.5357, -14.2828, -15.4836, -26.7051, -21.3092,\n",
      "        -42.1476, -13.5159, -27.7347,  -6.9720, -44.2893, -41.9002,  -8.8799,\n",
      "        -62.9474, -37.7076, -47.7303, -12.9793, -34.7164, -21.3111, -29.1878,\n",
      "        -15.1498, -13.3775, -25.1385, -11.1909, -13.6266,  -9.3727, -10.3053,\n",
      "         -7.4220,  -6.6858,  -5.4760, -14.2785, -34.7281, -12.3247, -12.9670,\n",
      "         -8.0088, -10.0923], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(20.6024, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tensor([-32.4528, -23.8501, -12.5190, -26.6107, -53.1373, -10.7137, -11.4962,\n",
      "         -7.9809, -44.4982, -13.0727, -21.3071,  -9.8006,  -7.0651, -29.5411,\n",
      "        -26.3822, -33.8382, -36.6686,  -6.3593, -17.2822,  -7.8190, -19.3688,\n",
      "         -9.5958,  -9.9847, -37.1856,  -6.2287, -46.5035, -19.6327,  -7.0736,\n",
      "         -7.5324, -26.6746, -41.9447,  -9.1322, -20.8351, -56.9876, -17.3170,\n",
      "        -28.5310, -57.4586, -44.5851, -29.5532, -16.8479, -38.0968, -20.1718,\n",
      "        -26.8299, -49.3237, -31.5955, -19.7936,  -8.3264, -24.4216, -26.5672,\n",
      "        -42.3350, -10.4321, -14.2238, -10.4359, -35.0613, -11.1031,  -8.3136,\n",
      "        -16.0030, -19.6125,  -6.1439, -10.6889,  -9.6742, -15.8148, -37.8270,\n",
      "        -17.4121, -41.1046,  -7.7667, -23.1267, -12.4791,  -8.8831, -11.1405,\n",
      "        -22.8425, -26.6569,  -4.8010, -44.0823, -31.0656,  -5.9808, -13.2978,\n",
      "         -7.1300, -13.0444, -42.1681, -30.0606, -25.9853,  -8.2143, -45.5236,\n",
      "         -6.8996,  -7.8327,  -7.5224,  -7.2925, -16.6086, -28.6715,  -8.1488,\n",
      "        -14.9051, -22.2111, -58.8410, -40.7914, -27.8318, -50.0463, -37.1406,\n",
      "         -9.9114, -29.6479], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(19.2743, grad_fn=<MeanBackward0>)\n",
      "this tensor([-10.0398, -64.4461,  -9.9177,  -4.9460,  -5.6284, -58.2021, -34.9420,\n",
      "        -11.9142, -21.2238, -16.6983, -10.7302,  -7.5128, -21.0705, -32.3167,\n",
      "         -7.1043, -30.7003,  -9.6869, -32.0027, -13.3643, -10.2053, -66.4084,\n",
      "        -18.9542, -12.5353, -48.0188, -11.7288, -38.2800,  -7.5467,  -7.1516,\n",
      "        -18.0463, -14.7182, -16.2827,  -9.8811, -40.3543, -11.4430, -86.9881,\n",
      "        -27.3438,  -8.9102, -31.4985,  -6.3647, -12.5120,  -7.3711,  -5.1991,\n",
      "        -40.7803, -11.3214, -16.8695, -22.5774,  -7.7247, -18.2626, -30.4778,\n",
      "        -11.3419, -28.7854,  -7.2514, -21.3478,  -5.1609, -11.2640, -12.7563,\n",
      "         -8.7173, -11.8144, -10.7620, -53.4943, -10.9920,  -6.4317,  -6.0437,\n",
      "         -7.2504,  -8.3279, -29.0032, -33.9915,  -7.6079, -22.9841, -17.0296,\n",
      "        -10.2336, -15.3262, -16.8790, -29.4379, -16.5388,  -8.8834, -48.3444,\n",
      "        -29.8850, -16.8693, -13.2145,  -8.9347,  -8.0455, -38.4350, -22.9974,\n",
      "         -5.4253, -24.6827, -47.4857, -32.0773,  -9.5142,  -7.4944,  -5.3776,\n",
      "        -25.0363, -20.3212, -74.2427,  -4.5482, -18.2553, -27.6203,  -7.2907,\n",
      "        -53.6283, -27.7711], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(17.6599, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -5.9395, -12.9570,  -9.5714,  -9.9715, -17.7150, -13.3710, -11.5982,\n",
      "        -33.4205, -36.9579, -25.2651,  -9.1846, -13.4658,  -9.2947, -25.9372,\n",
      "        -39.8585, -42.0062,  -8.6853, -34.3538, -23.3495, -16.3024, -32.9935,\n",
      "        -41.7722,  -8.0096, -30.3431,  -6.6047, -32.4499, -32.9540, -29.2787,\n",
      "        -10.9783, -63.8100,  -6.6577, -10.5051, -43.0587, -29.4145, -40.4278,\n",
      "        -64.1387,  -6.7219, -23.5684, -47.9804,  -9.6212, -34.0804, -18.9194,\n",
      "         -4.6344, -14.3447, -30.8742, -38.1350, -40.9790, -36.3954,  -5.9671,\n",
      "        -35.7862,  -4.3148, -14.7653, -27.7589, -41.0087,  -6.1482, -21.8948,\n",
      "        -42.5786, -29.0599, -38.7144,  -4.4942,  -7.6358, -23.4856, -31.4391,\n",
      "        -27.6092, -30.2459, -42.0343,  -5.8440, -40.8803,  -8.5517, -10.5196,\n",
      "         -7.5612,  -9.7158, -10.2371, -13.0464, -52.7225, -12.9028, -19.7261,\n",
      "        -48.0998, -42.1801, -25.3104, -27.4739, -24.0170, -30.1980,  -4.5702,\n",
      "        -73.2714, -21.0314, -10.2094, -13.2221, -30.5496, -23.2881, -40.0339,\n",
      "        -12.4585, -28.7217, -17.7699,  -7.4077, -54.0219, -70.9427,  -9.9519,\n",
      "         -9.8940,  -6.4752], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(21.2093, grad_fn=<MeanBackward0>)\n",
      "this tensor([-17.4875,  -7.4667, -24.9982,  -7.4129, -20.3346,  -8.4318, -38.8244,\n",
      "        -16.1893,  -5.1401,  -6.8598,  -7.9363, -14.0942,  -4.4360, -38.4111,\n",
      "        -27.2776, -31.5817, -35.8785,  -9.0523,  -6.2599, -15.0735, -11.6437,\n",
      "        -54.4259, -22.9537, -24.5260, -52.7411,  -6.2091, -52.7563,  -8.8713,\n",
      "         -7.5645,  -9.7173,  -6.6237, -29.2367,  -6.3513,  -7.2516, -41.7939,\n",
      "        -22.3554,  -6.6854,  -7.6809, -15.4946, -10.7397,  -8.2734, -32.2316,\n",
      "        -32.3222, -21.8039, -39.6519, -11.9792,  -8.7737,  -6.2297,  -6.8615,\n",
      "        -19.8005, -12.4052, -52.5987, -22.7530,  -7.8336, -20.0011,  -8.4466,\n",
      "         -7.5944,  -8.5429, -10.6355, -35.7872, -51.3395, -14.1533,  -7.5459,\n",
      "        -28.6863, -19.2886,  -4.9398, -15.9029, -36.3103, -23.8793, -35.1049,\n",
      "         -6.0896,  -8.8077, -32.3781, -15.4026,  -8.7008, -26.3652, -24.8334,\n",
      "         -7.9537, -10.7743, -14.8563, -10.3192, -34.0667, -37.7771,  -4.9957,\n",
      "        -12.4141,  -7.7838, -32.6958, -42.9554, -13.7672,  -7.8174, -24.6016,\n",
      "        -19.0134, -36.2016, -38.5600, -30.3770,  -8.2366, -27.9646,  -9.2056,\n",
      "         -6.2186, -11.7409], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(16.3420, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -8.4361,  -9.0631,  -6.5405, -11.5292, -18.8523, -33.4993, -33.1687,\n",
      "        -15.8244, -57.5503,  -8.2228, -14.4964, -24.3592, -31.1246, -14.5484,\n",
      "         -6.7166,  -8.1452, -23.1889,  -9.9866,  -5.4762, -37.5289,  -9.3122,\n",
      "         -6.6669, -41.1313,  -7.5570, -25.2697, -14.9841,  -8.0317, -39.9044,\n",
      "        -33.2783,  -9.1132, -14.7746, -10.8523, -28.9035, -28.9249,  -9.0598,\n",
      "        -45.0493, -36.7014, -14.4930, -14.4820, -23.1503, -50.9537, -32.7440,\n",
      "        -10.2927, -49.3143, -29.9353, -18.5685, -17.8401, -30.6320,  -9.7147,\n",
      "        -19.4208,  -6.4984, -13.9726, -49.5323, -36.1935, -39.6824, -63.5246,\n",
      "        -24.7988,  -8.0658, -30.9267, -28.0763, -35.6264, -19.9954,  -6.8338,\n",
      "         -7.6774, -37.3319,  -8.3882,  -8.1778,  -5.6846,  -9.2374, -41.5147,\n",
      "        -42.3792, -12.9197, -25.3810, -17.5290, -44.7306, -62.9311,  -6.9383,\n",
      "        -24.6769,  -7.3508, -27.6049, -14.4957, -17.8999, -16.6439, -20.0948,\n",
      "         -8.4523, -27.7956, -31.5298, -11.2489,  -5.0709, -40.2809, -35.9299,\n",
      "         -6.1161,  -6.4908, -17.3801,  -5.6508, -24.3947, -23.5018,  -4.4878,\n",
      "        -12.1368, -28.7692], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(18.5803, grad_fn=<MeanBackward0>)\n",
      "this tensor([-56.0614, -30.3695, -32.3496, -24.4421, -10.1995, -14.0907,  -5.3442,\n",
      "        -15.8674, -11.2383, -27.6664, -32.2647,  -7.8618, -39.7841, -44.4467,\n",
      "         -6.0497, -21.9794, -14.0294, -14.6489,  -7.4749, -13.2112, -14.2166,\n",
      "        -64.2973, -12.2007, -40.4439,  -6.8329,  -6.9165,  -8.2460,  -8.3047,\n",
      "         -9.5939, -27.9571, -27.7133, -21.2164,  -4.9304, -19.9591, -11.5484,\n",
      "         -8.9249, -23.8955,  -5.5031, -44.8818, -31.9765,  -8.3672, -16.2287,\n",
      "        -53.0382, -14.4313,  -7.9464, -24.3294, -12.4385,  -6.9540,  -7.4955,\n",
      "         -9.3864, -17.9514, -43.9898, -17.5987, -16.1387,  -9.8944, -21.3861,\n",
      "        -28.6528, -11.3037, -24.6720,  -7.8049,  -9.2849, -41.8863, -10.4916,\n",
      "        -11.0051,  -4.8815, -30.4443,  -7.1706, -63.8825, -12.6035, -10.4350,\n",
      "        -16.4131, -45.1058, -29.0557, -17.2760, -52.2609, -23.9576,  -6.4762,\n",
      "         -7.5043,  -9.6053, -12.4894, -32.5985, -32.6554, -23.9148,  -8.2260,\n",
      "         -7.1069,  -7.4019,  -7.6405,  -6.8204, -25.1015,  -6.9619, -11.2846,\n",
      "        -38.3183, -32.1646, -22.5324, -40.8522, -22.3321,  -4.5917,  -9.5224,\n",
      "        -16.6304, -16.4334], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(16.8829, grad_fn=<MeanBackward0>)\n",
      "this tensor([-32.6334,  -6.8103,  -6.3032,  -7.6079,  -8.0053,  -7.8254, -35.2752,\n",
      "         -5.7017,  -8.5648,  -5.0625,  -7.3099,  -8.4644,  -5.2930, -57.3271,\n",
      "        -21.5893,  -8.9534,  -9.0902, -32.5336,  -4.7093,  -9.4923, -16.6232,\n",
      "         -5.3163,  -8.2584, -30.7707, -43.7889, -36.6071, -34.7435, -13.4660,\n",
      "        -29.5376,  -9.4200,  -6.2526, -17.4265, -11.0368, -14.8419,  -7.6628,\n",
      "         -6.0993, -48.5944,  -8.7384, -10.3437, -24.6386, -43.2269, -33.5657,\n",
      "        -19.9610,  -6.2292, -38.7222,  -8.8124, -34.8520,  -9.7649, -29.0599,\n",
      "         -5.7974, -39.1808,  -8.4314,  -5.4326, -23.0196,  -5.2725,  -6.9300,\n",
      "        -15.0234, -13.5068, -10.4433, -35.8624, -41.8600, -10.4212,  -6.5270,\n",
      "         -6.1794, -41.9090, -23.0009,  -9.9392, -48.1190,  -4.9918, -57.8462,\n",
      "        -21.1828, -13.6030,  -7.3962, -25.3751, -33.4213, -17.0307, -17.5862,\n",
      "        -20.7476, -17.6275,  -9.3155, -24.9196,  -5.0563, -26.3709, -32.0116,\n",
      "         -7.8165,  -6.6079, -11.0615, -39.9188, -10.5995, -28.6383, -10.6475,\n",
      "        -15.4998, -25.2761, -20.6286, -26.7339, -21.3649, -20.7914, -11.1401,\n",
      "        -27.8496, -31.5895], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(16.1493, grad_fn=<MeanBackward0>)\n",
      "this tensor([-50.5329, -42.2564, -27.0998, -37.4393, -37.5313, -38.9223, -16.5192,\n",
      "         -8.6402, -33.5673, -19.5123, -37.5871,  -7.2672, -40.6862, -14.8998,\n",
      "         -8.2460, -46.3532, -61.8302, -35.5910, -42.2638, -19.1333, -47.0306,\n",
      "         -7.1008,  -6.7131, -30.0241, -28.7676,  -8.0431, -42.7166, -14.1048,\n",
      "         -6.6720, -27.8400,  -9.4968, -37.0006, -24.1510,  -4.6234,  -6.7926,\n",
      "        -29.7407, -22.2584, -27.0998, -25.2095, -17.8677, -34.7566,  -7.4965,\n",
      "         -5.1854,  -8.9048, -43.8150, -20.1528, -30.2055,  -7.7429, -25.7087,\n",
      "        -28.1351,  -5.1521, -41.1344, -10.4372, -23.1141,  -7.2795, -21.7135,\n",
      "         -5.6555,  -6.5787, -23.3734, -46.9586, -45.0030,  -7.4280,  -5.3824,\n",
      "        -18.4838, -11.2593, -32.8195,  -8.0803, -21.0757, -30.7707, -23.1822,\n",
      "         -6.5000,  -7.6953, -27.3869,  -9.8355,  -8.7194, -11.1332, -12.2374,\n",
      "        -23.8981, -11.1575, -27.8423,  -8.2305, -14.3240,  -8.1282, -27.2772,\n",
      "        -30.9098, -21.4094, -52.7320, -28.5875, -41.2921,  -7.2556,  -6.4668,\n",
      "        -45.8934, -13.6580,  -7.1412,  -7.6345, -12.1576, -14.6267,  -4.5570,\n",
      "        -33.5544, -59.6340], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(19.4340, grad_fn=<MeanBackward0>)\n",
      "this tensor([-10.0057, -27.5996,  -9.3857, -29.4993,  -8.8052, -28.5259,  -4.7719,\n",
      "         -6.7719, -11.5512, -42.7086,  -8.3637, -40.8144, -18.5201, -21.9889,\n",
      "        -38.4309,  -4.8504, -15.1405,  -7.5576, -45.0748, -19.6933, -33.1564,\n",
      "         -6.7691,  -9.4275,  -7.3256, -35.0107, -11.9032,  -6.6252,  -6.2590,\n",
      "        -13.4038,  -7.6080,  -5.0451, -49.1099, -20.0869,  -8.6316, -22.2134,\n",
      "        -30.4348, -12.4727,  -7.2798, -12.0266,  -5.6793, -18.5878, -37.0341,\n",
      "        -13.2747,  -6.2791, -41.6713, -15.2601, -12.3006, -21.9254,  -6.8067,\n",
      "         -4.7030, -18.8144,  -9.3685, -12.3456,  -9.7659,  -5.9628, -12.0401,\n",
      "        -31.1835, -17.9407, -29.3281, -20.6124, -36.5010,  -5.6705,  -6.7994,\n",
      "        -75.3929,  -6.2873, -24.0473,  -5.7372, -39.1910, -43.7204,  -4.4931,\n",
      "         -8.2793, -56.9035,  -7.9698, -39.2065, -21.2069,  -6.5777, -25.6533,\n",
      "        -26.9130,  -4.9334, -38.3536,  -4.9902, -15.4513,  -6.1687, -35.8573,\n",
      "         -4.9390,  -9.5509, -32.6914, -25.2195, -10.5800,  -4.9628, -23.8897,\n",
      "        -13.0882,  -4.8847, -40.6457, -17.6370,  -7.0372,  -7.9722,  -7.9854,\n",
      "        -23.6654,  -5.1586], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(15.6758, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tensor([-19.5844, -11.8444,  -7.7322, -29.9333,  -4.8443,  -8.1600,  -6.5958,\n",
      "        -29.3090,  -7.3186, -38.0440, -15.3497,  -6.6765, -17.1446, -40.6907,\n",
      "        -25.6002, -14.1497,  -4.3955, -11.1415, -13.7677, -23.3625, -15.8096,\n",
      "         -5.3385,  -6.0571, -39.9015, -48.5278, -12.5573,  -8.2184,  -5.6117,\n",
      "        -27.7508, -42.1132, -29.5606, -12.0998, -20.4448,  -8.1845,  -7.8330,\n",
      "        -36.0078,  -5.7370,  -9.2617,  -9.4005, -20.8178,  -9.5922, -14.3268,\n",
      "         -6.0193,  -6.7161,  -4.4503, -28.3806, -19.6931, -40.9612, -25.8639,\n",
      "         -8.9872,  -6.7783,  -5.9207, -14.1834, -21.6539, -15.1799, -49.1045,\n",
      "         -7.8484, -13.9249, -17.6008, -33.0958, -28.7648, -42.3280, -12.8921,\n",
      "        -13.6427, -31.6808, -25.0685, -40.7718, -36.0626,  -6.7071, -18.8007,\n",
      "        -43.6822, -33.5185,  -5.5774,  -8.0435, -22.2475,  -6.9569, -27.2928,\n",
      "        -39.2087, -16.6840, -10.7571, -39.9182, -46.5737, -36.1261, -15.5297,\n",
      "         -7.6508, -14.2994,  -6.1691,  -8.1946, -45.0297,  -7.6440,  -8.6940,\n",
      "         -8.6385, -11.8819, -15.7945,  -7.5613, -35.7864,  -8.0950, -23.9425,\n",
      "        -11.8472,  -7.3807], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(16.1567, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -6.7386,  -8.9610, -34.1489, -10.3988,  -7.3202, -23.3101, -38.0567,\n",
      "        -14.9026,  -6.0449,  -6.7415, -23.8899, -25.9261,  -8.8112, -21.3286,\n",
      "         -5.2424, -22.0010,  -9.0583, -39.3381, -21.5921,  -6.5211, -43.5443,\n",
      "         -6.8468, -14.7717, -10.8217,  -5.0271, -22.5046, -33.9319, -10.6889,\n",
      "         -5.7540, -19.0673, -23.7425, -41.1641,  -5.5312,  -9.8572, -21.9729,\n",
      "         -5.2090, -13.7935, -13.6510, -15.2483,  -7.3739, -15.5084, -13.4315,\n",
      "         -8.3557, -27.2684,  -5.1355,  -6.6167,  -5.3785,  -4.7401,  -6.0420,\n",
      "         -7.0680, -13.0397, -23.5375, -10.3767,  -9.2299,  -6.9239,  -9.2881,\n",
      "        -16.5083,  -6.4965, -10.9227,  -9.1860,  -8.7448, -31.7028, -10.2101,\n",
      "         -8.8854,  -7.1400,  -5.1628, -23.3246,  -6.9282,  -6.6488,  -7.4644,\n",
      "        -19.7943, -41.1217, -44.5873, -11.8226, -37.4851, -14.7719, -31.2748,\n",
      "        -11.3502, -60.8595,  -7.0021,  -7.9378,  -6.3103, -35.4207, -26.3045,\n",
      "        -21.7211, -33.7656, -38.2020, -30.4330, -11.9876,  -4.6325,  -6.2636,\n",
      "        -11.7320, -31.8284, -10.5831, -38.0698,  -8.9098, -11.0358,  -4.7479,\n",
      "        -34.8995,  -9.0687], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(14.1610, grad_fn=<MeanBackward0>)\n",
      "this tensor([-11.9983,  -6.0061, -12.1256, -34.3079, -33.2846, -42.0597,  -9.9087,\n",
      "        -13.6714,  -5.2239, -16.8550,  -6.7799, -14.0767,  -5.0619, -10.8029,\n",
      "        -16.7044, -10.0717, -10.4984, -19.5124, -28.7355,  -8.6959,  -6.4611,\n",
      "         -5.9021, -16.8802, -50.7187, -78.5115, -10.5982, -27.4213,  -6.5630,\n",
      "        -11.9060, -30.1855, -43.5560,  -6.6447,  -6.5592, -15.9776,  -4.7334,\n",
      "         -4.6703, -41.4674,  -7.3373, -35.7298,  -8.3311,  -8.1769, -17.3371,\n",
      "        -22.3594, -35.3787,  -4.8956, -25.3947,  -9.4605,  -5.7261, -14.6288,\n",
      "         -6.1789, -21.0068,  -9.9926, -16.7810,  -5.3263, -37.5285, -33.2694,\n",
      "        -10.3241, -19.2777,  -6.6007,  -6.3510,  -5.0690, -13.5135,  -9.6951,\n",
      "         -9.5174,  -9.3245,  -7.1730,  -6.9036,  -5.7907,  -9.1596,  -7.8976,\n",
      "        -20.2796, -22.1403, -31.8938,  -5.7158, -21.1188, -23.1671, -11.1809,\n",
      "         -6.4525, -33.9497, -17.7898,  -8.7060, -12.7493, -13.1080,  -5.1287,\n",
      "        -31.7266, -26.9451,  -6.3523,  -6.9422,  -7.4482,  -4.8501,  -5.3285,\n",
      "        -35.9659,  -6.3799, -19.3231, -12.5925, -47.3516,  -6.4060, -38.2230,\n",
      "         -7.3125, -38.3388], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(14.3176, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -9.1130, -44.1671, -15.2637, -10.7985, -47.7002,  -6.0992,  -4.3970,\n",
      "         -8.2488, -15.4700,  -7.8226,  -9.0468, -34.3834,  -8.0524,  -5.4970,\n",
      "        -29.6098, -41.6205, -18.7503, -20.9163, -16.4302, -27.2954, -22.8277,\n",
      "        -20.5494,  -5.9486, -10.0067,  -6.7204, -23.8873,  -6.0079,  -6.2621,\n",
      "        -10.7064, -35.5411,  -6.5020, -26.1314, -13.2252,  -6.8059, -13.5728,\n",
      "         -6.9344, -35.8522, -33.2733, -23.2313, -13.7428,  -4.9546,  -5.1410,\n",
      "         -6.7679, -12.6886, -13.4418,  -7.8149, -35.0699, -39.2887,  -5.3612,\n",
      "        -59.4392,  -9.6021, -10.6795, -24.7955, -23.6547, -17.1131, -23.6095,\n",
      "        -40.6622,  -6.0934, -40.8621, -21.0880,  -8.8066, -75.0895, -11.2697,\n",
      "         -8.1019, -16.7777, -28.2182, -24.3167, -24.3705, -10.4194, -77.4266,\n",
      "        -13.1999,  -7.2046,  -6.9899, -43.5479, -31.6937,  -4.7886, -23.3591,\n",
      "         -4.7368, -14.0129, -12.7674, -30.7517, -11.7290, -36.6814, -23.2360,\n",
      "        -10.7033,  -8.9849, -14.1693, -14.4252, -38.6758, -33.3361,  -8.4696,\n",
      "        -17.0464, -21.2233,  -4.7241, -11.3869, -27.5787,  -8.8561, -46.2781,\n",
      "         -7.8940, -14.1132], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(16.9229, grad_fn=<MeanBackward0>)\n",
      "this tensor([ -6.2970,  -6.5004, -12.7873,  -9.9170,  -9.2694, -12.2686,  -6.0375,\n",
      "         -7.1778, -26.1770, -18.6800, -17.9505,  -5.4010, -18.7864,  -9.6393,\n",
      "         -8.3477, -31.2946, -24.7523, -14.9726,  -8.5114,  -5.7144,  -9.6243,\n",
      "         -5.7606,  -9.5133, -29.1427, -22.1652,  -5.7208,  -7.3100,  -5.6115,\n",
      "         -4.6221,  -6.1666, -21.2256,  -5.6883, -15.4132, -16.8479, -23.3084,\n",
      "         -7.2917, -42.7094, -21.5270,  -9.3143, -14.2349, -13.2622,  -6.4467,\n",
      "        -15.1232, -26.1660,  -4.8283, -11.0542, -13.1985, -28.6944, -26.4680,\n",
      "         -4.9377, -12.2103, -22.8165, -12.3797,  -6.9759,  -6.0804, -32.9927,\n",
      "        -32.9478, -13.2693, -26.5003,  -6.4457, -11.9766,  -5.2542,  -4.6647,\n",
      "        -34.6816,  -9.7528,  -8.9512, -50.8655,  -9.2955, -35.6462, -10.3954,\n",
      "        -40.2004,  -6.1365, -38.7846,  -7.4139,  -7.8974, -44.9383,  -6.8674,\n",
      "        -29.7945, -34.4000, -30.6460, -10.1887, -31.3536, -29.3114, -15.2635,\n",
      "        -19.9951, -14.8100, -12.3504, -39.1510, -10.3082, -14.1932,  -5.2362,\n",
      "        -21.5719,  -5.9392,  -5.0481, -11.7954,  -6.0134,  -5.2232, -20.6952,\n",
      "        -12.5475, -11.8231], grad_fn=<SubBackward0>)\n",
      "loss:    tensor(13.6723, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0a45808fa545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvi_net_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nsf_uncond_bounded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_vi_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi_net_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/sbi/sbi/utils/vi.py\u001b[0m in \u001b[0;36mtrain_vi\u001b[0;34m(flow, dist, batch_size, learning_rate, stop_after_epochs, clip_grad_norm)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/distributions/base.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, context, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/flows/base.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, num_samples, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0membedded_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_reps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedded_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/coupling.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtransform_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentity_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         transform_split, logabsdet_split = self._coupling_transform_inverse(\n\u001b[1;32m    121\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/nn/nets/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/nn/nets/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vi_net_ = get_bflow('nsf_uncond_bounded', prior=prior, context=torch.ones(1,1))\n",
    "trained_vi_net = train_vi(vi_net_, posterior1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_samples = trained_vi_net.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = utils.pairplot(output_samples.detach().numpy(), limits=[[-2,2],[-2,2],[-2,2]],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on SNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import zeros, ones, eye\n",
    "from sbi.inference import SNL, SNPE, prepare_for_sbi\n",
    "from sbi.simulators.linear_gaussian import linear_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70536215f3af4f77ab52f719c8143e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural network successfully converged after 75 epochs.\n"
     ]
    }
   ],
   "source": [
    "num_dim = 3\n",
    "\n",
    "x_o = zeros((1, num_dim))\n",
    "num_samples = 500\n",
    "\n",
    "# likelihood_mean will be likelihood_shift+theta\n",
    "likelihood_shift = -1.0 * ones(num_dim)\n",
    "likelihood_cov = 0.1 * eye(num_dim)\n",
    "\n",
    "prior = utils.BoxUniform(-1.73 * ones(num_dim), 1.73 * ones(num_dim))\n",
    "\n",
    "simulator = lambda theta: linear_gaussian(theta, likelihood_shift, likelihood_cov)\n",
    "\n",
    "infer = SNL(\n",
    "    *prepare_for_sbi(simulator, prior),\n",
    "    mcmc_method=\"slice\",\n",
    "    show_progress_bars=True,\n",
    ")\n",
    "\n",
    "posterior = infer(num_rounds=1, num_simulations_per_round=1000).set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/standard.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"_shift\", torch.tensor(shift if (shift is not None) else 0.0)\n",
      "/home/michael/anaconda3/envs/sbi/lib/python3.8/site-packages/nflows/transforms/standard.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"_scale\", torch.tensor(scale if (scale is not None) else 1.0)\n",
      "/home/michael/Documents/sbi/sbi/inference/posterior.py:247: UserWarning: The log probability from SNL is only correct up to a normalizing constant.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:    tensor(30.8455, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(31.2933, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(30.2845, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(29.7605, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(22.2708, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(27.1570, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(25.5103, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(24.3957, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(27.2476, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(27.3562, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(24.0949, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(26.8114, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(24.0794, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(24.2537, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(27.5496, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(22.2296, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(23.3384, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(25.2096, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(22.8992, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(20.2157, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(24.6201, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(18.5348, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(21.2589, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(18.3773, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(21.2444, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(18.9649, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(17.4577, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(17.9672, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(18.4534, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(14.9269, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(18.5790, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(15.1727, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(15.9288, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(15.1218, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(12.8404, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(13.4592, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(13.1909, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(12.0648, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(11.9006, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(11.2459, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(11.5466, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(12.3602, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(9.9158, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(8.8485, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(10.4903, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(7.9273, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(8.7923, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(7.5750, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(8.3129, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(7.5751, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.5144, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.2329, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.6723, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.9457, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.7837, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.9035, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.8436, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.5012, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.9829, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.5820, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.6207, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.3394, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.3726, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(6.3573, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.7099, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.5452, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.1859, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.3041, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.0860, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.5379, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.5300, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.6944, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.7929, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.4459, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.6779, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(5.1521, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9350, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0285, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9575, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1246, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4355, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3460, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1572, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0417, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2105, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4461, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3481, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0839, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4266, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2725, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1939, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4171, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2321, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2948, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9347, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4329, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2366, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9956, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9227, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.6981, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2403, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2669, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2430, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8701, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3383, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8210, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8414, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9539, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9210, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0217, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8344, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.7304, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9026, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2994, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8331, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1326, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9373, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0808, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.5868, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.4720, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.9830, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9139, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.8905, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9662, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3987, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.7233, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1129, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9867, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1329, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3756, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1409, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2726, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.6714, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2104, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1792, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2205, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1155, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.3989, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9543, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0730, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.7952, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.7409, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.0432, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.1102, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(3.9475, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(4.2793, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vi_net_ = get_bflow('nsf_uncond_bounded', prior=prior, context=torch.ones(1,1))\n",
    "trained_vi_net = train_vi(vi_net_, posterior, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_samples = trained_vi_net.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = utils.pairplot(output_samples.detach().numpy(), limits=[[-2,2],[-2,2],[-2,2]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
