{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyknos/nflows/nn/nde\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sbi.inference import prepare_for_sbi, SNLE\n",
    "from sbi.simulators.linear_gaussian import diagonal_linear_gaussian\n",
    "import sbi.utils as sbi_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load theta and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../../arco_sims/hay_objectives_init_noise/hay_objectives_noisy.npz', allow_pickle=True)\n",
    "\n",
    "x = torch.as_tensor(data['data'], dtype=torch.float32)\n",
    "theta = torch.as_tensor(data['params'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000000, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameter_setup import load_ground_truth_params, load_prior_min, load_prior_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to standardize - needed to standardize x_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize_vals = np.load('../results/standardize_vals.npz')\n",
    "#x_mean = standardize_vals['data_mean']\n",
    "#x_std = standardize_vals['data_std']\n",
    "#\n",
    "#theta_mean = standardize_vals['theta_mean']\n",
    "#theta_std = standardize_vals['theta_std']\n",
    "\n",
    "x_mean = x.mean(dim=0)\n",
    "x_std = x.std(dim=0)\n",
    "\n",
    "theta_mean = theta.mean(dim=0)\n",
    "theta_std = theta.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (x-x_mean) / x_std\n",
    "theta = (theta-theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load x_o (which I got from running the simulator with Arco's ground truth params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_o = torch.as_tensor(np.load('../results/observation/x_o_hay.npz')['x_o'], dtype=torch.float32)\n",
    "x_o = (x_o - x_mean) / x_std\n",
    "x_o = x_o.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy simulator and dummy prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_simulator(theta):\n",
    "    return torch.ones(1,11)\n",
    "\n",
    "dummy_prior = sbi_utils.BoxUniform(torch.as_tensor(load_prior_min()), torch.as_tensor(load_prior_max()))\n",
    "_bound = torch.sqrt(torch.as_tensor(3.))\n",
    "dummy_prior_norm = sbi_utils.BoxUniform(-_bound*torch.ones(35), _bound*torch.ones(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.utils.get_nn_models import likelihood_nn, posterior_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator, prior, x_shape = prepare_for_sbi(dummy_simulator, dummy_prior_norm)\n",
    "\n",
    "nsf = likelihood_nn(\n",
    "    model='nsf',\n",
    "    theta_shape=prior.sample().shape,\n",
    "    x_o_shape=x_shape,\n",
    "    hidden_features=100,\n",
    "    flow_num_transforms=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = SNLE(\n",
    "    simulator, \n",
    "    prior,\n",
    "    x_shape,\n",
    "    external_data=(theta, x), # todo: remove the [:1000] to train on all datasamples\n",
    "    mcmc_method='slice', \n",
    "    density_estimator=nsf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Zero-length parameter theta implies zero simulations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network successfully converged after 36 epochs.\n"
     ]
    }
   ],
   "source": [
    "posterior = inference(\n",
    "    num_rounds=1,\n",
    "    num_simulations_per_round=0,\n",
    "    batch_size=100, # default is 50\n",
    "    stop_after_epochs=5, # default is 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../results/posteriors/200625_PosteriorSNL_HayObjectives.pickle', 'wb') as handle:\n",
    "    pickle.dump(posterior, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.utils.get_nn_models import get_vi_net\n",
    "from sbi.utils.vi import train_vi, train_mle\n",
    "import torch\n",
    "import sbi.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../bFlows')\n",
    "from bflows.utils.neural_net.get_bounded_flows import get_bflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_net = get_vi_net(parameter_dim=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temper the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "x_o_raw = torch.as_tensor(np.load('../results/observation/x_o_hay.npz')['x_o'], dtype=torch.float32)\n",
    "#x_o = x_o[::7]\n",
    "#x_o[0] = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_o = (x_o - x_mean) / x_std\n",
    "#x_o = x_o.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7385, -1.4222, -1.4183, -1.4148, -3.7464, -3.7221, -1.4562, -3.7375,\n",
       "         -3.7124, -3.7509, -1.4359]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:    tensor(587.6192, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(589.5394, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(571.5406, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(576.1451, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(565.2867, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(552.0637, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(547.5931, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(526.9346, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(513.1395, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(502.5460, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(488.6754, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(456.3266, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(427.1918, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(384.1162, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(349.7704, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(314.1287, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(292.5004, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(269.6989, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(264.0023, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(260.4235, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(259.0118, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(252.5528, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(248.9718, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(245.4677, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(240.0400, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(234.7751, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(227.4371, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(219.4762, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(212.4167, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(208.4969, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(203.7543, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(195.8353, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(194.6552, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(190.9035, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(192.2827, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(188.0603, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(182.9447, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(188.4731, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(179.9044, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(181.5920, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(183.2611, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(181.6760, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(179.5035, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(178.8158, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(178.5012, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(181.4497, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(175.6363, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(175.1589, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(173.4975, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(176.3616, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(175.4401, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(177.4095, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(177.4883, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(173.1867, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(175.2371, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(171.8075, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(171.5620, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(171.7382, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(171.2269, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.9067, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(170.6445, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(168.6832, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(170.8913, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.7947, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(170.8602, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.5038, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(171.4164, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(170.0452, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.0669, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(168.7455, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(168.2117, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(167.6274, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.4503, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(168.4236, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(169.1920, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.9130, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.3427, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.3552, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.1003, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.2846, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.1621, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.9889, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.3134, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(167.1558, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.8689, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(168.5600, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.9856, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.0049, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.1156, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.9473, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.8982, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.3154, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.7966, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.2041, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(167.7018, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.6385, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.3077, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.3973, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.1225, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.7337, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.3504, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(166.6884, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.3360, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.0804, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.8851, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.0463, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.6992, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.5882, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.8190, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.4284, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.9612, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.3948, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.4405, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.1084, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.6819, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.8168, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.8246, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.7588, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.1603, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.2166, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(165.8378, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.8855, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.7363, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.2103, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.1399, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9098, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9873, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.3761, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.5617, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9963, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.2409, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.6040, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.8306, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.3524, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0899, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.4223, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.4910, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.1858, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(164.3622, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.6211, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.7022, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.8331, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.3790, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.3422, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.9386, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.5969, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.0144, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.5813, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.6048, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.7730, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.5712, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.8518, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9265, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.1334, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0252, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.2964, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.8558, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.9962, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.3001, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.8087, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0855, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:    tensor(161.6002, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.5236, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.1431, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0247, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.3055, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9756, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.1827, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.5194, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4372, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9121, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.6620, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.8632, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.2605, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7926, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2710, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.7307, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.3555, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.3639, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7927, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0863, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9169, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.6159, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7643, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.0104, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.7015, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0455, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.0036, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.6589, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.4725, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.2712, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.0321, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.0304, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.4572, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(163.4387, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.2429, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(162.5694, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9802, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9542, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7655, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2598, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9054, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2826, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2663, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2170, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2242, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4679, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2880, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2417, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6067, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2292, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.7577, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0789, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9281, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.8909, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6591, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1955, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.4306, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.8826, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6150, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1125, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.3794, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7262, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9057, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8423, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.1221, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7800, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.6213, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.4058, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2909, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.1834, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9171, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0597, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2172, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9314, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.2676, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4360, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9996, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4716, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.6619, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.6835, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.7794, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9971, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9887, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1303, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6658, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0549, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6302, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8850, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.5500, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.1617, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0523, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0336, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9541, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9110, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6695, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7443, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3733, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.5497, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7773, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1063, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.6486, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3399, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.5280, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1872, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6761, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1474, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8515, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2212, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0252, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.5542, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7861, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4359, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2170, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7918, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6261, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1468, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1369, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3109, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.9449, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0273, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7429, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3522, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1696, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1567, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0887, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2054, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.4327, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1427, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7773, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0367, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4934, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4828, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.8447, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0898, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3029, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7843, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1286, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3940, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7473, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2442, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.9562, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.9879, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.5675, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7749, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7707, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8600, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6453, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2672, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.3875, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.8282, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2857, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2211, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1432, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.5982, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.9395, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.5952, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.7064, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.3312, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1196, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.4275, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.6962, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:    tensor(158.8453, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.8071, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1790, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1683, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.0771, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3855, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8634, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.5612, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8264, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.4956, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.4460, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2999, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7380, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.8998, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.3068, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.0378, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.6354, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1461, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.9735, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.6587, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.3963, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.5773, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.9916, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.9204, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.9039, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.8332, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.6570, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2982, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.5871, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1837, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.9359, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4331, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.0033, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1464, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.9642, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6346, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2271, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.1231, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0618, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1352, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.6066, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.9120, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.8066, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.5220, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.8891, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.0880, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.2260, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.2111, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.7436, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.6872, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.6013, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0855, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.3402, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.1906, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.6225, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.5843, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.7710, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(157.4660, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.3763, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(158.0007, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.6273, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.0415, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.8500, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.1057, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.6985, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.5102, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.7895, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(161.9485, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.7537, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(160.4739, grad_fn=<MeanBackward0>)\n",
      "loss:    tensor(159.7483, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vi_net_ = get_bflow('nsf_uncond_bounded', prior=prior, context=x_o, num_coupling=10, hidden_features=200)\n",
    "trained_vi_net = train_vi(vi_net_, posterior, batch_size=500, stop_after_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../results/posteriors/200626_PosteriorSNL_VI_HayObjectives.pickle', 'wb') as handle:\n",
    "    pickle.dump(trained_vi_net, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../results/posteriors/200625_PosteriorSNL_HayObjectives.pickle', 'rb') as handle:\n",
    "    posterior = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../results/posteriors/200626_PosteriorSNL_VI_HayObjectives.pickle', 'rb') as handle:\n",
    "    trained_vi_net = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new iter, 0\n",
      "new iter, 1\n",
      "new iter, 2\n",
      "new iter, 3\n",
      "new iter, 4\n",
      "new iter, 5\n",
      "new iter, 6\n",
      "new iter, 7\n",
      "new iter, 8\n",
      "new iter, 9\n"
     ]
    }
   ],
   "source": [
    "s = []\n",
    "num_iter = 10\n",
    "num_samples_per_iter = 1000\n",
    "\n",
    "for k in range(num_iter):\n",
    "    print(\"new iter,\", k)\n",
    "    samples = trained_vi_net.sample(num_samples_per_iter).detach()\n",
    "\n",
    "    # denormalize the samples\n",
    "    samples = samples * theta_std + theta_mean\n",
    "\n",
    "    # convert to list for pickling\n",
    "    samples_list = samples.numpy().tolist()\n",
    "    \n",
    "    s.append(samples_list)\n",
    "s = np.asarray(s)\n",
    "s = s.reshape(num_iter*num_samples_per_iter, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = s.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.760894775390625, 0.03747161105275154, 311.67449951171875, 0.04597768187522888, 125.99951171875, 0.03170895576477051, 0.003550072433426976, 0.00017799632041715086, 0.00033542027813382447, 0.17584046721458435, 0.004214823246002197, 0.006075966637581587, 0.00029447791166603565, 0.9574453830718994, 0.2807340621948242, 0.07596757262945175, 0.023587381467223167, 0.027448244392871857, 1.2038705348968506, 0.14533483982086182, 0.0008007129654288292, 0.0001848628744482994, 0.005856282077729702, 0.0016922764480113983, 0.0005353949964046478, 0.015331077389419079, 0.5456022024154663, -2.722404956817627, 1.8450510501861572, 0.33715224266052246, 6.530581595143303e-05, 2.844995105988346e-05, 3.344303331687115e-05, 2.7550133381737396e-05, 0.4508410692214966]\n"
     ]
    }
   ],
   "source": [
    "print(s_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/samples/200626_PosteriorSNL_hayObjectives_VI_samples.pickle', 'wb') as handle:\n",
    "    pickle.dump(s_list, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_params = [137.862136034238,\n",
    " 0.0005793520824526776,\n",
    " 199.1298048149789,\n",
    " 0.0006108049075983062,\n",
    " 152.1647419393015,\n",
    " 0.00742430653684668,\n",
    " 0.0010965218089651857,\n",
    " 0.0008186770602065786,\n",
    " 0.00011435310571497434,\n",
    " 0.0022763084379226854,\n",
    " 0.0036986082079423594,\n",
    " 0.00013883334761566004,\n",
    " 3.2474860530531394e-06,\n",
    " 0.005426837416265438,\n",
    " 0.10568666421909532,\n",
    " 0.04812676692103998,\n",
    " 0.094826660872338,\n",
    " 0.013854989311151315,\n",
    " 3.9010342040060975,\n",
    " 3.8851157748263354,\n",
    " 0.009964343408409574,\n",
    " 0.006000497448875096,\n",
    " 0.0012602755616811401,\n",
    " 0.01392240648099882,\n",
    " 0.06283710421562513,\n",
    " 6.68382138396179e-05,\n",
    " 0.08311048073340864,\n",
    " -2.9836949894223825,\n",
    " 1.9642986130169147,\n",
    " 1.2999358521956366,\n",
    " 4.4931548434199036e-05,\n",
    " 2.062212836678345e-05,\n",
    " 4.22059843297412e-05,\n",
    " 2.2409802171891654e-05,\n",
    " 1.7109080877160283]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_params_norm = (torch.as_tensor(gt_params) - theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = trained_vi_net.sample(10000).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = posterior.log_prob(samples, x=x_o).detach().numpy()\n",
    "prob_of_gt = posterior.log_prob(gt_params_norm.unsqueeze(0), x=x_o).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f981c406160>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "_ = plt.hist(probs, bins=100)\n",
    "ax.axvline(prob_of_gt, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-43.4859])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_prior_norm.log_prob(gt_params_norm.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictives\n",
    "Has to be done in a different virtual env with python 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
