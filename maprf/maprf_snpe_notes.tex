\documentclass[10pt,english]{article}
%\special{papersize=210mm,297mm}
%\usepackage[a4paper,left=20mm,right=20mm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{epsfig}
\usepackage{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{units}
\usepackage{pstricks}
\usepackage{color}
\usepackage{bm}
\bibliographystyle{plain}

\usepackage{framed}

% Super-handy maths abbreviations
\newcommand{\yb}{\mathbf{y}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\pstrxobs}{q_\phi(\theta|x_0)}
\newcommand{\pstrx}{q_\phi(\theta|x)}


\title{mapRF \& SNPE}
\author{}
\date{}

\begin{document}


\maketitle

\section{Sufficient statistics for nonlinearly parametrized filters in GLMs}
\label{seq:sufficiency_STA}

We consider the generalized linear model (GLMs) with Poisson noise distribution $\mathcal{P}(\bullet|\lambda)$, canonical link function $\eta= \log(\lambda)$ and linear predictor $\eta = \beta^\top \xb$. 
Additionally, we assume the linear filter $\beta(\theta)$ itself to be a fixed (nonlinear) function of (lower-dimensional) parameter $\theta$. 
\begin{align}
p(\yb | \Xb, \theta) &= \prod_i p(y_i | \xb_i, \theta) \nonumber \\
&= \prod_i \mathcal{P}\left(y_i | \lambda_i = \exp(\beta(\theta)^\top \xb_i) \right) \nonumber \\
&= \prod_i  \frac{\lambda_i^{y_i}}{y_i!} \exp(-\lambda_i) \nonumber \\
&= \prod_i \frac{1}{y_i!} \exp\left( y_i \beta(\theta)^\top \xb_i - \exp(\beta(\theta)^\top \xb_i) \right) \nonumber \\
&= \frac{1}{\prod_i y_i!} \exp\left( y_i \sum_i \beta(\theta)^\top \xb_i - \exp(\beta(\theta)^\top \xb_i) \right) \nonumber \\
&= \frac{1}{\prod_i y_i!} \exp\left( \beta(\theta)^\top \left(\Xb^\top \yb \right) - \sum_i  \exp(\beta(\theta)^\top \xb_i) \right) \nonumber \\
&=: h(\yb) g_{(\theta, \Xb)}(T(\yb)) \nonumber
\end{align}
with 
\begin{align}
h(\yb) &:= \frac{1}{\prod_i y_i!} \nonumber \\
g_{(\theta, \Xb)}(T(\yb)) &= \frac{1}{Z} \exp\left( \beta(\theta)^\top \left(\Xb^\top \yb \right) \right) \nonumber \\
T(\yb) &= \Xb^\top \yb \nonumber 
\end{align}
Note that this derivation is analoguous to derivations of sufficient statistics for 'standard' GLMs that treat $\beta$ as an independent variable rather than parametrized $\beta(\theta)$.
\begin{align}
p(\theta | \yb, \Xb) = p(\theta | \Xb^\top\yb, \Xb)
\end{align}

\section{SNPE SVI}
\label{seq:SVI_SNPV_vs_CDELFI}

\subsection{Derivation of non-SVI losses for CDELFI and SNPE}
Papamakarios \& Murray start out the derivation of their non-SVI algorithm from a variational approximation to a joint probability over $(\bf{x}, \theta)$ with convencience ('proposal
) prior $\tilde{p}(\theta)$ :
\begin{align}
D_{KL}\left( \ p(\bf{x}|\theta) \ \tilde{p}(\theta) \ || \ \tilde{p}(\bf{x}) \ q_\phi(\theta|\bf{x}) \ \right) &= - E_{p(\bf{x}|\theta)\tilde{p}(\theta)}\left[ \log q_\phi(\theta | \bf{x}) \right] + const. \nonumber \\
&\approx - \sum_n \log q_\phi(\theta_n | \bf{x}_n) \nonumber + const.
\end{align}
for $(\theta_n, \bf{x}_n) \sim \tilde{p}(\theta) p(\bf{x}|\theta)$.
The righthand-side is a variational lower bound that is optimized using stochastic gradient descent. 
Papamakarios \& Murray subsequently analytically correct for having used the wrong prior $\tilde{p}(\theta)$ in the loss, which however only works for certain functional forms for $p, \tilde{p}, q_\phi$.

\paragraph{}\noindent{}SNPE directly optimises the joint probability for the true prior:
\begin{align}
D_{KL}\left( \ p(\bf{x}|\theta) \ p(\theta) \ || \ p(\bf{x}) \ q_\phi(\theta|\bf{x}) \ \right) &= - E_{p(\bf{x}|\theta)p(\theta)}\left[ \log q_\phi(\theta | \bf{x}) \right] + const. \nonumber \\
&= - E_{p(\bf{x}|\theta)\tilde{p}(\theta)}\left[ \frac{p(\theta)}{\tilde{p}(\theta)} \log q_\phi(\theta | \bf{x}) \right] + const. \nonumber \\
&\approx - \sum_n \frac{p(\theta_n)}{\tilde{p}(\theta_n)} \log q_\phi(\theta_n | \bf{x}_n) + const. \nonumber
\end{align}

\subsection{Derivation of SVI losses for CDELFI and SNPE}

The derviation of the CDELFI SVI loss in Papamakarios \& Murray is only sketched (appendix D). 
Filling in between the verbal description of their approach, it seems to rest on 
\begin{align}
D_{KL}( \ q(\phi) \ || \ p(\phi \ | \ \{\bf{x}_n, \theta_n\}) \ ) &= - E_{q(\phi)}\left[ \log p(\phi \ | \ \{\bf{x}_n, \theta_n\}) \right]  + E_{q(\phi)}\left[ \log q(\phi) \right] \nonumber \\
&= - E_{q(\phi)}\left[ \log p( \{\bf{x}_n, \theta_n\} \ | \ \phi) \right]  + D_{KL}( \ q(\phi) \ || \ p(\phi) \ )  + const. \nonumber \\
&= - E_{q(\phi)}\left[ \sum_n \log p( \bf{x}_n, \theta_n \ | \ \phi) \right]  + D_{KL}( \ q(\phi) \ || \ p(\phi) \ )  + const. \nonumber \\
&= - E_{q(\phi)}\left[ \sum_n \log q_\phi( \theta_n | \bf{x}_n) \right]  + D_{KL}( \ q(\phi) \ || \ p(\phi) \ )  + const. \nonumber
\end{align}
where we plugged in the \textbf{joint-data likelihood} $p( \bf{x}_n, \theta_n \ | \ \phi) \propto q_\phi(\theta_n \ | \ \bf{x}_n) \tilde{p}(\bf{x}_n)$. 
This choice of likelihood $p( \bf{x}_n, \theta_n \ | \ \phi)$ is sensible given that we desired
\begin{align}
q_\phi(\theta | \bf{x}) \ \tilde{p}(\bf{x}) \approx p(x |\theta) \ \tilde{p}(\theta) \nonumber
\end{align}
already in the non-SVI case, and that in turn $(\bf{x}_n, \theta_n) \sim p(x|\theta)\tilde{p}(\theta)$ by construction of the artificial dataset.



\noindent{}What for SNPE? The variational bound here (for the first round, and without calibration kernel) reads 
\begin{align}
D_{KL}( \ q(\phi) \ || \ p(\phi \ | \ \{\bf{x}_n, \theta_n\}) \ ) &= - E_{q(\phi)}\left[ \log p( \{ \bf{x}_n, \theta_n \ \} \ | \ \phi) \right]  + D_{KL}( \ q(\phi) \ || \ p(\phi) \ )  + const. \nonumber \\
&= - E_{q(\phi)}\left[ \sum_n \frac{p(\theta)}{\tilde{p}(\theta)}\log q_\phi( \theta_n | \bf{x}_n) \right]  + D_{KL}( \ q(\phi) \ || \ p(\phi) \ )  + const. \nonumber
\end{align}
for the functional forms of $p(\phi)$ and $q(\phi)$ as above.
Note that we still draw data from the proposal prior, i.e.
\begin{align}
(\bf{x}_n, \theta_n) \sim p(x|\theta)\tilde{p}(\theta),\nonumber
\end{align} 
but for SNPE we actually want to directly approximate the real posterior,
\begin{align}
q_\phi(\theta \ | \ \bf{x}) p(\bf{x}) \approx p(x |\theta) p(\theta). \nonumber
\end{align}
Thus the task here is to find a likelihood $p( \bf{x}_n, \theta_n \ | \ \phi)$ that bridges this gap: 
on one hand being a good model for the data $(\bf{x}_n, \theta_n) \sim p(\bf{x}|\theta)\tilde{p}(\theta)$, but on the other hand being useful/adjustable to model $p(\bf{x} |\theta) p(\theta)$.

Reading the likelihood off the variational bound however, we find
\begin{align}
p( \bf{x}_n, \theta_n \ | \ \phi) \propto q_\phi(\theta_n \ | \ \bf{x}_n)^\frac{p(\theta_n)}{\tilde{p}(\theta_n)}
\label{eq:SNPE_likelihood}
\end{align}

\begin{framed}
\noindent{}\textbf{Remark:} What exactly does such a likelihood correspond to? 

Generally, is it even a proper likelihood? 
Take for instance a uniform prior on scalar $\theta$ with $p(\theta) = \frac{1}{b-a}$ for $\theta \in [a,b]$ and finite $a,b\in\mathbb{R}, b>a$, and $p(\theta) = 0$ otherwise. 
If the proposal prior has infinite support (as is the case if it is Gaussian), then the likelihood for fixed $\phi$ has density$p(\bf{x}, \theta \ | \ \phi) = 1$
for $(\bf{x}, \theta)$ with $\theta \notin [a,b]$.
Since $a,b$ are finite, $p(\bf{x}, \theta \ | \ \phi)$ in this case cannot be normalized over the full support $(\bf{x}, \theta) \in \mathbb{R}^{dim(\bf{x})} \times \mathbb{R}$. 

Similarly, if $p(\theta), \tilde{p}(\theta)$ and $q_\phi(\theta|\bf{x})$ all are univariate Gaussians, one can show that $p(\bf{x},\theta | \phi)$ becomes unnormalizable (for any fixed $\bf{x}$) if the proposal $\tilde{p}$ is broader than the prior $p$, i.e. $\tilde{\sigma}^2 > \sigma^2$:
\begin{align}
q_\phi(\theta | \bf{x})^\frac{p(\theta)}{\tilde{p}(\theta)} &\propto \exp c \exp \left( 2 \log \frac{|\theta - \mu_{\phi}|}{\sigma_{\phi}} + \left(\frac{\mu}{\sigma} - \frac{\tilde{\mu}}{\tilde{\sigma}}\right) \theta - \left(\sigma^{-2} - \tilde{\sigma}^{-2} \right) \theta^2 \right) \nonumber, \\
q_\phi(\theta | \bf{x})^\frac{p(\theta)}{\tilde{p}(\theta)} &\rightarrow 1 \mbox{ for } \theta \rightarrow \pm \infty \nonumber \mbox{ and } \tilde{\sigma}^{-2} < \sigma^{-2}
\end{align}
where $c \leq 0$ is a constant, and parameters $\mu, \tilde{\mu}, \mu_\phi$ correspond to prior $p$, proposal prior $\tilde{p}$ and posterior approximation $q_\phi$, respectively.
Most application scenarios will have $\tilde{\sigma} < \sigma$, but this was neither assumed a necessary condition so far, nor can it be guaranteed under all circumstances. 

An immediate next question is whether we need any (normalization) constraints on $p(\theta)/\tilde{p}(\theta)$ for eq. \ref{eq:SNPE_likelihood} to be a valid likelihood - if such constraints exist, they could lead to sensible normalization factors for the importance weights. 
Note that any such normalization factor $Z$ would show up as a sort of global temperature, as in $p( \bf{x}_n, \theta_n \ | \ \phi)^{\frac{1}{Z}}$
\end{framed}

\begin{framed}
\noindent{}\textbf{Remark:} An alternative likelihood to eq. \ref{eq:SNPE_likelihood} one might consider is 
\begin{align}
p( \bf{x}_n, \theta_n \ | \ \phi) \propto q_\phi(\theta_n \ | \ \bf{x}_n) \frac{p(\theta_n) \ \tilde{p}(\bf{x}_n)}{\tilde{p}(\theta_n) \ p(\bf{x}_n)},
\end{align}
since it is the density that translates between the known real and proposal priors. 
The multiplicative factors $\frac{p(\theta_n) \ \tilde{p}(\bf{x}_n)}{\tilde{p}(\theta_n) \ p(\bf{x}_n)}$ however are just constant factors in the loss that have no influence on the optimization results - we would recover the exact same solution as CDELFI and still need to explicitly correct for the proposal prior post-learning. 
\end{framed}
\end{document}
