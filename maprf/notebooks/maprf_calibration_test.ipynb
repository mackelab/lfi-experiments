{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SNPE & RF\n",
    "\n",
    "learning receptive field parameters from inputs (white-noise videos) and outputs (spike trains) of linear-nonlinear neuron models with parameterized linear filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibration testing, late-round SNPE-A and multimodality\n",
    "\n",
    "- this notebook implements **simulation-based calibration** (SBC) for the SNPE-A fits to the mapRF application\n",
    "\n",
    "\n",
    "- an important insight regarding SBC is that it does not test posteriors $p(\\theta|x_o)$, but entire conditional densities $p(\\theta |x)$\n",
    "- for SNPE (A/B), this means we can do SBC very cheaply, as these methods return $p(\\theta |x)$ 'amortized' if run from the prior $p(\\theta)$\n",
    "\n",
    "\n",
    "- another important insight is that SNPE (A/B) is also 'locally' amortized when run from proposal priors $\\tilde{p}(\\theta)$,\n",
    "i.e. for all $x \\sim \\tilde{p}(x) = \\int p(x|\\theta) \\tilde{p}(\\theta) d\\theta$\n",
    "- for SNPE-A, this means we can cheaply test calibration for later rounds (not just the first round) by comparing the calibration of the **uncorrected** conditional density $\\tilde{p}(\\theta|x)$ (as directly returned by the MDN) against the proposal $\\tilde{p}(\\theta)$.\n",
    "- doesn't work for SNPE-B (which on later rounds is valid only for $x\\sim\\tilde{p}(x)$, but returns the corrected $p(\\theta|x)$)\n",
    "\n",
    "\n",
    "- below, we show results that suggest that SBC can be used to find previously undetected multimodality in the posteriors: \n",
    "- in cases where the conditional density for $x\\sim \\tilde{p}(x)$ is generally multimodal, but the MDN failed to capture that (see first- and second-round results), we find the distribution of rank-statistic to be multimodal \n",
    "- this behavior disappears as soon as the MDN 'gets' that the posterior is in fact multimodal (see round #3 below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "from lfimodels.maprf.utils import get_maprf_prior_01, setup_sim, setup_sampler, get_data_o, quick_plot, contour_draws\n",
    "\n",
    "from lfimodels.maprf.maprf import maprf as model\n",
    "from lfimodels.maprf.maprfStats import maprfStats\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## training data and true parameters, data, statistics\n",
    "\n",
    "idx_cell = 3 # load toy cell number i \n",
    "filename = './results/toy_cells/toy_cell_' + str(idx_cell) + '.npy'\n",
    "g, prior, d = setup_sim(seed, path='.')\n",
    "obs_stats, pars_true = get_data_o(filename, g, seed)\n",
    "rf = g.model.params_to_rf(pars_true)[0]#[10:31, 10:31]\n",
    "\n",
    "plt.imshow(rf, interpolation='None')\n",
    "plt.show()\n",
    "\n",
    "print('spike count', obs_stats[0,-1])\n",
    "\n",
    "labels_params=['bias', 'gain', 'phase', 'freq', 'angle', 'ratio', 'width', 'xo', 'yo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SBC(object):\n",
    "    \"\"\" SIMULATION-BASED CALIBRATION\"\"\"\n",
    "    def __init__(self, generator, inf, f, dim):\n",
    "        \n",
    "        self.generator = generator # delfi generator object\n",
    "        self.inf = inf             # delfi inference object\n",
    "        self.f = f                 # test-function (maps x->f(x))\n",
    "        self.dim = dim             # dimensionality of f(x)\n",
    "        \n",
    "    def sample_full(self, N):\n",
    "        out = self.generator.gen(N) # will sample from generator.proposal unless it's None\n",
    "        return out[0], out[1]\n",
    "\n",
    "    def get_conditional(self, x):\n",
    "        return self.inf.predict_uncorrected(x)\n",
    "    \n",
    "    def test(self, N, L):\n",
    "        data = self.sample_full(N)  \n",
    "        N = data[0].shape[0]\n",
    "        \n",
    "        res  = np.empty((N, self.dim))\n",
    "        \n",
    "        for i in range(N):\n",
    "            f0 = self.f(data[0][i,:]).reshape(1,-1)\n",
    "            p = self.get_conditional(data[1][i,:])\n",
    "            \n",
    "            batch = self.f(p.gen(L))\n",
    "            assert batch.shape==(L, f0.size)\n",
    "                \n",
    "            res[i,:] = np.sum( f0 < batch , axis=0)\n",
    "\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some copy-pasted parameter settings for the used network and training regime\n",
    "\n",
    "filter_sizes=[3,3,3,3,2]   # 5 conv ReLU layers\n",
    "n_filters=(16,16,32,32,64) # 16 to 64 filters\n",
    "pool_sizes=[1,2,2,2,2]     # \n",
    "n_hiddens=[100,100,100]     # 3 fully connected layers\n",
    "\n",
    "n_train=100000\n",
    "n_components=8\n",
    "n_rounds=1\n",
    "n_inputs_hidden = 1\n",
    "\n",
    "lr_decay = 0.99\n",
    "epochs=20\n",
    "minibatch=50\n",
    "\n",
    "svi=False          \n",
    "reg_lambda=0.      \n",
    "pilot_samples=1000 \n",
    "prior_norm = True   \n",
    "init_norm = False  \n",
    "rank = None   \n",
    "\n",
    "run_id = 7 # seventh iteration of network/settings for eLife application\n",
    "run_id_first = 6 # seventh iteration branches from sixth only after first round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first round\n",
    "\n",
    "- first-round SNPE-A fit: single-component MoG fit to all $x\\sim p(x)$ resulting from the (broad) prior\n",
    "- very hard problem (prior big $\\rightarrow$ $p(x)$ broad $\\rightarrow$ many different $x$ to map onto their $\\theta$\n",
    "\n",
    "\n",
    "- we compare the calibration of $p(\\theta|x)$ (as returned by the MDN on round #1) against the prior $p(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round_ = 1\n",
    "\n",
    "g, _, _ = setup_sim(seed, path='.') # reinit generator seed \n",
    "_,_ = get_data_o(filename, g, seed) # for pilot runs\n",
    "filename4 = './results/SNPE/maprf_100k_elife_prior01_run_'+str(run_id_first)+'_round' + str(round_) + '_param9_nosvi_base_net_only.pkl'\n",
    "tmp = io.load_pkl(filename4)\n",
    "inf = infer.CDELFI(generator=g, obs=obs_stats, prior_norm=prior_norm, init_norm=init_norm,\n",
    "                 pilot_samples=pilot_samples, seed=seed, reg_lambda=reg_lambda, svi=svi,\n",
    "                 n_components=tmp['network.spec_dict']['n_components'], rank=rank,\n",
    "                 n_hiddens=n_hiddens, n_filters=n_filters, n_inputs = (1,d,d),\n",
    "                 filter_sizes=filter_sizes, pool_sizes=pool_sizes, n_inputs_hidden=n_inputs_hidden,verbose=True)\n",
    "inf.network.params_dict = tmp['network.params_dict']\n",
    "inf.round = round_\n",
    "posterior=inf.predict_uncorrected(obs_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_prior = dd.TransformedNormal(m=g.prior.m, S = g.prior.S,\n",
    "                            flags=[0,0,2,1,2,1,1,0,0],\n",
    "                            lower=[0,0,0,0,0,0,0,0,0], upper=[0,0,np.pi,0,2*np.pi,0,0,0,0]) \n",
    "\n",
    "plot_post = dd.mixture.TransformedGaussianMixture.MoTG(\n",
    "                            ms= [posterior.xs[i].m for i in range(posterior.n_components)], \n",
    "                            Ss =[posterior.xs[i].S for i in range(posterior.n_components)],\n",
    "                            a = posterior.a, \n",
    "                            flags=[0,0,2,1,2,1,1,0,0],\n",
    "                            lower=[0,0,0,0,0,0,0,0,0], upper=[0,0,np.pi,0,2*np.pi,0,0,0,0]) \n",
    "\n",
    "\n",
    "lims = np.array([[-2, -2,    0, 0,       0, 0, 0, -1.5, -1], \n",
    "                 [ 2,  2,np.pi, 3, 2*np.pi, 3, 3, 1.5,   1]]).T\n",
    "\n",
    "fig, _ = plot_pdf(plot_post, pdf2=plot_prior, lims=lims, gt=plot_post._f(pars_true.reshape(1,-1)).reshape(-1), \n",
    "                  figsize=(16,16), resolution=100, diag_only=True, diag_only_cols=3, diag_only_rows=3,\n",
    "                  labels_params=labels_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "sbc = SBC(generator=g, inf=inf, f=f, dim=9)\n",
    "\n",
    "N = 10000\n",
    "L = N//100\n",
    "res = sbc.test(N, L)\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.hist(res[:,i], color='r', normed=True, bins=np.linspace(0,L+1,L+2)-.5)\n",
    "    plt.title(labels_params[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second round\n",
    "\n",
    "- second-round SNPE-A fit: single-component MoG fit to all $x\\sim \\tilde{p}(x)$ resulting from the single-component proposal\n",
    "- easier problem (proposal narrow $\\rightarrow$ $\\tilde{p}(x)$ narrower $\\rightarrow$ fewer different $x$ to map onto their $\\theta$\n",
    "\n",
    "\n",
    "- we compare the calibration of  **uncorrected** $\\tilde{p}(\\theta|x)$ (returned by the MDN on round #2) against the **proposal prior** $\\tilde{p}(\\theta)$ returned from round #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok, slightly retarded I didn't specifically save the proposals. First have to load first-round net to get that\n",
    "\n",
    "round_ = 1\n",
    "g, _, _ = setup_sim(seed, path='.') # reinit generator seed \n",
    "_,_ = get_data_o(filename, g, seed) # for pilot runs\n",
    "filename4 = './results/SNPE/maprf_100k_elife_prior01_run_'+str(run_id_first)+'_round' + str(round_) + '_param9_nosvi_base_net_only.pkl'\n",
    "tmp = io.load_pkl(filename4)\n",
    "inf = infer.CDELFI(generator=g, obs=obs_stats, prior_norm=prior_norm, init_norm=init_norm,\n",
    "                 pilot_samples=pilot_samples, seed=seed, reg_lambda=reg_lambda, svi=svi,\n",
    "                 n_components=tmp['network.spec_dict']['n_components'], rank=rank,\n",
    "                 n_hiddens=n_hiddens, n_filters=n_filters, n_inputs = (1,d,d),\n",
    "                 filter_sizes=filter_sizes, pool_sizes=pool_sizes, n_inputs_hidden=n_inputs_hidden,verbose=True)\n",
    "inf.network.params_dict = tmp['network.params_dict']\n",
    "\n",
    "proposal = inf.predict_uncorrected(obs_stats)\n",
    "\n",
    "\n",
    "round_ = 2\n",
    "g, _, _ = setup_sim(seed, path='.') # reinit generator seed \n",
    "_,_ = get_data_o(filename, g, seed) # for pilot runs\n",
    "filename4 = './results/SNPE/maprf_100k_elife_prior01_run_'+str(run_id)+'_round' + str(round_) + '_param9_nosvi_base_net_only.pkl'\n",
    "tmp = io.load_pkl(filename4)\n",
    "inf = infer.CDELFI(generator=g, obs=obs_stats, prior_norm=prior_norm, init_norm=init_norm,\n",
    "                 pilot_samples=pilot_samples, seed=seed, reg_lambda=reg_lambda, svi=svi,\n",
    "                 n_components=tmp['network.spec_dict']['n_components'], rank=rank,\n",
    "                 n_hiddens=n_hiddens, n_filters=n_filters, n_inputs = (1,d,d),\n",
    "                 filter_sizes=filter_sizes, pool_sizes=pool_sizes, n_inputs_hidden=n_inputs_hidden,verbose=True)\n",
    "inf.network.params_dict = tmp['network.params_dict']\n",
    "inf.generator.proposal = proposal\n",
    "\n",
    "posterior=inf.predict_uncorrected(obs_stats)\n",
    "inf.round = round_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **uncorrected** posterior $\\tilde{p}(\\theta | x_o)$ in round 2\n",
    "\n",
    "- notice the estimated marginal to the gain is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_post = dd.mixture.TransformedGaussianMixture.MoTG(\n",
    "                            ms= [posterior.xs[i].m for i in range(posterior.n_components)], \n",
    "                            Ss =[posterior.xs[i].S for i in range(posterior.n_components)],\n",
    "                            a = posterior.a, \n",
    "                            flags=[0,0,2,1,2,1,1,0,0],\n",
    "                            lower=[0,0,0,0,0,0,0,0,0], upper=[0,0,np.pi,0,2*np.pi,0,0,0,0]) \n",
    "\n",
    "fig, _ = plot_pdf(plot_post, pdf2=plot_prior, lims=lims, gt=plot_post._f(pars_true.reshape(1,-1)).reshape(-1), \n",
    "                  figsize=(16,16), resolution=100, diag_only=True, diag_only_cols=3, diag_only_rows=3,\n",
    "                  labels_params=labels_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "sbc = SBC(generator=g, inf=inf, f=f, dim=9)\n",
    "\n",
    "N = 10000\n",
    "L = N//100\n",
    "res = sbc.test(N, L)\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.hist(res[:,i], color='r', normed=True, bins=np.linspace(0,L+1,L+2)-.5)\n",
    "    plt.title(labels_params[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# third-round\n",
    "\n",
    "\n",
    "- **last**-round SNPE-A fit: **four**-component MoG fit to all $x\\sim \\tilde{p}(x)$ resulting from a single-component proposal \n",
    "\n",
    "\n",
    "\n",
    "- we compare the calibration of **uncorrected** $\\tilde{p}(\\theta|x)$ (returned by the MDN on round #3) against the **proposal prior** $\\tilde{p}(\\theta)$ returned from round #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ok, slightly retarded I didn't specifically save the proposals. First have to load second-round net to get that\n",
    "\n",
    "round_ = 2\n",
    "g, _, _ = setup_sim(seed, path='.') # reinit generator seed \n",
    "_,_ = get_data_o(filename, g, seed) # for pilot runs\n",
    "filename4 = './results/SNPE/maprf_100k_elife_prior01_run_'+str(run_id)+'_round' + str(round_) + '_param9_nosvi_base_net_only.pkl'\n",
    "tmp = io.load_pkl(filename4)\n",
    "inf = infer.CDELFI(generator=g, obs=obs_stats, prior_norm=prior_norm, init_norm=init_norm,\n",
    "                 pilot_samples=pilot_samples, seed=seed, reg_lambda=reg_lambda, svi=svi,\n",
    "                 n_components=tmp['network.spec_dict']['n_components'], rank=rank,\n",
    "                 n_hiddens=n_hiddens, n_filters=n_filters, n_inputs = (1,d,d),\n",
    "                 filter_sizes=filter_sizes, pool_sizes=pool_sizes, n_inputs_hidden=n_inputs_hidden,verbose=True)\n",
    "inf.network.params_dict = tmp['network.params_dict']\n",
    "\n",
    "proposal = inf.predict_uncorrected(obs_stats)\n",
    "\n",
    "\n",
    "round_ = 3\n",
    "g, _, _ = setup_sim(seed, path='.') # reinit generator seed \n",
    "_,_ = get_data_o(filename, g, seed) # for pilot runs\n",
    "filename4 = './results/SNPE/maprf_100k_elife_prior01_run_'+str(run_id)+'_round' + str(round_) + '_param9_nosvi_base_net_only.pkl'\n",
    "tmp = io.load_pkl(filename4)\n",
    "inf = infer.CDELFI(generator=g, obs=obs_stats, prior_norm=prior_norm, init_norm=init_norm,\n",
    "                 pilot_samples=pilot_samples, seed=seed, reg_lambda=reg_lambda, svi=svi,\n",
    "                 n_components=tmp['network.spec_dict']['n_components'], rank=rank,\n",
    "                 n_hiddens=n_hiddens, n_filters=n_filters, n_inputs = (1,d,d),\n",
    "                 filter_sizes=filter_sizes, pool_sizes=pool_sizes, n_inputs_hidden=n_inputs_hidden,verbose=True)\n",
    "inf.network.params_dict = tmp['network.params_dict']\n",
    "inf.generator.proposal = proposal\n",
    "\n",
    "posterior=inf.predict_uncorrected(obs_stats)\n",
    "inf.round = round_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **uncorrected** posterior $\\tilde{p}(\\theta | x_o)$ in round #3\n",
    "\n",
    "- notice the skewed weighting between the two modes for the gain - they're actually almost equally large after the analytical correction for the proposal, which in this case was closer to the right mode (see above for the uncorrected posterior on round #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_post = dd.mixture.TransformedGaussianMixture.MoTG(\n",
    "                            ms= [posterior.xs[i].m for i in range(posterior.n_components)], \n",
    "                            Ss =[posterior.xs[i].S for i in range(posterior.n_components)],\n",
    "                            a = posterior.a, \n",
    "                            flags=[0,0,2,1,2,1,1,0,0],\n",
    "                            lower=[0,0,0,0,0,0,0,0,0], upper=[0,0,np.pi,0,2*np.pi,0,0,0,0]) \n",
    "\n",
    "fig, _ = plot_pdf(plot_post, pdf2=plot_prior, lims=lims, gt=plot_post._f(pars_true.reshape(1,-1)).reshape(-1), \n",
    "                  figsize=(16,16), resolution=100, diag_only=True, diag_only_cols=3, diag_only_rows=3,\n",
    "                  labels_params=labels_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "sbc = SBC(generator=g, inf=inf, f=f, dim=9)\n",
    "\n",
    "N = 10000\n",
    "L = N//100\n",
    "res = sbc.test(N, L)\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.hist(res[:,i], color='r', normed=True, bins=np.linspace(0,L+1,L+2)-.5)\n",
    "    plt.title(labels_params[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibrating calibration\n",
    "- sanity check:\n",
    "- let us set $p(\\theta|x) = p(\\theta) \\ \\forall x$ for now, which is trivially self-consistent with $p(\\theta)$.\n",
    "- same for resp. $\\tilde{p}(\\theta|x) = \\tilde{p}(\\theta) \\ \\forall x$ and $\\tilde{p}(\\theta)$.\n",
    "- histograms should be beautifully uniform\n",
    "- if they look too wild, we should use more samples (higher N) for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SBC_naive(object):\n",
    "    \"\"\" SIMULATION-BASED CALIBRATION\"\"\"\n",
    "    def __init__(self, generator, inf, f, dim):\n",
    "        \n",
    "        self.generator = generator # delfi generator object\n",
    "        self.inf = inf             # delfi inference object\n",
    "        self.f = f                 # test-function (maps x->f(x))\n",
    "        self.dim = dim             # dimensionality of f(x)\n",
    "        \n",
    "    def sample_full(self, N):\n",
    "        out = self.generator.gen(N) # will sample from generator.proposal unless it's None\n",
    "        return out[0], out[1]\n",
    "\n",
    "    def get_conditional(self, x):\n",
    "        if self.generator.proposal is None:\n",
    "            return self.generator.prior\n",
    "        else:\n",
    "            return self.generator.proposal \n",
    "    \n",
    "    def test(self, N, L):\n",
    "        data = self.sample_full(N)  \n",
    "        N = data[0].shape[0]\n",
    "        \n",
    "        res  = np.empty((N, self.dim))\n",
    "        \n",
    "        for i in range(N):\n",
    "            f0 = self.f(data[0][i,:]).reshape(1,-1)\n",
    "            p = self.get_conditional(data[1][i,:])\n",
    "            \n",
    "            batch = self.f(p.gen(L))\n",
    "            assert batch.shape==(L, f0.size)\n",
    "                \n",
    "            res[i,:] = np.sum( f0 < batch , axis=0)\n",
    "\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "sbc = SBC_naive(generator=g, inf=inf, f=f, dim=9)\n",
    "\n",
    "N = 10000\n",
    "L = N//100\n",
    "res = sbc.test(N, L)\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.hist(res[:,i], color='r', normed=True, bins=np.linspace(0,L+1,L+2)-.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
