{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.hodgkinhuxley.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfimodels.hodgkinhuxley.HodgkinHuxley import HodgkinHuxley\n",
    "from lfimodels.hodgkinhuxley.HodgkinHuxleyStatsMoments import HodgkinHuxleyStatsMoments\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_params, labels_params = utils.obs_params()\n",
    "\n",
    "seed = None\n",
    "I, t_on, t_off, dt = utils.syn_current()\n",
    "m = HodgkinHuxley(I, dt, seed=seed, cython=True)\n",
    "p = utils.prior(true_params=true_params, seed=seed)\n",
    "s = HodgkinHuxleyStatsMoments(t_on=t_on, t_off=t_off)\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "obs = utils.syn_obs_data(I, dt, true_params, seed=seed)\n",
    "obs_stats = utils.syn_obs_stats(I=I, t_on=t_on, t_off=t_off, dt=dt, params=true_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res = infer.SNPE(g, \n",
    "                 obs=obs_stats, \n",
    "                 pilot_samples=1000, \n",
    "                 n_hiddens=[50], \n",
    "                 seed=seed, \n",
    "                 prior_norm=True)\n",
    "\n",
    "# run with N samples\n",
    "n_train = 3000\n",
    "n_rounds = 3\n",
    "kernel_loss = None\n",
    "epochs_cbk = None\n",
    "minibatch_cbk = None\n",
    "out = res.run(n_train=n_train, n_rounds=n_rounds, \n",
    "                  kernel_loss=kernel_loss, epochs_cbk = epochs_cbk, minibatch_cbk=minibatch_cbk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_min = res.generator.prior.lower\n",
    "prior_max = res.generator.prior.upper\n",
    "\n",
    "prior_lims = np.concatenate((prior_min.reshape(-1,1),\n",
    "                             prior_max.reshape(-1,1)),\n",
    "                            axis=1)\n",
    "\n",
    "plot_pdf(out[2][-1].xs[0], lims=prior_lims, samples=None, \n",
    "         gt=true_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = 1\n",
    "stats, params, w = out[1][r][1], out[1][r][0], out[1][r][2]\n",
    "\n",
    "bam = np.zeros(w.size)\n",
    "paramz = (params *res.params_std) + res.params_mean\n",
    "for i in range(w.size):\n",
    "    bam[i] = np.all(paramz[i,:] <= res.generator.prior.upper) and np.all(paramz[i,:] >= res.generator.prior.lower)\n",
    "print('bam', bam.mean())\n",
    "\n",
    "w /= w.sum()\n",
    "\n",
    "print('ESS', 1./np.sum(w**2))\n",
    "\n",
    "idx_worst = np.argmax(w)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "tmp = np.cumsum(np.sort(w))\n",
    "plt.plot(tmp)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "\n",
    "true_paramz = (true_params - res.params_mean) / res.params_std\n",
    "plt.plot(params.mean(axis=0).reshape(-1))\n",
    "plt.plot(true_paramz.reshape(-1) , 'ro-')\n",
    "plt.plot(params[idx_worst,:], 'g')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "obs_statz = (obs_stats - res.stats_mean) / res.stats_std\n",
    "plt.plot(stats.mean(axis=0).reshape(-1))\n",
    "plt.plot(obs_stats.reshape(-1) , 'ro-')\n",
    "plt.plot(stats[idx_worst,:], 'g')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res_k = infer.SNPE(g, obs=obs_stats, pilot_samples=1000, n_hiddens=[50], seed=seed, prior_norm=True)\n",
    "\n",
    "# run with N samples\n",
    "n_train = 3000\n",
    "n_rounds = 3\n",
    "kernel_loss = 'x_kl'\n",
    "epochs_cbk = 10000\n",
    "minibatch_cbk = n_train\n",
    "out_k = res_k.run(n_train=n_train, n_rounds=n_rounds, \n",
    "                  kernel_loss=kernel_loss, epochs_cbk = epochs_cbk, minibatch_cbk=minibatch_cbk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_min = res_k.generator.prior.lower\n",
    "prior_max = res_k.generator.prior.upper\n",
    "\n",
    "prior_lims = np.concatenate((prior_min.reshape(-1,1),\n",
    "                             prior_max.reshape(-1,1)),\n",
    "                            axis=1)\n",
    "\n",
    "plot_pdf(out_k[2][-1].xs[0], lims=prior_lims, samples=None, \n",
    "         gt=true_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = 4\n",
    "stats, params, w = out_k[1][r][1], out_k[1][r][0], out_k[1][r][2]\n",
    "w /= w.sum()\n",
    "\n",
    "print('ESS', 1./np.sum(w**2))\n",
    "\n",
    "idx_worst = np.argmax(w)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "tmp = np.cumsum(np.sort(w))\n",
    "plt.plot(tmp)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(out_k[0][r]['cbk_loss'])\n",
    "plt.show()\n",
    "\n",
    "print('A', out_k[0][r]['cbkrnl'].A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=1\n",
    "plt.plot(out_k[0][r]['cbk_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in range(n_rounds):\n",
    "    plot_pdf(out[2][r], pdf2=out_k[2][r], lims=prior_lims, samples=None, resolution=100,\n",
    "         gt=true_params, figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.glm.utils as utils\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.misc\n",
    "import sys\n",
    "\n",
    "\n",
    "def run_smc(model, prior, summary, obs_stats, n_params, seed=None,n_particles=1e3,eps_init=2,maxsim=5e7,\n",
    "            dist_=None, ps=None):\n",
    "    \"\"\"Runs Sequential Monte Carlo ABC algorithm.\n",
    "    Adapted from epsilonfree code https://raw.githubusercontent.com/gpapamak/epsilon_free_inference/8c237acdb2749f3a340919bf40014e0922821b86/demos/mg1_queue_demo/mg1_abc.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "         Model\n",
    "    prior :\n",
    "         Prior\n",
    "    summary :\n",
    "         Function to compute summary statistics\n",
    "    obs_stats: \n",
    "         Observed summary statistics\n",
    "    n_params : \n",
    "         Number of parameters\n",
    "    seed : int or None\n",
    "        If set, randomness in sampling is disabled\n",
    "    n_particles : int\n",
    "        Number of particles for SMC-ABC\n",
    "    eps_init : Float\n",
    "        Initial tolerance for SMC-ABC\n",
    "    maxsim : int\n",
    "        Maximum number of simulations for SMC-ABC\n",
    "    \"\"\"\n",
    "    n_particles = int(n_particles)\n",
    "    \n",
    "    if dist_ is None:\n",
    "        dist_ = calc_dist\n",
    "    # check for subfolders, create if they don't exist\n",
    "    dirs = {}\n",
    "    dirs['dir_abc'] = './results/abc/'\n",
    "    for k, v in dirs.items():\n",
    "        if not os.path.exists(v):\n",
    "            os.makedirs(v)\n",
    "\n",
    "    prefix = str(n_params)+'params'\n",
    "    \n",
    "    #####################\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # set parameters\n",
    "    eps_last = 0.01\n",
    "    eps_decay = 0.9\n",
    "    ess_min = 0.5\n",
    "    maxsim = int(maxsim)\n",
    "\n",
    "    all_ps = []\n",
    "    all_logweights = []\n",
    "    all_eps = []\n",
    "    all_nsims = []\n",
    "\n",
    "    # sample initial population\n",
    "    weights = np.ones(n_particles, dtype=float) / n_particles\n",
    "    logweights = np.log(weights)\n",
    "    eps = eps_init\n",
    "    iter = 0\n",
    "    nsims = 0\n",
    "\n",
    "    if ps is None:\n",
    "        ps = np.empty([n_particles, n_params])\n",
    "\n",
    "        for i in range(n_particles):\n",
    "\n",
    "            dist = float('inf')\n",
    "\n",
    "            while dist > eps:\n",
    "                ps[i] = prior.gen(n_samples=1)[0]\n",
    "                states = model.gen_single(ps[i])\n",
    "                stats = summary.calc([states])\n",
    "                dist = dist_(stats, obs_stats)\n",
    "                nsims += 1\n",
    "                \n",
    "    else: \n",
    "        assert ps.shape == (n_particles, n_params)\n",
    "        assert ps.dtype == float\n",
    "\n",
    "    all_ps.append(ps)\n",
    "    all_logweights.append(logweights)\n",
    "    all_eps.append(eps)\n",
    "    all_nsims.append(nsims)\n",
    "\n",
    "    break_flag = False\n",
    "\n",
    "    print('iteration = {0}, eps = {1:.2}, ess = {2:.2%}'.format(iter, float(eps), 1.0))\n",
    "\n",
    "    while eps > eps_last:\n",
    "\n",
    "        iter += 1\n",
    "        eps *= eps_decay\n",
    "\n",
    "        # calculate population covariance\n",
    "        mean = np.mean(ps, axis=0)\n",
    "        cov = 2.0 * (np.dot(ps.T, ps) / n_particles - np.outer(mean, mean))\n",
    "        std = np.linalg.cholesky(cov)\n",
    "\n",
    "        # perturb particles\n",
    "        new_ps = np.empty_like(ps)\n",
    "        new_logweights = np.empty_like(logweights)\n",
    "\n",
    "        for i in range(n_particles):\n",
    "\n",
    "            dist = float('inf')\n",
    "\n",
    "            while dist > eps:\n",
    "                idx = discrete_sample(weights)[0]\n",
    "                new_ps[i] = ps[idx] + np.dot(std, np.random.randn(n_params))\n",
    "                states = model.gen_single(new_ps[i])\n",
    "                stats = summary.calc([states])\n",
    "                dist = dist_(stats, obs_stats)\n",
    "                nsims += 1\n",
    "                if nsims>=maxsim:\n",
    "                    print('Maximum number of simulations reached.')\n",
    "                    break_flag = True\n",
    "                    break\n",
    "            #new_ps_i = new_ps[i]\n",
    "            logkernel = -0.5 * np.sum(np.linalg.solve(std, (new_ps[i] - ps).T) ** 2, axis=0)\n",
    "            #new_logweights[i] = -float('inf') if np.any(new_ps_i > prior_max) or np.any(new_ps_i < prior_min) else -scipy.misc.logsumexp(logweights + logkernel)\n",
    "            new_logweights[i] = prior.eval(new_ps[i, np.newaxis], log=True)[0] - scipy.misc.logsumexp(logweights + logkernel)\n",
    "\n",
    "            if break_flag:\n",
    "                break\n",
    "\n",
    "        if break_flag:\n",
    "            break\n",
    "\n",
    "        ps = new_ps\n",
    "        logweights = new_logweights - scipy.misc.logsumexp(new_logweights)\n",
    "        weights = np.exp(logweights)\n",
    "\n",
    "        # calculate effective sample size\n",
    "        ess = 1.0 / (np.sum(weights ** 2) * n_particles)\n",
    "        print('iteration = {0}, eps = {1:.2}, ess = {2:.2%}'.format(iter, float(eps), ess))\n",
    "\n",
    "        if ess < ess_min:\n",
    "\n",
    "            # resample particles\n",
    "            new_ps = np.empty_like(ps)\n",
    "\n",
    "            for i in range(n_particles):\n",
    "                idx = discrete_sample(weights)[0]\n",
    "                new_ps[i] = ps[idx]\n",
    "\n",
    "            ps = new_ps\n",
    "            weights = np.ones(n_particles, dtype=float) / n_particles\n",
    "            logweights = np.log(weights)\n",
    "\n",
    "        all_ps.append(ps)\n",
    "        all_logweights.append(logweights)\n",
    "        all_eps.append(eps)\n",
    "        all_nsims.append(nsims)\n",
    "\n",
    "    return all_ps, all_logweights, all_eps, all_nsims\n",
    "\n",
    "\n",
    "        \n",
    "def calc_dist(stats_1, stats_2):\n",
    "    \"\"\"Euclidian distance between summary statistics\"\"\"\n",
    "    return np.sqrt(np.sum((stats_1 - stats_2) ** 2))\n",
    "\n",
    "def discrete_sample(p, n_samples=1):\n",
    "    \"\"\"\n",
    "    Samples from a discrete distribution.\n",
    "    :param p: a distribution with N elements\n",
    "    :param n_samples: number of samples\n",
    "    :return: vector of samples\n",
    "    \"\"\"\n",
    "\n",
    "    # check distribution\n",
    "    #assert isdistribution(p), 'Probabilities must be non-negative and sum to one.'\n",
    "\n",
    "    # cumulative distribution\n",
    "    c = np.cumsum(p[:-1])[np.newaxis, :]\n",
    "\n",
    "    # get the samples\n",
    "    r = np.random.rand(n_samples, 1)\n",
    "    return np.sum((r > c).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../nips_2017/')\n",
    "#from scripts.run_abc import run_smc as run_smc \n",
    "import delfi.utils.io as io\n",
    "import time\n",
    "\n",
    "def z_dist(stats_1, stats_2):\n",
    "    \"\"\"Euclidian distance between summary statistics\"\"\"\n",
    "    statz_1 = (stats_1 - res.stats_mean) / res.stats_std\n",
    "    statz_2 = (stats_2 - res.stats_mean) / res.stats_std\n",
    "    return np.sqrt(np.sum((statz_1 - statz_2) ** 2))\n",
    "\n",
    "\n",
    "n_particles=1e3\n",
    "eps_init=0.8\n",
    "maxsim=1e4\n",
    "seed=42\n",
    "\n",
    "ps = out[2][-1].gen(int(n_particles))\n",
    "\n",
    "stats = np.vstack([s.calc(datum) for datum in m.gen(ps)])\n",
    "statz = (stats - res.stats_mean) / res.stats_std\n",
    "obs_statz = (obs_stats - res.stats_mean) / res.stats_std\n",
    "dists = np.hstack([z_dist(obs_stats, stats[i,:]) for i in range(statz.shape[0])])\n",
    "plt.hist(dists)\n",
    "plt.show()\n",
    "\n",
    "for i in range(20):\n",
    "    plt.plot(np.sort(statz[:,i]))\n",
    "    plt.plot(np.arange(statz.shape[0]), np.ones(statz.shape[0])*obs_statz[0,i], 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../nips_2017/')\n",
    "#from scripts.run_abc import run_smc as run_smc \n",
    "import delfi.utils.io as io\n",
    "import time\n",
    "\n",
    "def z_dist(stats_1, stats_2):\n",
    "    \"\"\"Euclidian distance between summary statistics\"\"\"\n",
    "    statz_1 = (stats_1 - res.stats_mean) / res.stats_std\n",
    "    statz_2 = (stats_2 - res.stats_mean) / res.stats_std\n",
    "    return np.sqrt(np.sum((statz_1 - statz_2) ** 2))\n",
    "\n",
    "\n",
    "n_particles=1e3\n",
    "eps_init=0.8\n",
    "maxsim=1e4\n",
    "seed=42\n",
    "\n",
    "ps = p.gen(int(n_particles))\n",
    "\n",
    "stats = np.vstack([s.calc(datum) for datum in m.gen(ps)])\n",
    "statz = (stats - res.stats_mean) / res.stats_std\n",
    "obs_statz = (obs_stats - res.stats_mean) / res.stats_std\n",
    "dists = np.hstack([z_dist(obs_stats, stats[i,:]) for i in range(statz.shape[0])])\n",
    "plt.hist(dists)\n",
    "plt.show()\n",
    "\n",
    "for i in range(20):\n",
    "    plt.plot(np.sort(statz[:,i]))\n",
    "    plt.plot(np.arange(statz.shape[0]), np.ones(statz.shape[0])*obs_statz[0,i], 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_params = p.ndim\n",
    "t = time.time()\n",
    "ps_smc, logweights_smc, eps_smc, all_nsims_smc = run_smc(model=m, prior=p, summary=s,\n",
    "                                                         ps=ps,\n",
    "                                                         dist_=z_dist,\n",
    "                                                         obs_stats=obs_stats,\n",
    "                                                         n_params=n_params, \n",
    "                                                         seed=seed, \n",
    "                                                         n_particles=n_particles,\n",
    "                                                         eps_init=eps_init,\n",
    "                                                         maxsim=maxsim)\n",
    "\n",
    "weights_smc = np.exp(logweights_smc)\n",
    "nsims_smc = np.asarray(all_nsims_smc)\n",
    "\n",
    "m_smc = []\n",
    "cov_smc = []\n",
    "for i in range(len(ps_smc)):\n",
    "    m_smc.append(np.dot(weights_smc[i],ps_smc[i]))\n",
    "    cov_smc.append(np.cov(ps_smc[i].T,aweights = weights_smc[i]))\n",
    "    \n",
    "print('total time', time.time() -t)\n",
    "\n",
    "p_smc = dd.Gaussian(m=m_smc[-1], S=cov_smc[-1])\n",
    "sam = p_smc.gen(1000) #(p_smc.gen(1000) - res_k.params_mean) / res_k.params_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(out[2][-1].xs[0].S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cov_smc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_smc[-1].shape, sam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sam = ps_smc[-1]\n",
    "for r in range(n_rounds-1, n_rounds):\n",
    "    plot_pdf(out[2][r], pdf2=out_k[2][r], lims=prior_lims, samples=sam.T, resolution=100,\n",
    "         gt=true_params, figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../nips_2017/')\n",
    "#from scripts.run_abc import run_smc as run_smc \n",
    "import delfi.utils.io as io\n",
    "import time\n",
    "\n",
    "n_particles=1e3\n",
    "eps_init=0.1\n",
    "maxsim=2e4\n",
    "seed=42\n",
    "ps = out_k[2][-1].gen(int(n_particles))\n",
    "\n",
    "def z_dist(stats_1, stats_2):\n",
    "    \"\"\"Euclidian distance between summary statistics\"\"\"\n",
    "    statz_1 = (stats_1 - res.stats_mean) / res.stats_std\n",
    "    statz_2 = (stats_2 - res.stats_mean) / res.stats_std\n",
    "    return np.sqrt(np.sum((statz_1 - statz_2) ** 2))\n",
    "\n",
    "#stats = np.vstack([s.calc(datum) for datum in m.gen(ps)])\n",
    "#statz = (stats - res.stats_mean) / res.stats_std\n",
    "#obs_statz = (obs_stats - res.stats_mean) / res.stats_std\n",
    "#dists = np.hstack([z_dist(obs_stats, stats[i,:]) for i in range(statz.shape[0])])\n",
    "#plt.hist(dists)\n",
    "#plt.show()\n",
    "\n",
    "#for i in range(20):\n",
    "#    plt.plot(np.sort(statz[:,i]))\n",
    "#    plt.plot(np.arange(statz.shape[0]), np.ones(statz.shape[0])*obs_statz[0,i], 'r')\n",
    "#    plt.show()\n",
    "\n",
    "n_params = p.ndim\n",
    "t = time.time()\n",
    "ps_smc, logweights_smc, eps_smc, all_nsims_smc = run_smc(model=m, prior=p, summary=s,\n",
    "                                                         ps=ps,\n",
    "                                                         dist_=z_dist,\n",
    "                                                         obs_stats=obs_stats,\n",
    "                                                         n_params=n_params, \n",
    "                                                         seed=seed, \n",
    "                                                         n_particles=n_particles,\n",
    "                                                         eps_init=eps_init,\n",
    "                                                         maxsim=maxsim)\n",
    "\n",
    "weights_smc = np.exp(logweights_smc)\n",
    "nsims_smc = np.asarray(all_nsims_smc)\n",
    "\n",
    "m_smc = []\n",
    "cov_smc = []\n",
    "for i in range(len(ps_smc)):\n",
    "    m_smc.append(np.dot(weights_smc[i],ps_smc[i]))\n",
    "    cov_smc.append(np.cov(ps_smc[i].T,aweights = weights_smc[i]))\n",
    "    \n",
    "print('total time', time.time() -t)\n",
    "\n",
    "p_smc = dd.Gaussian(m=m_smc[-1], S=cov_smc[-1])\n",
    "sam = p_smc.gen(1000) #(p_smc.gen(1000) - res_k.params_mean) / res_k.params_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
