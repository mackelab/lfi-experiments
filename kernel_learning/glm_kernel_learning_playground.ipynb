{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning kernels for the GLM example. \n",
    "\n",
    "we optimize kernels such that\n",
    "$ K(x_n, x_0) p(\\theta_n) / \\tilde{p}(\\theta_n) \\approx 1$. \n",
    "\n",
    "Spoiler:\n",
    "starts to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approach\n",
    "\n",
    "The above problem doesn't require MDNs at all. \n",
    "Once prior, proposal, kernel and simulator are fixed and we drew an artificial dataset $(x_n, \\theta_n)$, we're good to play. \n",
    "Let's run SNPE as usual, note down the data-sets $(x_n, \\theta_n)$, proposal priors and importance weights it produced over rounds, and afterwards play with the kernel on those fixed targets. \n",
    "\n",
    "- Remark: results look a lot worse if we convert to Students-t distributions. Could be that kernel shape (squared-exponential in $x$) has to match proposal-prior shape (squared in $\\theta$ for students-T with df=3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try out a bunch of simple squared losses. \n",
    "\n",
    "### 1. basic squared loss\n",
    "\n",
    "argmin $ \\sum_n \\left( 1 - \\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $, emphasizing the absolute value of $\\approx 1$. \n",
    "\n",
    "### 2. inverse-kernel loss\n",
    "\n",
    "argmin $ \\sum_n \\left( \\frac{1}{K(x_n,x_0)} - \\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $, emphasizes that the kernel should be small where importance weights are large\n",
    "\n",
    "### 3. log-space loss\n",
    "\n",
    "argmin $ \\sum_n \\left( \\log(\\frac{1}{K(x_n,x_0)}) - \\log(\\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)}) \\right)^2 = \\sum_n \\left( \\log(\\frac{p(\\theta_n)K(x_n,x_0)}{\\tilde{p}(\\theta_n)}) \\right)^2$ emphasizes ratios of $\\approx 1$.\n",
    "\n",
    "### 4. inverse weights-dominated loss\n",
    "\n",
    "argmin $ \\sum_n \\left( K(x_n,x_0) - \\frac{\\tilde{p}(\\theta_n)}{p(\\theta_n)} \\right)^2 $, emphasizes that the kernel should be large where importance weights are small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.glm.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfimodels.glm.GLM import GLM\n",
    "from lfimodels.glm.GLMStats import GLMStats\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42\n",
    "m = GLM(seed=seed)\n",
    "p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "s = GLMStats(n_summary=m.n_params)\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "true_params, labels_params = utils.obs_params()\n",
    "obs = utils.obs_data(true_params, seed=seed)\n",
    "obs_stats = utils.obs_stats(true_params, seed=seed)\n",
    "\n",
    "rerun = False  # if False, will try loading file from disk\n",
    "\n",
    "try:\n",
    "    assert rerun == False, 'rerun requested'\n",
    "    sam = np.load('sam.npz')['arr_0']\n",
    "except:\n",
    "    sam = utils.pg_mcmc(true_params, obs)\n",
    "    np.savez('sam.npz', sam)\n",
    "    \n",
    "seed = 98\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res = infer.SNPE(g, \n",
    "                 obs=obs_stats, \n",
    "                 n_hiddens=[50], \n",
    "                 seed=seed, \n",
    "                 convert_to_T=None, \n",
    "                 pilot_samples=0,\n",
    "                 svi=True,\n",
    "                 reg_lambda=0.01,\n",
    "                 prior_norm=False)\n",
    "\n",
    "logs, tds, posteriors = res.run(n_train=5000, \n",
    "                                n_rounds=2, \n",
    "                                minibatch=100, \n",
    "                                epochs=1000, \n",
    "                                round_cl=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNPE fits over rounds (used as proposal priors in the follow-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run with Gaussian proposals\n",
    "for r in range(len(tds)):\n",
    "    posterior = posteriors[r]\n",
    "    plot_pdf(posterior.xs[0], \n",
    "             lims=[-2,2], \n",
    "             samples=sam, \n",
    "             gt=true_params, \n",
    "             figsize=(9,9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick-check: efficacy of importance sampling on this setting\n",
    "- importance sampling should take the parameter statistics (mean and var) of the proposal (black lines) and bring them back to that of the prior (cyan lines). \n",
    "- if the importance-weighted statistics are far off the prior statistics, IS didn't work. \n",
    "- if the importance-weighted statistics are not even closer to the prior statistics than those of the proposal, something is messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_ = dg.Default(model=m, prior=p, summary=s)\n",
    "th_prior, x_prior = g_.gen(5000)\n",
    "\n",
    "r = 1 # pick proposal after first round\n",
    "\n",
    "params = tds[r][0].astype(np.float32)\n",
    "\n",
    "if r > 0:\n",
    "    p_prior = g.prior.eval(params,log=False)\n",
    "    p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "    iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "else:\n",
    "    iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "\n",
    "w = (iws).reshape(-1,1)\n",
    "w = w / np.sum(w)\n",
    "\n",
    "mu_th =   np.sum( w * tds[r][0],           axis=0).reshape(1,-1)\n",
    "sig2_th = np.sum( w * (tds[r][0]-mu_th)**2, axis=0)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(th_prior,axis=0), 'co--')\n",
    "plt.plot(tds[r][0].mean(axis=0), 'ko-')\n",
    "plt.plot(mu_th.flatten(), 'ro-')\n",
    "#plt.plot(true_params.flatten(), 'go--')\n",
    "plt.legend(['E[th]', 'raw E[th]','IS E[th]'])\n",
    "plt.xlabel('# summary statistic')\n",
    "plt.title('raw and importance sampled data means')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.var(th_prior,axis=0), 'co--')\n",
    "plt.plot(tds[r][0].var(axis=0), 'ko-')\n",
    "plt.plot(sig2_th.flatten(), 'ro-')\n",
    "plt.legend(['Var[th]', 'raw Var[th]','IS Var[th]'])\n",
    "plt.xlabel('# summary statistic')\n",
    "plt.title('raw and importance sampled data variances')\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "KernelLayer implements a very simple kernel \n",
    "\n",
    "$ K(x_n,x_0) = \\exp( - (x-x_0)^\\top A (x - x_0))$, \n",
    "\n",
    "where $A =BB^\\top$ and $B$ is diagonal.\n",
    "\n",
    "- simple kernel cannot follow $\\frac{\\tilde{p}(\\theta_n)}{p(\\theta_n)} > 1$ (cannot correct for importance weights < 1)\n",
    "- kernels end up learning $\\forall n: K(x_n,x_0) = 1$ (via $A \\approx 0$)\n",
    "\n",
    "KernelLayer_offset implements  \n",
    "\n",
    "$ K(x_n,x_0) = c \\exp( - (x-x_0)^\\top A (x - x_0) )$\n",
    "\n",
    "where $c = exp(Z)$ is a non-negative scaling factor\n",
    "\n",
    "- adding pre-factor exp(Z) to kernel\n",
    "- prefaktor starts shifting (to negative $Z$), but still $A \\approx 1$\n",
    "\n",
    "We use offset kernels in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "dtype = theano.config.floatX\n",
    "ndim_x = 10\n",
    "\n",
    "class KernelLayer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, B=lasagne.init.Normal(0.01), **kwargs):\n",
    "        super(KernelLayer, self).__init__(incoming, **kwargs)\n",
    "        num_inputs = self.input_shape[1]\n",
    "        self.eye = T.eye(num_inputs)\n",
    "        self.B = self.add_param(B, (num_inputs, ), name='B')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        D = T.dot(self.B*self.eye, self.B*self.eye.T)\n",
    "        inner = (input.dot(D)*input).sum(axis=1)\n",
    "        return T.exp(-inner)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0],)\n",
    "\n",
    "    \n",
    "class KernelLayer_offset(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, B=lasagne.init.Normal(0.01), Z=lasagne.init.Normal(0.01), **kwargs):\n",
    "        super(KernelLayer_offset, self).__init__(incoming, **kwargs)\n",
    "        num_inputs = self.input_shape[1]\n",
    "        self.eye = T.eye(num_inputs)\n",
    "        self.B = self.add_param(B, (num_inputs, ), name='B')\n",
    "        self.Z = self.add_param(Z, (1,), name='Z')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        D = T.dot(self.B*self.eye, self.B*self.eye.T)\n",
    "        inner = (input.dot(D)*input).sum(axis=1)\n",
    "        return T.exp(-inner + self.Z)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0],)    \n",
    "    \n",
    "input_var = T.fmatrix('inputs')\n",
    "target_var = T.fvector('targets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. basic loss\n",
    "\n",
    "argmin $ \\sum_n \\left( 1 - \\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $\n",
    "\n",
    "- final average errors $\\approx 1$, achieved by trivial solution  $\\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\approx 0$. \n",
    "- no obvious trend in kernel value relative to importance weight size. Kernel too simple?\n",
    "- generally, kernels focusing on countering the largest weights (offeset Z negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for r in range(1, len(tds)): # pick best fit\n",
    "    \n",
    "    print('round #' + str(r))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # x - x0\n",
    "    dx = (tds[r][1] - obs_stats).astype(np.float32)\n",
    "    # weights (normalized)\n",
    "    params = tds[r][0].astype(np.float32)\n",
    "\n",
    "    if r > 0:\n",
    "        p_prior = g.prior.eval(params,log=False)\n",
    "        p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "        iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "    else:\n",
    "        iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "    \n",
    "    #iws = (tds[r][2].reshape(-1)).astype(np.float32) # weights returned by SNPE are renormalized by default now\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (1 - prediction*target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    idx = np.argsort(iws)\n",
    "    plt.plot(train_errs)\n",
    "    plt.title('training error')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    krnl  = pred[idx]\n",
    "    ikrnl = 1./pred[idx]\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogy(iiws[idx])\n",
    "    plt.semilogy(krnl)\n",
    "    plt.legend(['inv. weights', 'kernel'])\n",
    "    plt.title('kernel should track inverse weights')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(ikrnl)\n",
    "    plt.legend(['weights', 'inv. kernel'])\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    plt.title('inv. kernel should track weights')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(krnl*iws[idx])\n",
    "    plt.legend(['weights', 'kernel*weights'])\n",
    "    plt.title('kernel*weights should be flatter than weights')\n",
    "    plt.show()\n",
    "    \n",
    "    print('loss with / without learned kernels', np.mean((iws[idx]-1)**2), np.mean((pred[idx]*iws[idx]-1)**2))\n",
    "    print('mean and std of raw importance weights', np.mean(iws), np.std(iws))\n",
    "    print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))   \n",
    "    \n",
    "    \n",
    "    w = (iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x = np.sum( w * (tds[r][1]-mu_x)**2, axis=0)\n",
    "    mu_x, sig2_x\n",
    "\n",
    "    w = (pred*iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x_K =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x_K = np.sum( w * (tds[r][1]-mu_x_K)**2, axis=0)\n",
    "    mu_x_K, sig2_x_K\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.mean(x_prior,axis=0), 'co--')\n",
    "    plt.plot(tds[r][1].mean(axis=0), 'ko-')\n",
    "    plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "    plt.plot(mu_x.flatten(), 'ro-')\n",
    "    plt.plot(obs_stats.flatten(), 'go--')\n",
    "    plt.legend(['E[x]', 'raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data means')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.var(x_prior,axis=0), 'co--')\n",
    "    plt.plot(tds[r][1].var(axis=0), 'ko-')\n",
    "    plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "    plt.plot(sig2_x.flatten(), 'ro-')\n",
    "    plt.legend(['Var[x]', 'raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data variances')\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coordinate-wise loss landscape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "B = l_dot.B.get_value()\n",
    "A_base=  np.diag(B**2)\n",
    "\n",
    "Z = l_dot.Z.get_value()\n",
    "\n",
    "def A_(lambd=None, i=0):\n",
    "    A = A_base.copy()\n",
    "    if not lambd is None:\n",
    "        A[i,i]= lambd\n",
    "    return A\n",
    "def K(dx, A, Z):\n",
    "    return  np.exp(- np.sum(dx.dot(A)* dx,axis=1) + Z )\n",
    "\n",
    "def loss_map(lambd, i):\n",
    "    return np.mean( (1 - iws*K(dx, A_(lambd,i), Z))**2 )\n",
    "\n",
    "\n",
    "lambds = np.exp(np.log(10) * np.linspace(-3, 3, 1000))\n",
    "l      = np.zeros_like(lambds)              \n",
    "plt.figure(figsize=(8,16))\n",
    "for i in range(10):    \n",
    "    plt.subplot(5,2,i+1)\n",
    "    for j in range(len(lambds)):\n",
    "        l[j] = loss_map(lambds[j], i)\n",
    "    plt.semilogx(lambds, l)\n",
    "    if A_base[i,i]>np.min(lambds):\n",
    "        plt.semilogx([A_base[i,i], A_base[i,i]], [1.,1.], 'r*', markersize=10)        \n",
    "    if i == 0:\n",
    "        plt.legend(['loss', 'found optimum'], loc=4)\n",
    "    if i > 7:\n",
    "        plt.xlabel('1/sig^2')\n",
    "    if np.mod(i,2) ==0:\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "plt.savefig('kernel_learning_GLM_example_basicLoss_offsetKernel_gaussianProposals_coordinatewiseErrorFunction.pdf')        \n",
    "plt.show()\n",
    "\n",
    "lss - train_errs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spherical kernel loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "def A_(lambd):\n",
    "    A = lambd * np.eye(10)    \n",
    "    return A\n",
    "def K(dx, A, Z):\n",
    "    return  np.exp(- np.sum(dx.dot(A)* dx,axis=1) + Z )\n",
    "\n",
    "def loss_map(lambd, Z):\n",
    "    return np.mean( (1 - iws*K(dx, A_(lambd), Z))**2 )\n",
    "\n",
    "\n",
    "lambds = np.exp(np.log(10) * np.linspace(-3, 3, 100))\n",
    "Zs     = [l_dot.Z.get_value()] #np.linspace(-10, 10, 20)\n",
    "l      = np.zeros((len(lambds), len(Zs)))            \n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "for j in range(len(lambds)):\n",
    "    for z in range(len(Zs)):\n",
    "        l[j, z] = loss_map(lambds[j], Zs[z])\n",
    "plt.loglog(lambds, l)\n",
    "plt.legend(['log loss', 'found optimum'], loc=4)\n",
    "plt.xlabel('1/sig^2')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "lss - train_errs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. inverse-kernel loss\n",
    "\n",
    "argmin $ \\sum_n \\left( \\frac{1}{K(x_n,x_0)} - \\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $\n",
    "\n",
    "- arguably we care more about too large than about too small weights - this simple squared loss is dominated by large weights, meaning the kernel (where possible) will try to counter those.\n",
    "\n",
    "\n",
    "- starts doing something constructive\n",
    "- average kernel-weighted importance weights become closer to $1$ than averag raw importance weights.\n",
    "- sometimes also learns a flat kernel. Local optimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for r in range(1, len(tds)):\n",
    "    \n",
    "    print('round #' + str(r))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # x - x0\n",
    "    dx = (tds[r][1] - obs_stats).astype(np.float32)\n",
    "    # weights (normalized)\n",
    "    \n",
    "    params = tds[r][0].astype(np.float32)\n",
    "\n",
    "    if r > 0:\n",
    "        p_prior = g.prior.eval(params,log=False)\n",
    "        p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "        iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "    else:\n",
    "        iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "    \n",
    "    #iws = (tds[r][2].reshape(-1)).astype(np.float32) # weights returned by SNPE are renormalized by default now\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (T.inv(prediction)-target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    idx = np.argsort(iws)\n",
    "    plt.plot(train_errs)\n",
    "    plt.title('training error')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    krnl  = pred[idx]\n",
    "    ikrnl = 1./pred[idx]\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogy(iiws[idx])\n",
    "    plt.semilogy(krnl)\n",
    "    plt.legend(['inv. weights', 'kernel'])\n",
    "    plt.title('kernel should track inverse weights')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(ikrnl)\n",
    "    plt.legend(['weights', 'inv. kernel'])\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    plt.title('inv. kernel should track weights')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(krnl*iws[idx])\n",
    "    plt.legend(['weights', 'kernel*weights'])\n",
    "    plt.title('kernel*weights should be flatter than weights')\n",
    "    plt.show()\n",
    "    \n",
    "    print('loss with/without learned kernels', np.mean((iws[idx]-1)**2), np.mean((iws[idx]-1/pred[idx])**2))\n",
    "    print('mean and std of raw importance weights', np.mean(iws), np.std(iws))    \n",
    "    print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))\n",
    "    \n",
    "    \n",
    "    w = (iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x = np.sum( w * (tds[r][1]-mu_x)**2, axis=0)\n",
    "    mu_x, sig2_x\n",
    "\n",
    "    w = (pred*iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x_K =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x_K = np.sum( w * (tds[r][1]-mu_x_K)**2, axis=0)\n",
    "    mu_x_K, sig2_x_K\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tds[r][1].mean(axis=0), 'ko-')\n",
    "    plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "    plt.plot(mu_x.flatten(), 'ro-')\n",
    "    plt.plot(obs_stats.flatten(), 'go--')\n",
    "    plt.legend(['raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data means')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(tds[r][1].var(axis=0), 'ko-')\n",
    "    plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "    plt.plot(sig2_x.flatten(), 'ro-')\n",
    "    plt.legend(['raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data variances')\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. log-space loss\n",
    "\n",
    "argmin $ \\sum_n \\left( \\log(\\frac{1}{K(x_n,x_0)}) - \\log(\\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)}) \\right)^2 = \\sum_n  \\left( \\log(1) - \\log(\\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)K(x_n,x_0)}) \\right)^2 = \\sum_n \\left( \\log(\\frac{p(\\theta_n)K(x_n,x_0)}{\\tilde{p}(\\theta_n)}) \\right)^2$\n",
    "\n",
    "- appear to suffer from most of the weights being negative: fit draws the geometric mean towards $1$, making the few large samples in the data-set gigantic. Would definately have to renormalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for r in range(1, len(tds)):\n",
    "    \n",
    "    print('round #' + str(r))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # x - x0\n",
    "    dx = (tds[r][1] - obs_stats).astype(np.float32)\n",
    "    # weights (normalized)\n",
    "    params = tds[r][0].astype(np.float32)\n",
    "\n",
    "    if r > 0:\n",
    "        p_prior = g.prior.eval(params,log=False)\n",
    "        p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "        iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "    else:\n",
    "        iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "    \n",
    "        \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./tds[r][2].reshape(-1)).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (T.log(prediction)+T.log(target_var))**2\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    idx = np.argsort(iws)\n",
    "    plt.plot(train_errs)\n",
    "    plt.title('training error')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    krnl  = pred[idx]\n",
    "    ikrnl = 1./pred[idx]\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogy(iiws[idx])\n",
    "    plt.semilogy(krnl)\n",
    "    plt.legend(['inv. weights', 'kernel'])\n",
    "    plt.title('kernel should track inverse weights')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(ikrnl)\n",
    "    plt.legend(['weights', 'inv. kernel'])\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    plt.title('inv. kernel should track weights')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.semilogy(iws[idx])\n",
    "    plt.semilogy(krnl*iws[idx])\n",
    "    plt.legend(['weights', 'kernel*weights'])\n",
    "    plt.title('kernel*weights should be flatter than weights')\n",
    "    plt.show()\n",
    "    \n",
    "    print('loss with/without learned kernels', np.mean(np.log(pred[idx]*iws[idx])**2), np.mean(np.log(iws[idx])**2))\n",
    "    print('mean and std of raw importance weights', np.mean(iws), np.std(iws))    \n",
    "    print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))\n",
    "    \n",
    "    \n",
    "    w = (iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x = np.sum( w * (tds[r][1]-mu_x)**2, axis=0)\n",
    "    mu_x, sig2_x\n",
    "\n",
    "    w = (pred*iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x_K =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x_K = np.sum( w * (tds[r][1]-mu_x_K)**2, axis=0)\n",
    "    mu_x_K, sig2_x_K\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tds[r][1].mean(axis=0), 'ko-')\n",
    "    plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "    plt.plot(mu_x.flatten(), 'ro-')\n",
    "    plt.plot(obs_stats.flatten(), 'go--')\n",
    "    plt.legend(['raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data means')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(tds[r][1].var(axis=0), 'ko-')\n",
    "    plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "    plt.plot(sig2_x.flatten(), 'ro-')\n",
    "    plt.legend(['raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data variances')\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. inverse weights-dominated loss\n",
    "\n",
    "argmin $ \\sum_n \\left( K(x_n,x_0) - \\frac{\\tilde{p}(\\theta_n)}{p(\\theta_n)} \\right)^2 $, i.e. squared error on inverse importance weights\n",
    "\n",
    "- squared error dominated by small weights (large $\\frac{\\tilde{p}(\\theta_n)}{p(\\theta_n)}$), leading to large positive offsets $Z$ (yielding $K(x,x_0)$ values around 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for r in range(1, len(tds)):\n",
    "    \n",
    "    print('round #' + str(r))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # x - x0\n",
    "    dx = (tds[r][1] - obs_stats).astype(np.float32)\n",
    "    # weights (normalized)\n",
    "    \n",
    "    params = tds[r][0].astype(np.float32)\n",
    "\n",
    "    if r > 0:\n",
    "        p_prior = g.prior.eval(params,log=False)\n",
    "        p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "        iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "    else:\n",
    "        iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "    \n",
    "    #iws = (tds[r][2].reshape(-1)).astype(np.float32) # weights returned by SNPE are renormalized by default now\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (prediction-target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iiws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    idx = np.argsort(iws)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,3,1)\n",
    "    \n",
    "    plt.plot(train_errs)\n",
    "    plt.title('training error')\n",
    "    plt.xlabel('gradient step')\n",
    "    \n",
    "    #plt.semilogy(iiws[idx])\n",
    "    #plt.semilogy(pred[idx])\n",
    "    #plt.legend(['inv. weights', 'kernel'])\n",
    "    #plt.title('kernel should track inverse weights')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(1./pred[idx])\n",
    "    plt.semilogy(iws[idx], linewidth=2.5)\n",
    "    plt.legend(['inv. kernel', 'weights'])\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    plt.title('inv. kernel should track weights')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.semilogy(pred[idx]*iws[idx])\n",
    "    plt.semilogy(iws[idx], linewidth=2.5)\n",
    "    plt.legend(['kernel*weights', 'weights'])\n",
    "    plt.title('kernel*weights should be flatter than weights')\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('loss with/without learned kernels', np.mean((iws[idx]-1)**2), np.mean((iws[idx]-1/pred[idx])**2))\n",
    "    print('mean and std of raw importance weights', np.mean(iws), np.std(iws))    \n",
    "    print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "    w = (iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x = np.sum( w * (tds[r][1]-mu_x)**2, axis=0)\n",
    "    mu_x, sig2_x\n",
    "\n",
    "    w = (pred*iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x_K =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x_K = np.sum( w * (tds[r][1]-mu_x_K)**2, axis=0)\n",
    "    mu_x_K, sig2_x_K\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tds[r][1].mean(axis=0), 'ko-')\n",
    "    plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "    plt.plot(mu_x.flatten(), 'ro-')\n",
    "    plt.plot(obs_stats.flatten(), 'go--')\n",
    "    plt.legend(['raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data means')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(tds[r][1].var(axis=0), 'ko-')\n",
    "    plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "    plt.plot(sig2_x.flatten(), 'ro-')\n",
    "    plt.legend(['raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data variances')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pick one for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in range(1, 2):\n",
    "    \n",
    "    print('round #' + str(r))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # x - x0\n",
    "    dx = (tds[r][1] - obs_stats).astype(np.float32)\n",
    "    # weights (normalized)\n",
    "    \n",
    "    params = tds[r][0].astype(np.float32)\n",
    "\n",
    "    if r > 0:\n",
    "        p_prior = g.prior.eval(params,log=False)\n",
    "        p_proposal = posteriors[r-1].eval(params,log=False)\n",
    "        iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "    else:\n",
    "        iws = np.ones(params.shape[0],dtype=np.float32)\n",
    "    \n",
    "    #iws = (tds[r][2].reshape(-1)).astype(np.float32) # weights returned by SNPE are renormalized by default now\n",
    "    \n",
    "    # inverse weights (capturing zero weigths)\n",
    "    iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "    l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "    prediction = lasagne.layers.get_output(l_dot)\n",
    "    loss = (prediction-target_var)**2\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "    updates = lasagne.updates.adam(\n",
    "                loss, params, learning_rate=0.001)\n",
    "    train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                                on_unused_input='ignore')\n",
    "    \n",
    "    print('initial kernel A:', l_dot.B.get_value())\n",
    "    train_errs = np.zeros(20000)\n",
    "    for i in range(train_errs.size):\n",
    "        train_errs[i], pred = train_fn(dx, iiws)\n",
    "    print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "    idx = np.argsort(iws)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,3,1)\n",
    "    \n",
    "    plt.plot(train_errs)\n",
    "    plt.title('training error')\n",
    "    plt.xlabel('gradient step')\n",
    "    \n",
    "    #plt.semilogy(iiws[idx])\n",
    "    #plt.semilogy(pred[idx])\n",
    "    #plt.legend(['inv. weights', 'kernel'])\n",
    "    #plt.title('kernel should track inverse weights')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.semilogy(1./pred[idx])\n",
    "    plt.semilogy(iws[idx], linewidth=2.5)\n",
    "    plt.legend(['inv. kernel', 'weights'])\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    plt.title('inv. kernel should track weights')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.semilogy(pred[idx]*iws[idx])\n",
    "    plt.semilogy(iws[idx], linewidth=2.5)\n",
    "    plt.legend(['kernel*weights', 'weights'])\n",
    "    plt.title('kernel*weights should be flatter than weights')\n",
    "    plt.xlabel('n (sorted by importance weight value)')\n",
    "    \n",
    "    plt.savefig('kernel_learning_GLM_example_r1_inverseWeightedLoss_gaussianProposals.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    print('loss with/without learned kernels', np.mean((iws[idx]-1)**2), np.mean((iws[idx]-1/pred[idx])**2))\n",
    "    print('mean and std of raw importance weights', np.mean(iws), np.std(iws))    \n",
    "    print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "    w = (iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x = np.sum( w * (tds[r][1]-mu_x)**2, axis=0)\n",
    "    mu_x, sig2_x\n",
    "\n",
    "    w = (pred*iws).reshape(-1,1)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mu_x_K =   np.sum( w * tds[r][1],           axis=0).reshape(1,-1)\n",
    "    sig2_x_K = np.sum( w * (tds[r][1]-mu_x_K)**2, axis=0)\n",
    "    mu_x_K, sig2_x_K\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tds[r][1].mean(axis=0), 'ko-')\n",
    "    plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "    plt.plot(mu_x.flatten(), 'ro-')\n",
    "    plt.plot(obs_stats.flatten(), 'go--')\n",
    "    plt.legend(['raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data means')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(tds[r][1].var(axis=0), 'ko-')\n",
    "    plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "    plt.plot(sig2_x.flatten(), 'ro-')\n",
    "    plt.legend(['raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.title('raw and importance sampled data variances')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sanity check: draw new data not returned from SNPE logs\n",
    "- might be overlooking something in the SNPE code and how it handles (Z-scores, shifts, stores...) it's logged datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "g.proposal = posteriors[0]\n",
    "\n",
    "th, x = g.gen(5000)\n",
    "\n",
    "# x - x0\n",
    "dx = (x - obs_stats).astype(np.float32)\n",
    "# weights (normalized)\n",
    "params = th.astype(np.float32)\n",
    "\n",
    "p_prior = g.prior.eval(params,log=False)\n",
    "p_proposal = g.proposal.eval(params,log=False)\n",
    "iws = ( p_prior / p_proposal ).astype(np.float32)\n",
    "\n",
    "#iws = (tds[r][2].reshape(-1)).astype(np.float32) # weights returned by SNPE are renormalized by default now\n",
    "\n",
    "# inverse weights (capturing zero weigths)\n",
    "iiws = np.minimum((1./iws).astype(np.float32), 1e20*np.ones_like(iws))\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, ndim_x),input_var=input_var)\n",
    "l_dot = KernelLayer_offset(l_in, name='kernel_layer')\n",
    "prediction = lasagne.layers.get_output(l_dot)\n",
    "loss = (1 - prediction*target_var)**2\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(l_dot, trainable=True)\n",
    "updates = lasagne.updates.adam(\n",
    "            loss, params, learning_rate=0.001)\n",
    "train_fn = theano.function([input_var, target_var], [loss, prediction], updates=updates,\n",
    "                            on_unused_input='ignore')\n",
    "\n",
    "print('initial kernel A:', l_dot.B.get_value())\n",
    "train_errs = np.zeros(20000)\n",
    "for i in range(train_errs.size):\n",
    "    train_errs[i], pred = train_fn(dx, iws)\n",
    "print('learned kernel A:', l_dot.B.get_value())\n",
    "\n",
    "idx = np.argsort(iws)\n",
    "plt.plot(train_errs)\n",
    "plt.title('training error')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "krnl  = pred[idx]\n",
    "ikrnl = 1./pred[idx]\n",
    "plt.subplot(1,3,1)\n",
    "plt.semilogy(iiws[idx])\n",
    "plt.semilogy(krnl)\n",
    "plt.legend(['inv. weights', 'kernel'])\n",
    "plt.title('kernel should track inverse weights')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.semilogy(iws[idx])\n",
    "plt.semilogy(ikrnl)\n",
    "plt.legend(['weights', 'inv. kernel'])\n",
    "plt.xlabel('n (sorted by importance weight value)')\n",
    "plt.title('inv. kernel should track weights')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.semilogy(iws[idx])\n",
    "plt.semilogy(krnl*iws[idx])\n",
    "plt.legend(['weights', 'kernel*weights'])\n",
    "plt.title('kernel*weights should be flatter than weights')\n",
    "plt.show()\n",
    "\n",
    "print('loss with / without learned kernels', np.mean((iws[idx]-1)**2), np.mean((pred[idx]*iws[idx]-1)**2))\n",
    "print('mean and std of raw importance weights', np.mean(iws), np.std(iws))\n",
    "print('mean and std of kernel-weighted importance weights', np.mean(iws*pred), np.std(iws*pred))   \n",
    "\n",
    "\n",
    "w = (iws).reshape(-1,1)\n",
    "w = w / np.sum(w)\n",
    "\n",
    "mu_x =   np.sum( w *  x,           axis=0).reshape(1,-1)\n",
    "sig2_x = np.sum( w * (x-mu_x)**2, axis=0)\n",
    "mu_x, sig2_x\n",
    "\n",
    "w = (pred*iws).reshape(-1,1)\n",
    "w = w / np.sum(w)\n",
    "\n",
    "mu_x_K =   np.sum( w *  x,           axis=0).reshape(1,-1)\n",
    "sig2_x_K = np.sum( w * (x-mu_x_K)**2, axis=0)\n",
    "mu_x_K, sig2_x_K\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(x_prior,axis=0), 'co--')\n",
    "plt.plot(x.mean(axis=0), 'ko-')\n",
    "plt.plot(mu_x_K.flatten(), 'bo-')\n",
    "plt.plot(mu_x.flatten(), 'ro-')\n",
    "plt.plot(obs_stats.flatten(), 'go--')\n",
    "plt.legend(['E[x]', 'raw E[x]', 'kernel-weighted E_K[x]','pure IS E_IS[x]', 'real x_0'])\n",
    "plt.xlabel('# summary statistic')\n",
    "plt.title('raw and importance sampled data means')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.var(x_prior,axis=0), 'co--')\n",
    "plt.plot(th.var(axis=0), 'ko-')\n",
    "plt.plot(sig2_x_K.flatten(), 'bo-')\n",
    "plt.plot(sig2_x.flatten(), 'ro-')\n",
    "plt.legend(['Var[x]', 'raw Var[x]', 'kernel-weighted Var_K[x]','pure IS Var_IS[x]'])\n",
    "plt.xlabel('# summary statistic')\n",
    "plt.title('raw and importance sampled data variances')\n",
    "\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "import seaborn\n",
    "\n",
    "B = l_dot.B.get_value()\n",
    "A_base=  np.diag(B**2)\n",
    "\n",
    "Z = l_dot.Z.get_value()\n",
    "\n",
    "def A_(lambd=None, i=0):\n",
    "    A = A_base.copy()\n",
    "    if not lambd is None:\n",
    "        A[i,i]= lambd\n",
    "    return A\n",
    "def K(dx, A, Z):\n",
    "    return  np.exp(- np.sum(dx.dot(A)* dx,axis=1) + Z )\n",
    "\n",
    "def loss_map(lambd, i):\n",
    "    return np.mean( (1 - iws*K(dx, A_(lambd,i), Z))**2 )\n",
    "\n",
    "\n",
    "lambds = np.exp(np.log(10) * np.linspace(-3, 3, 1000))\n",
    "l      = np.zeros_like(lambds)              \n",
    "plt.figure(figsize=(8,16))\n",
    "for i in range(10):    \n",
    "    plt.subplot(5,2,i+1)\n",
    "    for j in range(len(lambds)):\n",
    "        l[j] = loss_map(lambds[j], i)\n",
    "    plt.semilogx(lambds, l)\n",
    "    if A_base[i,i]>np.min(lambds):\n",
    "        plt.semilogx([A_base[i,i], A_base[i,i]], [1.,1.], 'r*', markersize=10)        \n",
    "    if i == 0:\n",
    "        plt.legend(['loss', 'found optimum'], loc=4)\n",
    "    if i > 7:\n",
    "        plt.xlabel('1/sig^2')\n",
    "    if np.mod(i,2) ==0:\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "#plt.savefig('kernel_learning_GLM_example_basicLoss_offsetKernel_gaussianProposals_coordinatewiseErrorFunction.pdf')        \n",
    "plt.show()\n",
    "\n",
    "lss - train_errs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
