{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning kernels for the GLM example. \n",
    "\n",
    "we optimize kernels such that\n",
    "$ K(x_n, x_0) p(\\theta_n) / \\tilde{p}(\\theta_n) \\approx 1$. \n",
    "\n",
    "Spoiler:\n",
    "starts to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approach\n",
    "\n",
    "The above problem doesn't require MDNs at all. \n",
    "Once prior, proposal, kernel and simulator are fixed and we drew an artificial dataset $(x_n, \\theta_n)$, we're good to play. \n",
    "Let's run SNPE as usual, note down the data-sets $(x_n, \\theta_n)$, proposal priors and importance weights it produced over rounds, and afterwards play with the kernel on those fixed targets. \n",
    "\n",
    "- Remark: results look a lot worse if we convert to Students-t distributions. Could be that kernel shape (squared-exponential in $x$) has to match proposal-prior shape (squared in $\\theta$ for students-T with df=3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. basic squared loss\n",
    "\n",
    "argmin $ \\sum_n \\left( 1 - \\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $, emphasizing the absolute value of $\\approx 1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.glm.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfimodels.glm.GLM import GLM\n",
    "from lfimodels.glm.GLMStats import GLMStats\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seeds = np.arange(90, 110)\n",
    "duration = 100\n",
    "\n",
    "for seed in seeds:\n",
    "    true_params, labels_params = utils.obs_params()\n",
    "    obs = utils.obs_data(true_params, seed=seed, duration = duration)\n",
    "    obs_stats = utils.obs_stats(true_params, seed=seed, duration = duration)\n",
    "\n",
    "    rerun = True  # if False, will try loading file from disk\n",
    "\n",
    "    try:\n",
    "        assert rerun == False, 'rerun requested'\n",
    "        sam = np.load('sam_' + str(duration) + '_' + str(seed) + '.npz')['arr_0']\n",
    "    except:\n",
    "        sam = utils.pg_mcmc(true_params, obs)\n",
    "        np.savez('sam_' + str(duration) + '_' + str(seed) + '.npz', sam)\n",
    "\n",
    "    n_train = 5000\n",
    "    n_rounds = 10\n",
    "    minibatch = 100\n",
    "    epochs = 500\n",
    "    round_cl = 999\n",
    "\n",
    "    n_hiddens=[50] \n",
    "    convert_to_T=None \n",
    "    pilot_samples=0\n",
    "    svi=True\n",
    "    reg_lambda=0.01\n",
    "    prior_norm=False\n",
    "    \n",
    "    \n",
    "    \n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "    res = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs, tds, posteriors = res.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss=None)\n",
    "\n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "    res_k = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs_k, tds_k, posteriors_k = res_k.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss='x_kl')\n",
    "\n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "    res_k2 = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs_k2, tds_k2, posteriors_k2 = res_k2.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss='basic')\n",
    "    \n",
    "    np.save('check_kernels_d' + str(duration) + '_' + str(seed), \n",
    "            {'seed': seed,\n",
    "             'duration' : duration, \n",
    "             'n_train' : n_train,\n",
    "             'n_rounds' : n_rounds,\n",
    "             'minibatch' : minibatch,\n",
    "             'epochs' : minibatch,\n",
    "\n",
    "             'n_hiddens' : [50], \n",
    "             'convert_to_T' : None, \n",
    "             'pilot_samples' : 0,\n",
    "             'svi' : True,\n",
    "             'reg_lambda': 0.01,\n",
    "             'prior_norm':False,             \n",
    "             'round_cl' : 999, \n",
    "             \n",
    "             'obs_stats' : obs_stats,\n",
    "             'true_params' : true_params,\n",
    "\n",
    "             'logs' : logs, \n",
    "             'logs_k' : logs_k, \n",
    "             'logs_k2' : logs_k2, \n",
    "             'tds' : tds,\n",
    "             'tds_k' : tds_k,\n",
    "             'tds_k2' : tds_k2,\n",
    "             'posteriors' : posteriors,\n",
    "             'posteriors_k' : posteriors_k,\n",
    "             'posteriors_k2' : posteriors_k2\n",
    "\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run with Gaussian proposals\n",
    "for r in range(len(tds_k)):\n",
    "    plot_pdf(posteriors[r],\n",
    "             pdf2=posteriors_k[r], \n",
    "             lims=[-2,2], \n",
    "             samples=sam, \n",
    "             gt=true_params, \n",
    "             figsize=(9,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run with Gaussian proposals\n",
    "for r in range(len(tds_k)):\n",
    "    plot_pdf(posteriors[r],\n",
    "             pdf2=posteriors_k2[r], \n",
    "             lims=[-2,2], \n",
    "             samples=sam, \n",
    "             gt=true_params, \n",
    "             figsize=(9,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
