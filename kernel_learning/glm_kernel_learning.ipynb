{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning kernels for the GLM example. \n",
    "\n",
    "we optimize kernels such that\n",
    "$ K(x_n, x_0) p(\\theta_n) / \\tilde{p}(\\theta_n) \\approx 1$. \n",
    "\n",
    "Spoiler:\n",
    "starts to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approach\n",
    "\n",
    "The above problem doesn't require MDNs at all. \n",
    "Once prior, proposal, kernel and simulator are fixed and we drew an artificial dataset $(x_n, \\theta_n)$, we're good to play. \n",
    "Let's run SNPE as usual, note down the data-sets $(x_n, \\theta_n)$, proposal priors and importance weights it produced over rounds, and afterwards play with the kernel on those fixed targets. \n",
    "\n",
    "- Remark: results look a lot worse if we convert to Students-t distributions. Could be that kernel shape (squared-exponential in $x$) has to match proposal-prior shape (squared in $\\theta$ for students-T with df=3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## to do:\n",
    "- nonlinear kernels: $k(x, x_0) \\propto \\exp - \\frac{1}{2} || F(x) - F(x_0) ||^2$ \n",
    "- full (correlated) covariance matrices\n",
    "- in fact, run full covariance matrices as $F(x) = Ax$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try out a bunch of simple squared losses. \n",
    "\n",
    "### 1. basic squared loss\n",
    "\n",
    "argmin $ \\sum_n \\left( 1 - \\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $\n",
    "\n",
    "### 2. locality-preserving loss\n",
    "\n",
    "argmin $ D_{KL}(\\tilde{p}(x) || p_K(x) ) \\approx $ argmin $\\log \\frac{1}{N} \\sum_n \\frac{p(\\theta_n)}{\\tilde{p}(\\theta_n)} K(x_n, x_0) - \\frac{1}{N} \\sum_n \\log K(x_n, x_0) $"
=======
    "\n",
    "### 1. basic squared loss\n",
    "\n",
    "argmin $ \\sum_n \\left( 1 - \\frac{K(x_n, x_0) p(\\theta_n)}{\\tilde{p}(\\theta_n)} \\right)^2 $, emphasizing the absolute value of $\\approx 1$. \n"
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
=======
    "collapsed": false,
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.glm.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfimodels.glm.GLM import GLM\n",
    "from lfimodels.glm.GLMStats import GLMStats\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
<<<<<<< HEAD
    "seed = 42\n",
    "\n",
    "duration=500\n",
    "\n",
    "true_params, labels_params = utils.obs_params()\n",
    "obs = utils.obs_data(true_params, seed=seed, duration=duration)\n",
    "obs_stats = utils.obs_stats(true_params, seed=seed, duration=duration)\n",
    "\n",
    "rerun = False  # if False, will try loading file from disk\n",
    "\n",
    "try:\n",
    "    assert rerun == False, 'rerun requested'\n",
    "    sam = np.load('sam.npz')['arr_0']\n",
    "except:\n",
    "    sam = utils.pg_mcmc(true_params, obs, duration=duration)\n",
    "    np.savez('sam.npz', sam)\n",
    "\n",
    "n_train=3000 \n",
    "n_rounds=5\n",
    "minibatch=100 \n",
    "epochs=500   \n",
    "round_cl=999\n",
    "\n",
    "n_hiddens = [50, 50]\n",
    "convert_to_T = None\n",
    "pilot_samples=1000\n",
    "svi=True\n",
    "reg_lambda=0.01\n",
    "prior_norm=False\n",
    "    \n",
    "m = GLM(duration=duration, seed=seed)\n",
    "p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "s = GLMStats(n_summary=m.n_params)\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res = infer.SNPE(g, \n",
    "                 obs=obs_stats, \n",
    "                 n_hiddens=n_hiddens, \n",
    "                 seed=seed, \n",
    "                 convert_to_T=convert_to_T, \n",
    "                 pilot_samples=pilot_samples,\n",
    "                 svi=svi,\n",
    "                 reg_lambda=reg_lambda,\n",
    "                 prior_norm=prior_norm)\n",
    "\n",
    "logs, tds, posteriors = res.run(n_train=n_train, \n",
    "                                n_rounds=n_rounds, \n",
    "                                minibatch=minibatch, \n",
    "                                epochs=epochs, \n",
    "                                round_cl=round_cl, \n",
    "                                kernel_loss=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version with learned kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GLM(duration=duration, seed=seed)\n",
    "p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "s = GLMStats(n_summary=m.n_params)\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res_k = infer.SNPE(g, \n",
    "                 obs=obs_stats, \n",
    "                 n_hiddens=n_hiddens, \n",
    "                 seed=seed, \n",
    "                 convert_to_T=convert_to_T, \n",
    "                 pilot_samples=pilot_samples,\n",
    "                 svi=svi,\n",
    "                 reg_lambda=reg_lambda,\n",
    "                 prior_norm=prior_norm)\n",
    "\n",
    "logs_k, tds_k, posteriors_k = res_k.run(n_train=n_train, \n",
    "                                n_rounds=n_rounds, \n",
    "                                minibatch=minibatch, \n",
    "                                epochs=epochs, \n",
    "                                round_cl=round_cl, \n",
    "                                kernel_loss='x_kl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GLM(duration=duration, seed=seed)\n",
    "p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "s = GLMStats(n_summary=m.n_params)\n",
    "g = dg.Default(model=m, prior=p, summary=s)\n",
    "res_k2 = infer.SNPE(g, \n",
    "                 obs=obs_stats, \n",
    "                 n_hiddens=n_hiddens, \n",
    "                 seed=seed, \n",
    "                 convert_to_T=convert_to_T, \n",
    "                 pilot_samples=pilot_samples,\n",
    "                 svi=svi,\n",
    "                 reg_lambda=reg_lambda,\n",
    "                 prior_norm=prior_norm)\n",
    "\n",
    "logs_k2, tds_k2, posteriors_k2 = res_k2.run(n_train=n_train, \n",
    "                                n_rounds=n_rounds, \n",
    "                                minibatch=minibatch, \n",
    "                                epochs=epochs, \n",
    "                                round_cl=round_cl, \n",
    "                                kernel_loss='ess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(n_rounds):\n",
    "    plt.plot(logs[r]['loss'])\n",
    "    plt.plot(logs_k[r]['loss'])\n",
    "    plt.plot(logs_k2[r]['loss'])\n",
    "    plt.legend(['raw', 'x_kl', 'max_ess'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "\n",
    "sets = [tds, tds_k, tds_k2]\n",
    "\n",
    "ess   = np.zeros((len(tds), 3))\n",
    "mu_   = np.zeros((len(tds), 3, obs_stats.size)) \n",
    "sig2_ = np.zeros((len(tds), 3, obs_stats.size))\n",
    "sig2e = np.zeros((len(tds), 3, obs_stats.size))\n",
    "\n",
    "for r in range(len(tds)):\n",
    "    for i in range(3):\n",
    "        w = sets[i][r][2].reshape(-1,1)\n",
    "        w /= w.sum()\n",
    "        ess[r, i] = 1./np.sum(w**2)\n",
    "\n",
    "        stats = sets[i][r][1]\n",
    "        sig2e[r, i, :] = np.var( stats, axis = 0)\n",
    "        mu_[r, i, :]   = np.sum( w * stats,                   axis=0).reshape(1,-1)\n",
    "        sig2_[r, i, :] = np.sum( w * (stats-mu_[r, i, :])**2, axis=0)    \n",
    "            \n",
    "plt.plot(np.arange(1, len(tds)), ess[1:,:])\n",
    "plt.legend(['raw iws', 'iws + x_kl', 'iws + max_ess', 'unweighted'])\n",
    "plt.xlabel('round')\n",
    "plt.ylabel('effective sample size')\n",
    "plt.show()    \n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(12,16))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.plot(np.arange(0,len(tds)), sig2e[:, :, i], '--')\n",
    "    plt.plot(np.arange(0,len(tds)), sig2_[:,:,i], linewidth=1.5)    \n",
    "    plt.legend(['raw', 'x_kl', 'max_ess'])\n",
    "plt.show()    \n",
    "\"\"\"\n",
    "plt.figure(figsize=(12,16))\n",
    "for r in range(1,len(tds)):\n",
    "    plt.subplot(2,2,r)\n",
    "    plt.plot(sig2_[r,:,:].T, linewidth=1.5)    \n",
    "    plt.plot(sig2e[r, 1, :].T, 'k--')\n",
    "    plt.xlabel('# summary statistic')\n",
    "    plt.ylabel('empirical variance')\n",
    "    plt.title('SNPE round #' + str(r+1))\n",
    "    plt.legend(['raw iws', 'iws + x_kl', 'iws + max_ess', 'unweighted'])\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rerun = True  # if False, will try loading file from disk\n",
    "\n",
    "try:\n",
    "    assert rerun == False, 'rerun requested'\n",
    "    sam = np.load('sam.npz')['arr_0']\n",
    "except:\n",
    "    sam = utils.pg_mcmc(true_params, obs, duration=duration)\n",
    "    np.savez('sam.npz', sam)\n"
=======
    "seeds = np.arange(90, 110)\n",
    "duration = 100\n",
    "\n",
    "for seed in seeds:\n",
    "    true_params, labels_params = utils.obs_params()\n",
    "    obs = utils.obs_data(true_params, seed=seed, duration = duration)\n",
    "    obs_stats = utils.obs_stats(true_params, seed=seed, duration = duration)\n",
    "\n",
    "    rerun = True  # if False, will try loading file from disk\n",
    "\n",
    "    try:\n",
    "        assert rerun == False, 'rerun requested'\n",
    "        sam = np.load('sam_' + str(duration) + '_' + str(seed) + '.npz')['arr_0']\n",
    "    except:\n",
    "        sam = utils.pg_mcmc(true_params, obs)\n",
    "        np.savez('sam_' + str(duration) + '_' + str(seed) + '.npz', sam)\n",
    "\n",
    "    n_train = 5000\n",
    "    n_rounds = 10\n",
    "    minibatch = 100\n",
    "    epochs = 500\n",
    "    round_cl = 999\n",
    "\n",
    "    n_hiddens=[50] \n",
    "    convert_to_T=None \n",
    "    pilot_samples=0\n",
    "    svi=True\n",
    "    reg_lambda=0.01\n",
    "    prior_norm=False\n",
    "    \n",
    "    \n",
    "    \n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "    res = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs, tds, posteriors = res.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss=None)\n",
    "\n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "    res_k = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs_k, tds_k, posteriors_k = res_k.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss='x_kl')\n",
    "\n",
    "    m = GLM(seed=seed, duration = duration)\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    g = dg.Default(model=m, prior=p, summary=s)\n",
    "    res_k2 = infer.SNPE(g, \n",
    "                     obs=obs_stats, \n",
    "                     n_hiddens=n_hiddens, \n",
    "                     seed=seed, \n",
    "                     convert_to_T=convert_to_T, \n",
    "                     pilot_samples=pilot_samples,\n",
    "                     svi=svi,\n",
    "                     reg_lambda=reg_lambda,\n",
    "                     prior_norm=prior_norm)\n",
    "\n",
    "    logs_k2, tds_k2, posteriors_k2 = res_k2.run(n_train=n_train, \n",
    "                                    n_rounds=n_rounds, \n",
    "                                    minibatch=minibatch, \n",
    "                                    epochs=epochs, \n",
    "                                    round_cl=round_cl, \n",
    "                                    kernel_loss='basic')\n",
    "    \n",
    "    np.save('check_kernels_d' + str(duration) + '_' + str(seed), \n",
    "            {'seed': seed,\n",
    "             'duration' : duration, \n",
    "             'n_train' : n_train,\n",
    "             'n_rounds' : n_rounds,\n",
    "             'minibatch' : minibatch,\n",
    "             'epochs' : minibatch,\n",
    "\n",
    "             'n_hiddens' : [50], \n",
    "             'convert_to_T' : None, \n",
    "             'pilot_samples' : 0,\n",
    "             'svi' : True,\n",
    "             'reg_lambda': 0.01,\n",
    "             'prior_norm':False,             \n",
    "             'round_cl' : 999, \n",
    "             \n",
    "             'obs_stats' : obs_stats,\n",
    "             'true_params' : true_params,\n",
    "\n",
    "             'logs' : logs, \n",
    "             'logs_k' : logs_k, \n",
    "             'logs_k2' : logs_k2, \n",
    "             'tds' : tds,\n",
    "             'tds_k' : tds_k,\n",
    "             'tds_k2' : tds_k2,\n",
    "             'posteriors' : posteriors,\n",
    "             'posteriors_k' : posteriors_k,\n",
    "             'posteriors_k2' : posteriors_k2\n",
    "\n",
    "             })"
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in range(len(tds)):\n",
    "    plot_pdf(posteriors[r], \n",
    "             pdf2=posteriors_k[r],\n",
=======
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run with Gaussian proposals\n",
    "for r in range(len(tds_k)):\n",
    "    plot_pdf(posteriors[r],\n",
    "             pdf2=posteriors_k[r], \n",
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
    "             lims=[-2,2], \n",
    "             samples=sam, \n",
    "             gt=true_params, \n",
    "             figsize=(9,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in range(len(tds)):\n",
    "    plot_pdf(posteriors[r], \n",
    "             pdf2=posteriors_k2[r],\n",
=======
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run with Gaussian proposals\n",
    "for r in range(len(tds_k)):\n",
    "    plot_pdf(posteriors[r],\n",
    "             pdf2=posteriors_k2[r], \n",
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
    "             lims=[-2,2], \n",
    "             samples=sam, \n",
    "             gt=true_params, \n",
    "             figsize=(9,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "res.network.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.stats_std, res_k.stats_std, res_k2.stats_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import lasagne.layers as ll\n",
    "\n",
    "network = res.network\n",
    "inputs  = tds[0][1]\n",
    "obs_z   = (obs_stats - res.stats_mean) / res.stats_std\n",
    "\n",
    "ks = list(network.layer.keys())\n",
    "hiddens = np.where([i[:6]=='hidden' for i in ks])[0]\n",
    "layer_index = hiddens[0]\n",
    "hl = network.layer[ks[layer_index]]\n",
    "print(hl.name)\n",
    "\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=ll.get_output(hl))\n",
    "\n",
    "plt.plot(f_eval(inputs)[:, :].T, 'k.')\n",
    "plt.plot(f_eval(obs_z).reshape(-1), 'ro')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "layer_index = hiddens[-1]\n",
    "hl = network.layer[ks[layer_index]]\n",
    "print(hl.name)\n",
    "\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=ll.get_output(hl))\n",
    "\n",
    "plt.plot(f_eval(inputs)[:, :].T, 'k.')\n",
    "plt.plot(f_eval(obs_z).reshape(-1), 'ro')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(network.layer['mixture_means'])\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=network.dms)\n",
    "plt.plot(f_eval(inputs)[0].T, 'k.')\n",
    "plt.plot(f_eval( obs_z)[0].T, 'ro')\n",
    "plt.plot(true_params, 'bx', ms=5)\n",
    "plt.show()\n",
    "\n",
    "print(f_eval( obs_z)[0])\n",
    "print(posteriors[-1].xs[0].m)\n",
    "print(true_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import lasagne.layers as ll\n",
    "\n",
    "network = res.network\n",
    "inputs  = tds[-1][1]\n",
    "obs_z   = (obs_stats - res.stats_mean) / res.stats_std\n",
    "\n",
    "ks = list(network.layer.keys())\n",
    "hiddens = np.where([i[:6]=='hidden' for i in ks])[0]\n",
    "layer_index = hiddens[0]\n",
    "hl = network.layer[ks[layer_index]]\n",
    "print(hl.name)\n",
    "\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=ll.get_output(hl))\n",
    "\n",
    "plt.plot(f_eval(inputs)[:, :].T, 'k.')\n",
    "plt.plot(f_eval(obs_z).reshape(-1), 'ro')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "layer_index = hiddens[-1]\n",
    "hl = network.layer[ks[layer_index]]\n",
    "print(hl.name)\n",
    "\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=ll.get_output(hl))\n",
    "\n",
    "plt.plot(f_eval(inputs)[:, :].T, 'k.')\n",
    "plt.plot(f_eval(obs_z).reshape(-1), 'ro')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(network.layer['mixture_means'])\n",
    "f_eval = theano.function(\n",
    "    inputs=[network.stats],\n",
    "    outputs=network.dms)\n",
    "plt.plot(f_eval(inputs)[0].T, 'k.')\n",
    "plt.plot(f_eval( obs_z)[0].T, 'ro')\n",
    "plt.plot(true_params, 'bx', ms=5)\n",
    "plt.show()\n",
    "\n",
    "print(f_eval( obs_z)[0])\n",
    "print(posteriors[-1].xs[0].m)\n",
    "print(true_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (posteriors_k[-1].xs[0].m - sam.mean(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (posteriors_k2[-1].xs[0].m - sam.mean(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (posteriors[-1].xs[0].m - sam.mean(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (np.diag(posteriors[-1].xs[0].S) - sam.var(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (np.diag(posteriors_k[-1].xs[0].S) - sam.var(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( np.sum( (np.diag(posteriors_k2[-1].xs[0].S) - sam.var(axis=1))**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1,n_rounds):\n",
    "    plt.plot(logs_k2[r]['cbkrnl'].A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1,n_rounds):\n",
    "    plt.plot(logs_k[r]['cbkrnl'].A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
   "metadata": {
    "collapsed": true
   },
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "celltoolbar": "Edit Metadata",
=======
>>>>>>> 35845e14560c7e606a3cb4f2e3d6e2201dd878c1
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
