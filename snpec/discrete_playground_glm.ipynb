{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM\n",
    "\n",
    "\n",
    "- GLM example as simple multivariate posterior estimation problem with available ground-truth (MCMC)\n",
    "- setup and code from NIPS 2017 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "import delfi.summarystats as ds\n",
    "import lfimodels.glm.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfimodels.glm.GLM import GLM\n",
    "from lfimodels.glm.GLMStats import GLMStats\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# load setup from NIPS 2017 paper\n",
    "\n",
    "len_filter = 9 # number of GLM filter parameters (= dim. of parameters)\n",
    "duration = 100 # simulation length (longer = tighter posteriors)\n",
    "\n",
    "true_params, labels_params = utils.obs_params(len_filter)\n",
    "obs = utils.obs_data(true_params, seed=seed, duration=duration)\n",
    "obs_stats = utils.obs_stats(true_params, seed=seed)\n",
    "\n",
    "# basic approach to controlling generator seeds\n",
    "def init_g(seed):\n",
    "    m = GLM(seed=seed, duration=duration, len_filter=len_filter)\n",
    "    #p = dd.Uniform(lower=[-3,-3,-3,-3,-3,-3,-3,-3,-3,-3], upper=[3,3,3,3,3,3,3,3,3,3])\n",
    "    p = utils.smoothing_prior(n_params=m.n_params, seed=seed)\n",
    "    s = GLMStats(n_summary=m.n_params)\n",
    "    return dg.Default(model=m, prior=p, summary=s)\n",
    "\n",
    "# MCMC comparison (this might take a while the first time !)\n",
    "rerun = False  # if False, will try loading file from disk\n",
    "try:\n",
    "    assert rerun == False, 'rerun requested'\n",
    "    sam = np.load('sam.npz')['arr_0']\n",
    "except:\n",
    "    sam = utils.pg_mcmc(true_params, obs)\n",
    "    np.savez('sam.npz', sam)\n",
    "    \n",
    "\n",
    "# SNPE parameters\n",
    "\n",
    "# training schedule\n",
    "n_train=1000\n",
    "n_rounds=5\n",
    "\n",
    "# fitting setup\n",
    "minibatch=100\n",
    "epochs=1000\n",
    "\n",
    "# network setup\n",
    "n_hiddens=[10,10]\n",
    "reg_lambda=0.01\n",
    "\n",
    "# convenience\n",
    "pilot_samples=1000\n",
    "svi=False\n",
    "verbose=True\n",
    "prior_norm=False\n",
    "\n",
    "# SNPE-C parameters\n",
    "n_null = 1\n",
    "\n",
    "# MAF parameters\n",
    "mode='random' # ordering of variables for MADEs\n",
    "n_mades = 2 # number of MADES\n",
    "act_fun = 'tanh'\n",
    "batch_norm = False # batch-normalization currently not supported\n",
    "train_on_all = False # now supported feature\n",
    "\n",
    "# MDN parameters\n",
    "n_components = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline: SNPE-A\n",
    "\n",
    "- SNPE-A hard to beat on this problem with Gaussian prior and Gaussian $q^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = init_g(seed=seed)\n",
    "\n",
    "res = infer.CDELFI(g, \n",
    "                 obs=obs_stats, \n",
    "                 n_hiddens=n_hiddens, \n",
    "                 seed=seed, \n",
    "                 reg_lambda=reg_lambda,\n",
    "                 pilot_samples=pilot_samples,\n",
    "                 svi=svi,\n",
    "                 n_components=n_components,\n",
    "                 verbose=verbose,\n",
    "                 prior_norm=prior_norm)\n",
    "\n",
    "t = timeit.time.time()\n",
    "\n",
    "logs_A, tds_A, posteriors_A = res.run(n_train=n_train, \n",
    "                    n_rounds=5, \n",
    "                    minibatch=minibatch, \n",
    "                    epochs=epochs)\n",
    "\n",
    "print(timeit.time.time() -  t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick look at problem setup, posterior vs. prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,_ = plot_pdf(posteriors_A[-1], \n",
    "         #pdf2=g.prior,\n",
    "         samples=sam, \n",
    "         gt=true_params, \n",
    "         resolution=100,\n",
    "         ticks=True,\n",
    "         figsize=(16,16));\n",
    "fig.suptitle('final posterior estimate vs MCMC samples and prior', fontsize=14)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNPE-C\n",
    "\n",
    "- version with rounds: first round is SNPE-A, then \n",
    "\n",
    "    - after every round, set $\\tilde{p}(\\theta) = q^*(\\theta|x_0)$\n",
    "    \n",
    "    - sample synthetic data set $\\mathcal{D} = \\{(\\theta_n,x_n)\\}_{n=1}^N$  for this round, with $(\\theta_n, x_n) \\sim p(x|\\theta)\\tilde{p}(\\theta)$\n",
    "    \n",
    "    - for every gradient step, sample alternatives $\\theta'_{nj}, j = 1 \\ldots, n_{null}$ from $\\theta'_{nj}$ depending on chosen rule: \n",
    "        - moo='resample' : $\\ \\theta'_{nj}\\sim Unif[\\{\\theta_m\\}_{n\\neq{}m}]$, i.e. $\\theta'_{nj}$ are resampled (without replacement) from the $\\theta_n\\sim \\tilde{p}(\\theta)$ in the same minibatch. \n",
    "        - moo='prior' : $\\ \\theta'_{nj}\\sim p(\\theta)$ \n",
    "        - moo='p_tilda' : $\\ \\theta'_{nj}\\sim \\tilde{p}(\\theta)$ with _fixed_ $\\tilde{p}(\\theta) = q^*(\\theta|x_0)$  (default)\n",
    "        - moo='q_phi_xo': $\\ \\theta'_{nj}\\sim q^*(\\theta | x_o)$ with _current_ $q^*$\n",
    "        - moo='q_phi_x' : $\\ \\theta'_{nj}\\sim q^*(\\theta | x_n)$ with _current_ $q^*$\n",
    "        - note that the two 'q_phi' rules violate the view of drawing $\\theta_n, \\theta'_{nj}$ iid ! atm it is not perfectly clear what they implement (prbly some form of SVI).\n",
    "    \n",
    "    - construct uniform discrete proposals over $\\{\\theta_n\\} \\cup \\{\\theta'_{nj}\\}_{j=1}^{n_{null}}$ and compute SNPE-C loss and gradients\n",
    "    \n",
    "    - do SGD with minibatches over $minibatch$ many $(\\theta_n, x_n)$\n",
    "    \n",
    "    \n",
    "- takes longer than SNPE-A because \n",
    "\n",
    "    - for every batch, need to sample $minibath * n_{null}$ many $\\theta'_{nj}$ from $minibatch$ many different MoGs $q^*(\\theta|x_n)$ !\n",
    "    - for every batch, need to evaluate $minibath * n_{null}$ many terms $q^*(\\theta'_{nj} | x_n)$ (versus $minibatch$ many for SNPE-A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_on_all:\n",
    "    epochs = [epochs//(r+1) for r in range(n_rounds)]\n",
    "\n",
    "# control MAF seed\n",
    "rng = np.random\n",
    "rng.seed(seed)\n",
    "\n",
    "# generator\n",
    "g = init_g(seed=seed)\n",
    "\n",
    "# inference object\n",
    "res_C = infer.SNPEC(g,\n",
    "                 obs=obs_stats,\n",
    "                 n_hiddens=n_hiddens,\n",
    "                 seed=seed,\n",
    "                 reg_lambda=reg_lambda,\n",
    "                 pilot_samples=pilot_samples,\n",
    "                 svi=svi,\n",
    "                 n_mades=n_mades, # providing this argument triggers usage of MAFs (vs. MDNs)\n",
    "                 act_fun=act_fun,\n",
    "                 mode=mode,\n",
    "                 rng=rng,\n",
    "                 batch_norm=batch_norm,\n",
    "                 verbose=verbose,\n",
    "                 prior_norm=prior_norm)\n",
    "\n",
    "# train\n",
    "t = timeit.time.time()\n",
    "\n",
    "logs_C, tds_C, posteriors_C = res_C.run(\n",
    "                    n_train=n_train,\n",
    "                    proposal='discrete',\n",
    "                    moo='p_tilda',\n",
    "                    n_null = n_null,\n",
    "                    n_rounds=n_rounds,\n",
    "                    train_on_all=train_on_all,\n",
    "                    minibatch=minibatch,\n",
    "                    epochs=epochs)\n",
    "\n",
    "print(timeit.time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in range(n_rounds):\n",
    "    plt.plot(logs_C[r]['loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# round-by-round comparison of posterior estimates\n",
    "- on the first round, SNPE-C = SNPE-A ! (want samples from $q^*_{F(\\phi,x_0)}( \\cdot{} | x_O)$ to not be much worse than samples from prior, which they are for initial $\\phi$ !) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in range(len(logs_C)):\n",
    "    \n",
    "    posterior_A = posteriors_A[-1]\n",
    "    posterior_C = posteriors_C[r]\n",
    "    posterior_C.ndim = posterior_A.ndim\n",
    "    \n",
    "    fig,_=plot_pdf(posterior_A, \n",
    "                   samples=posterior_C.gen(1000).T,\n",
    "                   gt=true_params, \n",
    "                   resolution=100,\n",
    "                   figsize=(16,16));\n",
    "    \n",
    "    fig.suptitle('SNPE-C (samples) vs SNPE-A (density levels), round r = '+str(r+1), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import snl.inference.nde as nde\n",
    "from snl.ml.models.mafs import ConditionalMaskedAutoregressiveFlow\n",
    "from delfi.utils.delfi2snl import SNLprior, SNLmodel\n",
    "\n",
    "\n",
    "# control MAF seed\n",
    "rng = np.random\n",
    "rng.seed(seed)\n",
    "\n",
    "# explicit call to MAF constructor\n",
    "theta, x = g.gen(1)\n",
    "n_inputs, n_outputs  = x.size, theta.size\n",
    "model = ConditionalMaskedAutoregressiveFlow(\n",
    "                n_inputs=n_inputs,\n",
    "                n_outputs=n_outputs,\n",
    "                n_hiddens=n_hiddens,\n",
    "                act_fun=act_fun,\n",
    "                n_mades=n_mades,\n",
    "                mode=mode,\n",
    "                rng=rng)\n",
    "\n",
    "\n",
    "# generator\n",
    "g = init_g(seed=seed)\n",
    "\n",
    "# inference object\n",
    "inf = nde.SequentialNeuralLikelihood(SNLprior(g.prior),               # method to draw parameters  \n",
    "                                     SNLmodel(g.model, g.summary).gen # method to draw summary stats\n",
    "                                    )\n",
    "\n",
    "# train\n",
    "t = timeit.time.time()\n",
    "\n",
    "rng = np.random # control  \n",
    "rng.seed(seed)  # MCMC seed\n",
    "model = inf.learn_likelihood(obs_stats.flatten(), model, n_samples=n_train, n_rounds=n_rounds, \n",
    "                             train_on_all=False, thin=10, save_models=False, \n",
    "                             logger=sys.stdout, rng=rng)\n",
    "\n",
    "print(timeit.time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snl.inference.mcmc as mcmc\n",
    "log_posterior = lambda t: model.eval([t, obs_stats.flatten()]) + inf.prior.eval(t)\n",
    "sampler = mcmc.SliceSampler(x=inf.all_ps[-1][-1], lp_f=log_posterior, thin=10)\n",
    "ps = sampler.gen(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posterior_A = posteriors_A[-1]\n",
    "posterior_C = posteriors_C[r]\n",
    "posterior_C.ndim = posterior_A.ndim\n",
    "\n",
    "fig,_=plot_pdf(posterior_A, \n",
    "               samples=ps.T,\n",
    "               gt=true_params, \n",
    "               resolution=100,\n",
    "               figsize=(16,16));\n",
    "    \n",
    "fig.suptitle('SNL (samples) vs SNPE-A (density levels), final round)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
